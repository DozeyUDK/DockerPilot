#!/usr/bin/env python3
# -*- coding: utf-8 -*-


import docker
import argparse
import yaml
import json
import os
import sys
import time
import requests
import logging
import signal
import subprocess
import threading
from datetime import datetime, timedelta
from pathlib import Path
from rich.console import Console
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeRemainingColumn, TimeElapsedColumn
from rich.prompt import Prompt, Confirm
from rich.panel import Panel
from rich.live import Live
from contextlib import contextmanager
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional, Any

# Import modules
from .models import LogLevel, DeploymentConfig, ContainerStats
from .container_manager import ContainerManager
from .image_manager import ImageManager
from .monitoring import MonitoringManager

class DockerPilotEnhanced:
    """Enhanced Docker container management tool with advanced deployment capabilities."""
    
    def __init__(self, config_file: str = None, log_level: LogLevel = LogLevel.INFO):
        self.console = Console()
        self._show_banner()
        self.client = None
        self.config = {}
        self.log_file = "docker_pilot.log"
        self.metrics_file = "docker_metrics.json"
        self.deployment_history = []
        self._health_check_defaults = None  # Lazy-loaded health check defaults
        self._current_deployment_container = None  # Track current deployment for cancellation
        self._sudo_password = None  # Sudo password from session (for web interface)
        self._progress_callback = None  # Callback for progress updates (for web interface)
        
        # Setup logging
        self._setup_logging(log_level)
        
        # Load configuration
        if config_file and Path(config_file).exists():
            self._load_config(config_file)
        
        # Initialize Docker client with retry logic
        client_initialized = self._init_docker_client()
        
        # Initialize managers only if Docker client is available
        if client_initialized and self.client:
            self.container_manager = ContainerManager(
                self.client, self.console, self.logger, self._error_handler
            )
            self.image_manager = ImageManager(
                self.client, self.console, self.logger, self._error_handler
            )
            self.monitoring_manager = MonitoringManager(
                self.client, self.console, self.logger, self.metrics_file
            )
        else:
            # Set managers to None if Docker client is not available
            # We'll check Docker availability in run_cli() and _run_interactive_menu()
            # for CLI context, and handle gracefully in web interface
            self.container_manager = None
            self.image_manager = None
            self.monitoring_manager = None
            self.logger.warning("Docker client not initialized - managers not available")
        
        # Setup signal handlers for graceful shutdown
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
        
        self.logger.info("Docker Pilot Enhanced initialized successfully")
    
    def _show_banner(self):
        """Display ASCII banner with application information"""
        banner = """
  _____             _             _____ _ _       _   
 |  __ \           | |           |  __ (_) |     | |  
 | |  | | ___   ___| | _____ _ __| |__) || | ___ | |_ 
 | |  | |/ _ \ / __| |/ / _ \ '__|  ___/ | |/ _ \| __|
 | |__| | (_) | (__|   <  __/ |  | |   | | | (_) | |_ 
 |_____/ \___/ \___|_|\_\___|_|  |_|   |_|_|\___/ \__|
                                                      
         by Dozey                                             
    """
        

        self.console.print(Panel(banner, title="[bold blue]Docker Managing Tool[/bold blue]", 
                                title_align="center", border_style="blue"))
        self.console.print(f"[dim]Author: dozey | Version: Enhanced[/dim]\n")
    
    def _parse_multi_target(self, target_string: str) -> List[str]:
        """Parse comma-separated list of containers/images.
        
        Args:
            target_string: String with comma-separated container/image names or IDs
            
        Returns:
            List of container/image names/IDs
        """
        if not target_string:
            return []
        
        # Split by comma and strip whitespace
        targets = [t.strip() for t in target_string.split(',') if t.strip()]
        return targets

    def _setup_logging(self, level: LogLevel):
        """Setup enhanced logging with rotation"""
        log_format = '%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
        
        # File handler with rotation
        from logging.handlers import RotatingFileHandler
        file_handler = RotatingFileHandler(
            self.log_file, maxBytes=10*1024*1024, backupCount=5
        )
        file_handler.setFormatter(logging.Formatter(log_format))
        
        # Console handler
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))
        
        # Setup logger
        self.logger = logging.getLogger('DockerPilot')
        self.logger.setLevel(getattr(logging, level.value))
        self.logger.addHandler(file_handler)
        self.logger.addHandler(console_handler)

    def _load_config(self, config_file: str):
        """Load configuration from YAML file"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                self.config = yaml.safe_load(f)
            self.logger.info(f"Configuration loaded from {config_file}")
        except Exception as e:
            self.logger.error(f"Failed to load config: {e}")
            self.config = {}
    
    def _check_cancel_flag(self, container_name: str = None) -> bool:
        """Check if deployment should be cancelled
        
        Args:
            container_name: Container being deployed (uses self._current_deployment_container if not provided)
        
        Returns:
            bool: True if deployment should be cancelled
        """
        if not container_name:
            container_name = self._current_deployment_container
        
        if not container_name:
            return False
        
        # Look for cancel flag in multiple locations
        cancel_flag_locations = [
            Path.cwd() / f'cancel_{container_name}.flag',
            Path.home() / 'DockerPilot' / f'cancel_{container_name}.flag',
            Path.home() / 'DockerPilot' / '.dockerpilot_extras' / f'cancel_{container_name}.flag',
        ]
        
        for flag_path in cancel_flag_locations:
            if flag_path.exists():
                self.logger.warning(f"Cancel flag detected for {container_name} at {flag_path}")
                # Remove flag after detecting
                try:
                    flag_path.unlink()
                except:
                    pass
                return True
        
        return False
    
    def _load_health_check_defaults(self) -> dict:
        """Load default health check configuration from JSON file
        
        Returns cached defaults or loads from health-checks-defaults.json
        """
        if self._health_check_defaults is not None:
            return self._health_check_defaults
        
        try:
            defaults_path = Path(__file__).parent / "configs" / "health-checks-defaults.json"
            
            if defaults_path.exists():
                with open(defaults_path, 'r', encoding='utf-8') as f:
                    self._health_check_defaults = json.load(f)
                self.logger.debug(f"Loaded health check defaults from {defaults_path}")
            else:
                # Fallback to minimal defaults if file doesn't exist
                self.logger.warning(f"Health check defaults file not found: {defaults_path}")
                self._health_check_defaults = {
                    'health_checks': {
                        'non_http_services': ['ssh', 'redis', 'mysql', 'postgresql', 'mongodb'],
                        'endpoint_mappings': {},
                        'default_endpoint': '/health'
                    }
                }
        except Exception as e:
            self.logger.error(f"Failed to load health check defaults: {e}")
            # Fallback to minimal defaults
            self._health_check_defaults = {
                'health_checks': {
                    'non_http_services': ['ssh', 'redis', 'mysql', 'postgresql', 'mongodb'],
                    'endpoint_mappings': {},
                    'default_endpoint': '/health'
                }
            }
        
        return self._health_check_defaults
    
    def _get_database_config(self, image_tag: str) -> dict:
        """Get database-specific configuration based on image tag.
        
        Args:
            image_tag: Docker image tag to check
            
        Returns:
            dict: Database configuration or empty dict if not a database
        """
        defaults = self._load_health_check_defaults()
        database_services = defaults.get('database_services', {})
        
        image_lower = image_tag.lower()
        
        # Check each database service pattern (longest match first for specificity)
        # Sort by length descending to match more specific names first
        sorted_db_names = sorted(database_services.keys(), key=len, reverse=True)
        
        for db_name in sorted_db_names:
            if db_name in image_lower:
                self.logger.debug(f"Matched database service: {db_name} for image {image_tag}")
                return database_services[db_name]
        
        # Return default/empty config for non-database services
        return {}
    
    def _get_database_name(self, image_tag: str) -> str:
        """Get database service name from image tag.
        
        Args:
            image_tag: Docker image tag to check
            
        Returns:
            str: Database name or empty string if not a database
        """
        defaults = self._load_health_check_defaults()
        database_services = defaults.get('database_services', {})
        
        image_lower = image_tag.lower()
        
        # Check each database service pattern (longest match first)
        sorted_db_names = sorted(database_services.keys(), key=len, reverse=True)
        
        for db_name in sorted_db_names:
            if db_name in image_lower:
                return db_name
        
        return ""
    
    def _is_database_service(self, image_tag: str) -> bool:
        """Check if image tag represents a database service.
        
        Args:
            image_tag: Docker image tag to check
            
        Returns:
            bool: True if it's a database service
        """
        db_config = self._get_database_config(image_tag)
        return len(db_config) > 0
    
    def _init_docker_client(self, max_retries: int = 3):
        """Initialize Docker client with retry logic
        
        Returns True if client initialized successfully, False otherwise.
        In web interface context, does not exit on failure.
        """
        for attempt in range(max_retries):
            try:
                self.client = docker.from_env()
                # Test connection
                self.client.ping()
                if hasattr(self, 'logger') and self.logger:
                    self.logger.info("Docker client connected successfully")
                return True
            except Exception as e:
                # Log the actual error for debugging
                error_msg = str(e)
                error_type = type(e).__name__
                if hasattr(self, 'logger') and self.logger:
                    self.logger.warning(f"Docker connection attempt {attempt + 1} failed ({error_type}): {error_msg}")
                else:
                    # Fallback to print if logger not available
                    print(f"WARNING: Docker connection attempt {attempt + 1} failed ({error_type}): {error_msg}")
                
                if attempt == max_retries - 1:
                    if hasattr(self, 'logger') and self.logger:
                        self.logger.error(f"Failed to connect to Docker daemon after {max_retries} attempts ({error_type}): {error_msg}")
                    else:
                        print(f"ERROR: Failed to connect to Docker daemon after {max_retries} attempts ({error_type}): {error_msg}")
                    
                    if hasattr(self, 'console') and self.console:
                        self.console.print(f"[bold red]‚ùå Cannot connect to Docker daemon![/bold red]")
                    self.client = None
                    # Don't exit here - let the calling code decide (run_cli() or web interface)
                    return False
                time.sleep(2)
        return False
    
    def _update_progress(self, stage: str, progress: int, message: str):
        """Update progress if callback is available
        
        Args:
            stage: Current stage name (e.g., 'backup', 'deploy', 'health_check')
            progress: Progress percentage (0-100)
            message: Human-readable message
        """
        if self._progress_callback:
            try:
                self._progress_callback(stage, progress, message)
            except Exception as e:
                if hasattr(self, 'logger') and self.logger:
                    self.logger.debug(f"Progress callback error: {e}")
    
    def _show_loading(self, message: str = "Processing", stop_event: threading.Event = None):
        """Show animated loading dots while operation is in progress
        
        Args:
            message: Message to display before dots
            stop_event: Threading event to stop the animation
        """
        dots = ['.', '..', '...', '....']
        idx = 0
        while stop_event is None or not stop_event.is_set():
            # Print loading message with animated dots
            sys.stdout.write(f'\r{message}{dots[idx % len(dots)]}')
            sys.stdout.flush()
            idx += 1
            time.sleep(0.5)  # Update every 0.5 seconds
        
        # Clear the line when done
        sys.stdout.write('\r' + ' ' * (len(message) + 4) + '\r')
        sys.stdout.flush()
    
    @contextmanager
    def _with_loading(self, message: str = "Processing"):
        """Context manager to show loading indicator during long operations
        
        Usage:
            with self._with_loading("Backing up data"):
                # Long operation here
                pass
        """
        stop_event = threading.Event()
        loading_thread = threading.Thread(
            target=self._show_loading,
            args=(message, stop_event),
            daemon=True
        )
        loading_thread.start()
        
        try:
            yield
        finally:
            stop_event.set()
            loading_thread.join(timeout=1.0)  # Wait max 1 second for thread to finish
            # Clear the loading line
            sys.stdout.write('\r' + ' ' * (len(message) + 4) + '\r')
            sys.stdout.flush()

    def _signal_handler(self, signum, frame):
        """Graceful shutdown handler"""
        self.logger.info(f"Received signal {signum}, shutting down gracefully...")
        self.console.print("\n[yellow]‚ö†Ô∏è Graceful shutdown initiated...[/yellow]")
        sys.exit(0)

    @contextmanager
    def _error_handler(self, operation: str, container_name: str = None):
        """Enhanced error handling context manager"""
        try:
            yield
        except docker.errors.NotFound as e:
            error_msg = f"Container/Image not found: {container_name or 'unknown'}"
            self.logger.error(f"{operation} failed: {error_msg}")
            self.console.print(f"[bold red]‚ùå {error_msg}[/bold red]")
        except docker.errors.APIError as e:
            error_msg = f"Docker API error during {operation}: {e}"
            self.logger.error(error_msg)
            self.console.print(f"[bold red]‚ùå {error_msg}[/bold red]")
        except requests.exceptions.RequestException as e:
            error_msg = f"Network error during {operation}: {e}"
            self.logger.error(error_msg)
            self.console.print(f"[bold red]‚ùå {error_msg}[/bold red]")
        except Exception as e:
            error_msg = f"Unexpected error during {operation}: {e}"
            self.logger.error(error_msg)
            self.console.print(f"[bold red]‚ùå {error_msg}[/bold red]")

    # ==================== CONTAINER MANAGEMENT ====================

    def list_containers(self, show_all: bool = True, format_output: str = "table") -> List[Any]:
        """Enhanced container listing with multiple output formats."""
        if not self.container_manager:
            self.logger.error("Container manager not initialized - Docker client not available")
            return []
        return self.container_manager.list_containers(show_all, format_output)

    def list_images(self, show_all: bool = True, format_output: str = "table", hide_untagged: bool = False) -> List[Any]:
        """Enhanced image listing with multiple output formats.
        
        Args:
            show_all: Show all images (including intermediate layers)
            format_output: Output format ('table' or 'json')
            hide_untagged: Hide images without tags (dangling images)
        """
        if not self.image_manager:
            self.logger.error("Image manager not initialized - Docker client not available")
            return []
        return self.image_manager.list_images(show_all, format_output, hide_untagged)
    
    def remove_image(self, image_name: str, force: bool = False) -> bool:
        """Remove Docker image."""
        if not self.image_manager:
            self.logger.error("Image manager not initialized - Docker client not available")
            return False
        return self.image_manager.remove_image(image_name, force)
    
    def prune_dangling_images(self, dry_run: bool = False) -> dict:
        """Remove all dangling images (images without tags).
        
        Args:
            dry_run: If True, only show what would be removed without actually removing
            
        Returns:
            dict: Statistics about removed images (images_deleted, space_reclaimed)
        """
        if not self.image_manager:
            self.logger.error("Image manager not initialized - Docker client not available")
            return {'images_deleted': 0, 'space_reclaimed': 0}
        
        if dry_run:
            # No loading for dry run as it's fast
            return self.image_manager.prune_dangling_images(dry_run)
        else:
            # Show loading indicator for actual removal
            with self._with_loading("Removing dangling images"):
                return self.image_manager.prune_dangling_images(dry_run)

    def container_operation(self, operation: str, container_name: str, **kwargs) -> bool:
        """Unified container operation handler with progress tracking."""
        if operation == 'update_restart_policy':
            return self.update_restart_policy(container_name, kwargs.get('policy', 'unless-stopped'))
        elif operation == 'run_image':
            return self.run_new_container(
                kwargs.get('image_name'),
                kwargs.get('name', container_name),
                kwargs.get('ports'),
                kwargs.get('command'),
                kwargs.get('environment'),
                kwargs.get('volumes'),
                kwargs.get('restart_policy', 'unless-stopped'),
                kwargs.get('network'),
                kwargs.get('privileged', False),
                kwargs.get('cpu_limit'),
                kwargs.get('memory_limit')
            )
        else:
            if not self.container_manager:
                self.logger.error("Container manager not initialized - Docker client not available")
                return False
            return self.container_manager.container_operation(operation, container_name, **kwargs)
    
    def update_restart_policy(self, container_name: str, policy: str = 'unless-stopped') -> bool:
        """Set restart policy on container."""
        if not self.container_manager:
            self.logger.error("Container manager not initialized - Docker client not available")
            return False
        return self.container_manager.update_restart_policy(container_name, policy)
    
    def run_new_container(self, image_name: str, name: str, ports: dict = None, 
                        command: str = None, environment: dict = None,
                        volumes: dict = None, restart_policy: str = 'unless-stopped',
                        network: str = None, privileged: bool = False,
                        cpu_limit: str = None, memory_limit: str = None) -> bool:
        """Run a new container with full configuration options.
        
        Args:
            image_name: Docker image name/tag
            name: Container name
            ports: Port mapping dict (e.g., {'80': '8080'})
            command: Command to run in container
            environment: Environment variables dict
            volumes: Volume mappings dict
            restart_policy: Restart policy (no, on-failure, always, unless-stopped)
            network: Network name or 'host' for host network
            privileged: Run container in privileged mode
            cpu_limit: CPU limit (e.g., '1.5' for 1.5 CPUs)
            memory_limit: Memory limit (e.g., '1g' for 1GB)
        """
        if not self.container_manager:
            self.logger.error("Container manager not initialized - Docker client not available")
            return False
        return self.container_manager.run_new_container(
            image_name, name, ports, command, environment, volumes,
            restart_policy, network, privileged, cpu_limit, memory_limit
        )
    
    def exec_container(self, container_name: str, command: str = "/bin/bash") -> bool:
        """Execute interactive command in running container."""
        import subprocess
        
        with self._error_handler(f"exec into container {container_name}", container_name):
            # Verify container exists and is running
            container = self.client.containers.get(container_name)
            if container.status != 'running':
                self.console.print(f"[bold red]‚ùå Container '{container_name}' is not running (status: {container.status})[/bold red]")
                return False
            
            self.logger.info(f"Executing interactive command in container {container_name}: {command}")
            self.console.print(f"[cyan]üìü Executing '{command}' in container '{container_name}'...[/cyan]")
            self.console.print(f"[dim]Type 'exit' to leave the container shell[/dim]\n")
            
            # Use subprocess to maintain interactive terminal
            # This allows proper TTY handling for interactive bash session
            try:
                result = subprocess.run(
                    ['docker', 'exec', '-it', container_name, command],
                    check=False
                )
                
                if result.returncode == 0:
                    self.console.print(f"\n[green]‚úÖ Exited from container '{container_name}'[/green]")
                    return True
                else:
                    self.console.print(f"\n[yellow]‚ö†Ô∏è Exec command exited with code {result.returncode}[/yellow]")
                    return False
                    
            except FileNotFoundError:
                self.console.print("[bold red]‚ùå Docker CLI not found. Please ensure Docker is installed and in PATH.[/bold red]")
                return False
            except Exception as e:
                self.console.print(f"[bold red]‚ùå Failed to execute command: {e}[/bold red]")
                self.logger.error(f"Exec failed: {e}")
                return False
        
        return False

    # ==================== MONITORING & METRICS ====================

    def get_container_stats(self, container_name: str) -> Optional[ContainerStats]:
        """Get comprehensive container statistics."""
        if not self.monitoring_manager:
            self.logger.error("Monitoring manager not initialized - Docker client not available")
            return None
        return self.monitoring_manager.get_container_stats(container_name)
    
    def monitor_containers_dashboard(self, containers: List[str] = None, duration: int = 300):
        """Real-time monitoring dashboard for multiple containers."""
        if not self.monitoring_manager:
            self.logger.error("Monitoring manager not initialized - Docker client not available")
            return
        return self.monitoring_manager.monitor_containers_dashboard(containers, duration)
    
    def get_container_stats_once(self, container_name: str) -> bool:
        """Get one-time container statistics snapshot (from dockerpilot-Lite)"""
        with self._error_handler(f"get stats for {container_name}", container_name):
            container = self.client.containers.get(container_name)
            
            # Get two measurements 1 second apart for accurate CPU calculation
            self.console.print(f"[cyan]üìä Collecting statistics for {container_name}...[/cyan]")
            
            stats1 = container.stats(stream=False)
            time.sleep(1)
            stats2 = container.stats(stream=False)
            
            # Calculate CPU percentage
            cpu_percent = 0.0
            try:
                cpu1_total = stats1['cpu_stats']['cpu_usage']['total_usage']
                cpu1_system = stats1['cpu_stats'].get('system_cpu_usage', 0)
                
                cpu2_total = stats2['cpu_stats']['cpu_usage']['total_usage']
                cpu2_system = stats2['cpu_stats'].get('system_cpu_usage', 0)
                
                cpu_delta = cpu2_total - cpu1_total
                system_delta = cpu2_system - cpu1_system
                
                online_cpus = len(stats2['cpu_stats']['cpu_usage'].get('percpu_usage', [1]))
                
                if system_delta > 0 and cpu_delta >= 0:
                    cpu_percent = (cpu_delta / system_delta) * online_cpus * 100.0
            except (KeyError, ZeroDivisionError) as e:
                self.logger.warning(f"CPU calculation error: {e}")
                cpu_percent = 0.0
            
            # Memory statistics
            mem_usage = stats2['memory_stats'].get('usage', 0)
            mem_limit = stats2['memory_stats'].get('limit', 1)
            mem_percent = (mem_usage / mem_limit) * 100.0 if mem_limit > 0 else 0
            
            # Network statistics
            network_stats = stats2.get('networks', {})
            rx_bytes = 0
            tx_bytes = 0
            for interface, net_data in network_stats.items():
                rx_bytes += net_data.get('rx_bytes', 0)
                tx_bytes += net_data.get('tx_bytes', 0)
            
            # Display results
            self.console.print(f"\n[bold cyan]üìä Container Statistics: {container_name}[/bold cyan]")
            self.console.print(f"[green]üñ•Ô∏è  CPU Usage: {cpu_percent:.2f}%[/green]")
            self.console.print(f"[blue]üíæ Memory: {mem_usage/(1024*1024):.2f} MB / {mem_limit/(1024*1024):.2f} MB ({mem_percent:.2f}%)[/blue]")
            
            if rx_bytes > 0 or tx_bytes > 0:
                self.console.print(f"[magenta]üåê Network RX: {rx_bytes/(1024*1024):.2f} MB, TX: {tx_bytes/(1024*1024):.2f} MB[/magenta]")
            
            # Process count
            if 'pids_stats' in stats2:
                pids = stats2['pids_stats'].get('current', 0)
                self.console.print(f"[yellow]‚ö° Processes: {pids}[/yellow]")
            
            return True
        
        return False
    
    def monitor_container_live(self, container_name: str, duration: int = 30) -> bool:
        """Live monitoring with screen clearing (from dockerpilot-Lite)"""
        with self._error_handler(f"live monitor {container_name}", container_name):
            container = self.client.containers.get(container_name)
            
            self.console.print(f"[cyan]Starting live monitoring for {container_name} ({duration}s)...[/cyan]")
            self.console.print(f"[yellow]Press Ctrl+C to stop[/yellow]\n")
            
            stats_stream = container.stats(stream=True)
            start_time = time.time()
            prev_stats = None
            
            try:
                for raw_stats in stats_stream:
                    current_time = time.time()
                    if current_time - start_time > duration:
                        break
                    
                    try:
                        # Parse stats data
                        if isinstance(raw_stats, bytes):
                            stats = json.loads(raw_stats.decode('utf-8'))
                        elif isinstance(raw_stats, str):
                            stats = json.loads(raw_stats)
                        else:
                            stats = raw_stats
                        
                        if not isinstance(stats, dict):
                            time.sleep(1)
                            continue
                        
                        # Calculate CPU if we have previous measurement
                        cpu_percent = 0.0
                        if prev_stats and isinstance(prev_stats, dict):
                            try:
                                cpu_stats = stats.get('cpu_stats', {})
                                prev_cpu_stats = prev_stats.get('cpu_stats', {})
                                
                                if 'cpu_usage' in cpu_stats and 'cpu_usage' in prev_cpu_stats:
                                    current_total = cpu_stats['cpu_usage'].get('total_usage', 0)
                                    prev_total = prev_cpu_stats['cpu_usage'].get('total_usage', 0)
                                    
                                    current_system = cpu_stats.get('system_cpu_usage', 0)
                                    prev_system = prev_cpu_stats.get('system_cpu_usage', 0)
                                    
                                    cpu_delta = current_total - prev_total
                                    system_delta = current_system - prev_system
                                    
                                    online_cpus = len(cpu_stats['cpu_usage'].get('percpu_usage', [1]))
                                    
                                    if system_delta > 0 and cpu_delta >= 0:
                                        cpu_percent = (cpu_delta / system_delta) * online_cpus * 100.0
                            except (KeyError, ZeroDivisionError, TypeError):
                                cpu_percent = 0.0
                        
                        # Memory stats
                        memory_stats = stats.get('memory_stats', {})
                        mem_usage = memory_stats.get('usage', 0) / (1024*1024)
                        mem_limit = memory_stats.get('limit', 1) / (1024*1024)
                        mem_percent = (mem_usage / mem_limit) * 100.0 if mem_limit > 0 else 0
                        
                        # Clear screen and display current stats
                        os.system('clear' if os.name == 'posix' else 'cls')
                        self.console.print(f"[bold cyan]üìä Live Monitoring: {container_name}[/bold cyan]")
                        self.console.print(f"[green]üñ•Ô∏è  CPU: {cpu_percent:.2f}%[/green]")
                        self.console.print(f"[blue]üíæ RAM: {mem_usage:.1f}MB / {mem_limit:.1f}MB ({mem_percent:.1f}%)[/blue]")
                        self.console.print(f"[yellow]‚è±Ô∏è  Time: {int(current_time - start_time)}/{duration}s[/yellow]")
                        self.console.print(f"[dim]Press Ctrl+C to stop[/dim]")
                        
                        prev_stats = stats
                        
                    except (json.JSONDecodeError, UnicodeDecodeError) as e:
                        self.logger.warning(f"Stats parsing error: {e}")
                        continue
                    except Exception as e:
                        self.logger.warning(f"Stats processing error: {e}")
                        continue
                    
                    time.sleep(1)
                
                self.console.print(f"\n[green]‚úÖ Live monitoring completed[/green]")
                return True
                
            except KeyboardInterrupt:
                self.console.print(f"\n[yellow]‚ö†Ô∏è Monitoring interrupted by user[/yellow]")
                return True
        
        return False
    
    def stop_and_remove_container(self, container_name: str, timeout: int = 10) -> bool:
        """Stop and remove container in one operation (from dockerpilot-Lite)"""
        with self._error_handler(f"stop and remove {container_name}", container_name):
            container = self.client.containers.get(container_name)
            
            self.console.print(f"[cyan]üõë Stopping container {container_name}...[/cyan]")
            if container.status == "running":
                container.stop(timeout=timeout)
                self.console.print(f"[green]‚úÖ Container stopped[/green]")
            else:
                self.console.print(f"[yellow]‚ÑπÔ∏è Container was not running[/yellow]")
            
            self.console.print(f"[cyan]üóëÔ∏è Removing container {container_name}...[/cyan]")
            container.remove()
            self.console.print(f"[green]‚úÖ Container {container_name} removed[/green]")
            
            self.logger.info(f"Container {container_name} stopped and removed")
            return True
        
        return False
    
    def exec_command_non_interactive(self, container_name: str, command: str) -> bool:
        """Execute command in container non-interactively (from dockerpilot-Lite)"""
        with self._error_handler(f"exec command in {container_name}", container_name):
            container = self.client.containers.get(container_name)
            
            if container.status != 'running':
                self.console.print(f"[red]‚ùå Container '{container_name}' is not running[/red]")
                return False
            
            self.console.print(f"[cyan]‚öôÔ∏è Executing: {command}[/cyan]")
            exec_log = container.exec_run(command)
            
            output = exec_log.output.decode()
            self.console.print(output)
            
            if exec_log.exit_code == 0:
                self.console.print(f"[green]‚úÖ Command executed successfully[/green]")
                return True
            else:
                self.console.print(f"[yellow]‚ö†Ô∏è Command exited with code {exec_log.exit_code}[/yellow]")
                return False
        
        return False
    
    def health_check_standalone(self, port: int, endpoint: str = "/health", 
                               timeout: int = 30, max_retries: int = 10) -> bool:
        """Standalone health check menu (from dockerpilot-Lite)"""
        url = f"http://localhost:{port}{endpoint}"
        self.console.print(f"[cyan]ü©∫ Testing health check: {url}[/cyan]")
        
        for i in range(max_retries):
            try:
                response = requests.get(url, timeout=5)
                if response.status_code == 200:
                    self.console.print(f"[green]‚úÖ Health check OK (attempt {i+1}/{max_retries})[/green]")
                    self.console.print(f"[green]Response time: {response.elapsed.total_seconds():.2f}s[/green]")
                    return True
                else:
                    self.console.print(f"[yellow]‚ö†Ô∏è Health check returned {response.status_code} (attempt {i+1}/{max_retries})[/yellow]")
            except requests.exceptions.RequestException as e:
                self.console.print(f"[yellow]‚ö†Ô∏è Health check failed (attempt {i+1}/{max_retries}): {e}[/yellow]")
            
            if i < max_retries - 1:
                time.sleep(3)
        
        self.console.print(f"[red]‚ùå Health check failed after {max_retries} attempts[/red]")
        return False

    # ==================== ADVANCED DEPLOYMENT ====================

    def create_deployment_config(self, config_path: str = "deployment.yml") -> bool:
        """Create deployment configuration template from file"""
        try:
            # Load template from configs directory
            template_path = Path(__file__).parent / "configs" / "deployment.yml.template"
            
            if not template_path.exists():
                self.logger.error(f"Template file not found: {template_path}")
                self.console.print(f"[red]Template file not found: {template_path}[/red]")
                return False
            
            with open(template_path, 'r', encoding='utf-8') as f:
                template_content = f.read()
            
            with open(config_path, 'w', encoding='utf-8') as f:
                f.write(template_content)
            
            self.console.print(f"[green]‚úÖ Deployment configuration template created: {config_path}[/green]")
            return True
        except Exception as e:
            self.logger.error(f"Failed to create config template: {e}")
            return False

    def deploy_from_config(self, config_path: str, deployment_type: str = "rolling") -> bool:
        """Deploy using configuration file"""
        try:
            with open(config_path, 'r') as f:
                config = yaml.safe_load(f)
            
            # Normalize deployment config to ensure all fields are in correct format
            deployment = config['deployment']
            # Ensure volumes is a dict
            if 'volumes' not in deployment or not isinstance(deployment.get('volumes'), dict):
                deployment['volumes'] = {}
            # Ensure port_mapping is a dict
            if 'port_mapping' not in deployment or not isinstance(deployment.get('port_mapping'), dict):
                deployment['port_mapping'] = {}
            # Ensure environment is a dict
            if 'environment' not in deployment or not isinstance(deployment.get('environment'), dict):
                deployment['environment'] = {}
            
            deployment_config = DeploymentConfig(**deployment)
            build_config = config.get('build', {})
            
            self.logger.info(f"Starting {deployment_type} deployment from config: {config_path}")
            
            if deployment_type == "blue-green":
                return self._blue_green_deploy_enhanced(deployment_config, build_config)
            elif deployment_type == "canary":
                return self._canary_deploy(deployment_config, build_config)
            else:  # rolling deployment
                return self._rolling_deploy(deployment_config, build_config)
                
        except Exception as e:
            self.logger.error(f"Deployment from config failed: {e}")
            return False

    def _rolling_deploy(self, config: DeploymentConfig, build_config: dict) -> bool:
        """Enhanced rolling deployment with zero-downtime and full logging"""
        self.console.print(f"\n[bold cyan]üöÄ ROLLING DEPLOYMENT STARTED[/bold cyan]")

        deployment_start = datetime.now()
        deployment_id = f"deploy_{int(deployment_start.timestamp())}"
        
        # Auto-detect health check endpoint based on image type
        detected_endpoint = self._detect_health_check_endpoint(config.image_tag)
        if detected_endpoint != config.health_check_endpoint:
            self.logger.info(f"Auto-detected health check endpoint: {detected_endpoint} (was: {config.health_check_endpoint})")
            config.health_check_endpoint = detected_endpoint

        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=self.console
        ) as progress:

            # Phase 1: Prepare image (check, pull, or build)
            build_task = progress.add_task("üî® Preparing image...", total=None)
            try:
                success, message = self._prepare_image(config.image_tag, build_config)
                if not success:
                    progress.update(build_task, description=f"‚ùå {message}")
                    self.console.print(f"[bold red]‚ùå {message}[/bold red]")
                    return False
                progress.update(build_task, description=f"‚úÖ {message}")
            except Exception as e:
                progress.update(build_task, description="‚ùå Image preparation failed")
                self.logger.error(f"Image preparation failed: {e}")
                self.console.print(f"[bold red]‚ùå Image preparation failed: {e}[/bold red]")
                return False

            # Phase 2: Check existing container
            health_task = progress.add_task("üîç Checking existing deployment...", total=None)
            existing_container = None
            try:
                existing_container = self.client.containers.get(config.container_name)
                if existing_container.status == "running":
                    progress.update(health_task, description="‚úÖ Found running container")
                else:
                    progress.update(health_task, description="‚ö†Ô∏è Container exists but not running")
            except docker.errors.NotFound:
                progress.update(health_task, description="‚ÑπÔ∏è No existing container (first deployment)")

            # Phase 3: Create and start new container with temporary name
            temp_name = f"{config.container_name}_new_{deployment_id}"
            deploy_task = progress.add_task("üöÄ Deploying new version...", total=None)
            try:
                # Add command if provided in config (for images that exit immediately without command)
                create_kwargs = {
                    'image': config.image_tag,
                    'name': temp_name,
                    'ports': config.port_mapping,
                    'environment': config.environment,
                    'volumes': self._normalize_volumes(config.volumes),
                    'restart_policy': {"Name": config.restart_policy},
                    'network': config.network,
                    **self._get_resource_limits(config)
                }
                if hasattr(config, 'command') and config.command:
                    create_kwargs['command'] = config.command
                elif 'alpine' in config.image_tag.lower():
                    # Alpine needs a command to stay running
                    create_kwargs['command'] = ['sh', '-c', 'sleep 3600']
                
                new_container = self.client.containers.create(**create_kwargs)

                # Start container
                try:
                    new_container.start()
                    progress.update(deploy_task, description="‚úÖ New container started")
                except Exception as e:
                    progress.update(deploy_task, description="‚ùå New container deployment failed")
                    self.logger.error(f"Container start failed: {e}")
                    try:
                        logs = new_container.logs().decode()
                        self.logger.error(f"Container logs:\n{logs}")
                    except:
                        pass
                    return False

                # Grace period - longer for HTTP services like nginx
                grace_period = 5
                if 'nginx' in config.image_tag.lower() or 'http' in config.image_tag.lower():
                    grace_period = 15  # nginx needs more time to start
                time.sleep(grace_period)
                
                # Verify container is running before health check
                try:
                    new_container.reload()
                    if new_container.status != "running":
                        self.logger.error(f"Container {new_container.name} is not running (status: {new_container.status})")
                        new_container.stop()
                        new_container.remove()
                        return False
                except Exception as e:
                    self.logger.warning(f"Could not verify container status: {e}")

            except Exception as e:
                progress.update(deploy_task, description="‚ùå New container creation failed")
                self.logger.error(f"New container creation failed: {e}")
                return False

            # Phase 4: Health check new container (only if ports are mapped)
            if config.port_mapping:
                health_check_task = progress.add_task("ü©∫ Health checking new deployment...", total=None)
                host_port = list(config.port_mapping.values())[0]
                
                # Wait a bit more and verify container is running before health check
                time.sleep(2)
                try:
                    new_container.reload()
                    if new_container.status != "running":
                        progress.update(health_check_task, description="‚ùå Container not running")
                        new_container.stop()
                        new_container.remove()
                        return False
                except Exception as e:
                    self.logger.warning(f"Could not verify container status: {e}")
                
                if not self._advanced_health_check(
                    host_port,
                    config.health_check_endpoint,
                    config.health_check_timeout,
                    config.health_check_retries
                ):
                    progress.update(health_check_task, description="‚ùå Health check failed - rolling back")
                    try:
                        logs = new_container.logs().decode()
                        self.logger.error(f"Health check failed. Container logs:\n{logs}")
                    except Exception as e:
                        self.logger.error(f"Could not fetch logs: {e}")

                    # Rollback
                    try:
                        new_container.stop()
                        new_container.remove()
                    except Exception as e:
                        self.logger.error(f"Rollback failed: {e}")
                    return False
                progress.update(health_check_task, description="‚úÖ Health check passed")
            else:
                progress.add_task("ü©∫ No port mapping, skipping health check", total=None)

            # Phase 5: Traffic switch (stop old, rename new)
            switch_task = progress.add_task("üîÑ Switching traffic...", total=None)
            try:
                if existing_container and existing_container.status == "running":
                    existing_container.stop(timeout=10)
                    existing_container.remove()

                new_container.rename(config.container_name)
                progress.update(switch_task, description="‚úÖ Traffic switched successfully")
            except Exception as e:
                progress.update(switch_task, description="‚ùå Traffic switch failed")
                self.logger.error(f"Traffic switch failed: {e}")
                return False

        # Deployment summary
        deployment_end = datetime.now()
        duration = deployment_end - deployment_start
        self._record_deployment(deployment_id, config, "rolling", True, duration)

        self.console.print(f"\n[bold green]üéâ ROLLING DEPLOYMENT COMPLETED SUCCESSFULLY![/bold green]")
        self.console.print(f"[green]Duration: {duration.total_seconds():.1f}s[/green]")
        if config.port_mapping:
            port = list(config.port_mapping.values())[0]
            self.console.print(f"[green]Application available at: http://localhost:{port}[/green]")
        else:
            self.console.print(f"[green]Application deployed (no port mapping set)[/green]")

        return True

    def view_container_logs(self, container_name: str = None, tail: int = 50):
        """View container logs."""
        if not self.container_manager:
            self.logger.error("Container manager not initialized - Docker client not available")
            return None
        return self.container_manager.view_container_logs(container_name, tail)
    
    def view_container_json(self, container_name: str):
        """Display container information in JSON format."""
        if not self.container_manager:
            self.logger.error("Container manager not initialized - Docker client not available")
            return None
        return self.container_manager.view_container_json(container_name)


    def _blue_green_deploy_enhanced(self, config: DeploymentConfig, build_config: dict, skip_backup: bool = False) -> bool:
        """Enhanced Blue-Green deployment with advanced features
        
        Args:
            config: Deployment configuration
            build_config: Build configuration
            skip_backup: Skip data backup (faster but risky for production)
        """
        self.console.print(f"\n[bold cyan]üîµüü¢ BLUE-GREEN DEPLOYMENT STARTED[/bold cyan]")
        
        deployment_start = datetime.now()
        deployment_id = f"bg_deploy_{int(deployment_start.timestamp())}"
        
        # Track current deployment for cancellation support
        self._current_deployment_container = config.container_name
        
        # Auto-detect health check endpoint based on image type
        detected_endpoint = self._detect_health_check_endpoint(config.image_tag)
        if detected_endpoint != config.health_check_endpoint:
            self.logger.info(f"Auto-detected health check endpoint: {detected_endpoint} (was: {config.health_check_endpoint})")
            config.health_check_endpoint = detected_endpoint
        
        blue_name = f"{config.container_name}_blue"
        green_name = f"{config.container_name}_green"
        
        # Determine current active container
        active_container = None
        active_name = None
        
        # Check for blue-green containers first
        try:
            blue_container = self.client.containers.get(blue_name)
            if blue_container.status == "running":
                active_container = blue_container
                active_name = "blue"
        except docker.errors.NotFound:
            pass
        
        if not active_container:
            try:
                green_container = self.client.containers.get(green_name)
                if green_container.status == "running":
                    active_container = green_container
                    active_name = "green"
            except docker.errors.NotFound:
                pass
        
        # If no blue-green container found, check for main container (without suffix)
        # This handles migration from old deployment to blue-green
        if not active_container:
            try:
                main_container = self.client.containers.get(config.container_name)
                if main_container.status == "running":
                    active_container = main_container
                    active_name = "main"
                    self.console.print(f"[yellow]Found existing container '{config.container_name}', will migrate to blue-green[/yellow]")
            except docker.errors.NotFound:
                pass
        
        target_name = "green" if active_name == "blue" else "blue"
        target_container_name = green_name if target_name == "green" else blue_name
        
        self.console.print(f"[cyan]Current active: {active_name or 'none'} | Deploying to: {target_name}[/cyan]")
        
        # CHECKPOINT 1: Check for cancellation before backup
        if self._check_cancel_flag():
            self.console.print("[yellow]üõë Deployment cancelled by user (before backup)[/yellow]")
            self._current_deployment_container = None
            # Clean up any orphaned backup containers
            self._cleanup_backup_containers()
            return False
        
        # Backup OUTSIDE Progress context to avoid "Only one live display" error
        backup_path = None
        if active_container and not skip_backup:
            self._update_progress('backup', 5, 'üíæ Creating data backup...')
            self.console.print(f"[cyan]üíæ Backing up container data...[/cyan]")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = f"backup_{config.container_name}_pre_deploy_{timestamp}"
            
            # Try to reuse existing backup first (reuse_existing=True by default, max age 24 hours)
            if self.backup_container_data(active_container.name, backup_path, reuse_existing=True, max_backup_age_hours=24):
                self._update_progress('backup', 20, '‚úÖ Backup completed')
                # Check if backup was reused or newly created
                backup_dir = Path(backup_path)
                if backup_dir.exists() and (backup_dir / 'backup_metadata.json').exists():
                    try:
                        with open(backup_dir / 'backup_metadata.json', 'r') as f:
                            metadata = json.load(f)
                        backup_time_str = metadata.get('backup_time', '')
                        if backup_time_str:
                            backup_time = datetime.fromisoformat(backup_time_str.replace('Z', '+00:00'))
                            if backup_time.tzinfo is None:
                                backup_time = backup_time.replace(tzinfo=datetime.now().astimezone().tzinfo)
                            age_seconds = (datetime.now(backup_time.tzinfo) - backup_time).total_seconds()
                            if age_seconds > 60:  # More than 1 minute old, it was reused
                                self.console.print(f"[green]üíæ Using existing backup: {backup_path}[/green]")
                            else:
                                self.console.print(f"[green]üíæ Data backup saved to: {backup_path}[/green]")
                    except:
                        self.console.print(f"[green]üíæ Data backup saved to: {backup_path}[/green]")
                else:
                    self.console.print(f"[green]üíæ Using existing backup[/green]")
            else:
                self.console.print(f"[yellow]‚ö†Ô∏è Data backup failed, but continuing deployment...[/yellow]")
                self.logger.warning("Data backup failed before deployment - this is risky for production!")
        elif skip_backup and active_container:
            self.console.print("[yellow]‚ö†Ô∏è Skipping data backup (--skip-backup flag)[/yellow]")
        else:
            self.console.print("[cyan]‚ÑπÔ∏è No active container to backup[/cyan]")
        
        # CHECKPOINT 2: Check for cancellation after backup
        if self._check_cancel_flag():
            self.console.print("[yellow]üõë Deployment cancelled by user (after backup)[/yellow]")
            self._current_deployment_container = None
            # Clean up any orphaned backup containers
            self._cleanup_backup_containers()
            return False
        
        # Clean up any orphaned backup containers from previous interrupted deployments
        # This ensures we start with a clean state
        self._cleanup_backup_containers()
        
        with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}")) as progress:
            
            # Add backup status to progress display
            if backup_path:
                progress.add_task(f"‚úÖ Data backed up to {backup_path}", total=None)
            
            # Build or pull image
            build_task = progress.add_task("üî® Preparing image...", total=None)
            try:
                success, message = self._prepare_image(config.image_tag, build_config)
                if not success:
                    progress.update(build_task, description=f"‚ùå {message}")
                    self.console.print(f"[bold red]‚ùå {message}[/bold red]")
                    return False
                progress.update(build_task, description=f"‚úÖ {message}")
            except Exception as e:
                progress.update(build_task, description="‚ùå Image preparation failed")
                self.logger.error(f"Image preparation failed: {e}")
                self.console.print(f"[bold red]‚ùå Image preparation failed: {e}[/bold red]")
                return False
            
            # Clean up existing target container
            cleanup_task = progress.add_task(f"üßπ Cleaning up {target_name} slot...", total=None)
            try:
                old_target = self.client.containers.get(target_container_name)
                old_target.stop()
                old_target.remove()
            except docker.errors.NotFound:
                pass
            progress.update(cleanup_task, description=f"‚úÖ {target_name.title()} slot cleaned")
            
            # Deploy to target slot
            self._update_progress('deploy', 50, f'üöÄ Deploying to slot {target_name}...')
            deploy_task = progress.add_task(f"üöÄ Deploying to {target_name} slot...", total=None)
            
            # Prepare container creation parameters
            normalized_volumes = self._normalize_volumes(config.volumes)
            self.logger.debug(f"Normalized volumes: {normalized_volumes}")
            
            container_kwargs = {
                'image': config.image_tag,
                'name': target_container_name,
                'detach': True,
                'environment': config.environment,
                'volumes': normalized_volumes,
                'restart_policy': {"Name": config.restart_policy},
            }
            
            # Handle network mode
            if config.network == 'host':
                # With host network, ports are directly mapped - no port mapping needed
                container_kwargs['network_mode'] = 'host'
                # For host network, we can't use different ports for testing
                # So we need to stop the active container first to free up the port
                if active_container:
                    # Stop the active container to free up the port for host network
                    container_to_stop_name = active_container.name
                    self.console.print(f"[yellow]Stopping active container '{container_to_stop_name}' to free port for host network...[/yellow]")
                    try:
                        active_container.stop(timeout=10)
                        self.console.print(f"[green]Active container stopped[/green]")
                        # Wait a moment for port to be released
                        time.sleep(2)
                    except Exception as e:
                        self.logger.warning(f"Failed to stop active container: {e}")
                        # Try to continue anyway - the new container might fail with port conflict
                temp_port_mapping = None
            else:
                # Use different port for parallel testing when not using host network
                temp_port_mapping = None  # Initialize before conditional
                if config.port_mapping and len(config.port_mapping) > 0:
                    temp_port_mapping = {}
                    for container_port, host_port in config.port_mapping.items():
                        temp_port_mapping[container_port] = str(int(host_port) + 1000)  # +1000 for temp
                    container_kwargs['ports'] = temp_port_mapping
                if config.network and config.network != 'bridge':
                    container_kwargs['network'] = config.network
            
            # Add resource limits
            container_kwargs.update(self._get_resource_limits(config))
            
            # Add privileged mode if requested (needed for DB2 with bind mounts to support setuid)
            # Also auto-detect for infrastructure containers (minikube, kubernetes, etc.)
            requires_privileged = False
            if hasattr(config, 'privileged') and config.privileged:
                requires_privileged = True
            else:
                # Auto-detect infrastructure containers that require privileged mode
                image_lower = config.image_tag.lower()
                infrastructure_containers = ['minikube', 'kicbase', 'kubernetes', 'k8s', 'kind', 'k3s', 'k3d']
                for infra_container in infrastructure_containers:
                    if infra_container in image_lower:
                        requires_privileged = True
                        self.logger.info(f"Auto-detected infrastructure container requiring privileged mode: {infra_container}")
                        break
                
                # Also check if active container has privileged mode enabled
                if active_container:
                    try:
                        active_privileged = active_container.attrs.get('HostConfig', {}).get('Privileged', False)
                        if active_privileged:
                            requires_privileged = True
                            self.logger.info(f"Active container has privileged mode enabled, copying to new container")
                    except Exception as e:
                        self.logger.debug(f"Could not check active container privileged mode: {e}")
            
            if requires_privileged:
                container_kwargs['privileged'] = True
                self.logger.info(f"Container {target_container_name} will run in privileged mode")
            
            # Add command if provided in config (for images that exit immediately without command)
            if hasattr(config, 'command') and config.command:
                container_kwargs['command'] = config.command
            elif 'alpine' in config.image_tag.lower():
                # Alpine needs a command to stay running
                container_kwargs['command'] = ['sh', '-c', 'sleep 3600']
            
            try:
                target_container = self.client.containers.run(**container_kwargs)
                
                progress.update(deploy_task, description=f"‚úÖ {target_name.title()} container deployed")
                self._update_progress('deploy', 60, f'‚úÖ Kontener {target_name} wdro≈ºony')
                
                # Longer startup grace period for databases and services with slow startup
                startup_grace = 5
                db_config = self._get_database_config(config.image_tag)
                if db_config:
                    startup_grace = db_config.get('startup_grace_period', 15)
                    db_name = self._get_database_name(config.image_tag) or 'database'
                    self.logger.info(f"Extended startup grace period: {startup_grace}s for {db_name} service")
                
                time.sleep(startup_grace)
                
                # CHECKPOINT 3: Check for cancellation after container creation
                if self._check_cancel_flag():
                    self.console.print("[yellow]üõë Deployment cancelled by user (after container creation)[/yellow]")
                    # Cleanup new container
                    try:
                        target_container.stop()
                        target_container.remove()
                    except:
                        pass
                    self._current_deployment_container = None
                    # Clean up any orphaned backup containers
                    self._cleanup_backup_containers()
                    return False
                
                # Migrate data from active container to new container
                if active_container and active_container.status == "running":
                    migrate_task = progress.add_task("üì¶ Migrating data to new container...", total=None)
                    try:
                        migration_success = self._migrate_container_data(active_container, target_container, config)
                        if migration_success:
                            progress.update(migrate_task, description="‚úÖ Data migration completed")
                        else:
                            progress.update(migrate_task, description="‚ö†Ô∏è Data migration had issues (continuing...)")
                            self.logger.warning("Data migration completed with warnings, continuing deployment")
                    except Exception as e:
                        self.logger.error(f"Data migration failed: {e}")
                        progress.update(migrate_task, description="‚ö†Ô∏è Data migration failed (continuing...)")
                        # Don't fail deployment if migration fails - just log warning
                
            except Exception as e:
                progress.update(deploy_task, description=f"‚ùå {target_name.title()} deployment failed")
                self.logger.error(f"Container creation failed: {e}")
                self.logger.error(f"Container kwargs: {container_kwargs}")
                # Try to get more details about the error
                if hasattr(e, 'explanation'):
                    self.logger.error(f"Error explanation: {e.explanation}")
                return False
            
            # Comprehensive validation of new deployment
            self._update_progress('health_check', 70, f'ü©∫ Checking container health {target_name}...')
            health_task = progress.add_task(f"üîç Comprehensive validation of {target_name} deployment...", total=None)
            
            # Determine port for validation
            validation_port = None
            if config.network == 'host':
                # With host network, use the original port directly
                validation_port = list(config.port_mapping.values())[0] if config.port_mapping else '3000'
            elif 'temp_port_mapping' in locals() and temp_port_mapping and len(temp_port_mapping) > 0:
                validation_port = list(temp_port_mapping.values())[0]
            else:
                validation_port = list(config.port_mapping.values())[0] if config.port_mapping else None
            
            if validation_port:
                # Check if this is a non-HTTP service (endpoint is None)
                if config.health_check_endpoint is None:
                    # Skip HTTP health check for non-HTTP services (SSH, Redis, infrastructure, etc.)
                    progress.update(health_task, description=f"‚ÑπÔ∏è Skipping HTTP health check (non-HTTP service)")
                    self.logger.info(f"Skipping HTTP health check for {target_name} (non-HTTP service)")
                else:
                    # First, do basic health check to ensure service is responding
                    progress.update(health_task, description=f"ü©∫ Basic health check ({target_name})...")
                    
                    # Increase retries for slow-starting services (databases, etc.)
                    health_retries = config.health_check_retries
                    db_config = self._get_database_config(config.image_tag)
                    if db_config:
                        health_retries = max(health_retries, db_config.get('health_check_retries', 20))
                        db_name = self._get_database_name(config.image_tag) or 'database'
                        self.logger.info(f"Extended health check retries: {health_retries} for {db_name} service")
                        
                        # Add extra wait time before validation if configured
                        additional_wait = db_config.get('additional_wait_before_validation', 0)
                        if additional_wait > 0:
                            self.logger.info(f"Waiting additional {additional_wait}s for {db_name} to finish initialization...")
                            time.sleep(additional_wait)
                    
                    if not self._advanced_health_check(
                        validation_port,
                        config.health_check_endpoint,
                        config.health_check_timeout,
                        health_retries
                    ):
                        progress.update(health_task, description=f"‚ùå {target_name.title()} basic health check failed")
                        try:
                            target_container.stop()
                            target_container.remove()
                        except:
                            pass
                        return False
                
                # Then, comprehensive validation
                progress.update(health_task, description=f"üîç Comprehensive validation ({target_name})...")
                is_valid, error_msg = self._comprehensive_container_validation(
                    target_container, config, validation_port, target_name
                )
                
                if not is_valid:
                    progress.update(health_task, description=f"‚ùå {target_name.title()} validation failed")
                    self.logger.error(f"Container validation failed: {error_msg}")
                    
                    # Get container logs for debugging
                    try:
                        logs = target_container.logs(tail=50).decode('utf-8', errors='ignore')
                        self.logger.error(f"Container logs (last 50 lines):\n{logs}")
                    except:
                        pass
                    
                    # Cleanup failed container
                    try:
                        target_container.stop()
                        target_container.remove()
                    except:
                        pass
                    
                    return False
                
                progress.update(health_task, description=f"‚úÖ {target_name.title()} validation passed")
                self._update_progress('health_check', 80, f'‚úÖ Validation of {target_name} completed successfully')
            else:
                self.logger.warning("No ports mapped for validation, skipping comprehensive check")
                progress.update(health_task, description=f"‚ö†Ô∏è {target_name.title()} no ports to validate")
                
                # Still do basic container status check
                try:
                    target_container.reload()
                    if target_container.status != "running":
                        progress.update(health_task, description=f"‚ùå {target_name.title()} container not running")
                        try:
                            target_container.stop()
                            target_container.remove()
                        except:
                            pass
                        return False
                except Exception as e:
                    self.logger.warning(f"Could not verify container status: {e}")
            
            # Parallel testing phase (optional)
            if self._should_run_parallel_tests():
                test_task = progress.add_task("üß™ Running parallel tests...", total=None)
                # Check if container has ports for testing
                has_ports = config.port_mapping and len(config.port_mapping) > 0
                
                if not has_ports:
                    self.logger.warning("No ports mapped, skipping parallel tests")
                    progress.update(test_task, description="‚ö†Ô∏è No ports to test")
                else:
                    # Determine test port based on network mode
                    if config.network == 'host':
                        test_port = list(config.port_mapping.values())[0]
                    elif 'temp_port_mapping' in locals() and temp_port_mapping and len(temp_port_mapping) > 0:
                        test_port = list(temp_port_mapping.values())[0]
                    else:
                        test_port = list(config.port_mapping.values())[0]
                    
                    if not self._run_parallel_tests(test_port, config):
                        progress.update(test_task, description="‚ùå Parallel tests failed")
                        # Cleanup and abort
                        try:
                            target_container.stop()
                            target_container.remove()
                        except:
                            pass
                        return False
                    progress.update(test_task, description="‚úÖ Parallel tests passed")
            
            # CHECKPOINT 4: Check for cancellation before traffic switch
            if self._check_cancel_flag():
                self.console.print("[yellow]üõë Deployment cancelled by user (before traffic switch)[/yellow]")
                # Cleanup target container
                try:
                    target_container.stop()
                    target_container.remove()
                except:
                    pass
                self._current_deployment_container = None
                return False
            
            # Traffic switch with zero-downtime
            self._update_progress('traffic_switch', 90, 'üîÑ Switching traffic (zero-downtime)...')
            switch_task = progress.add_task("üîÑ Zero-downtime traffic switch...", total=None)
            
            try:
                # Stop target container temporarily
                target_container.stop()
                target_container.remove()
                
                # CRITICAL: Stop old container BEFORE creating final container with original ports
                # Otherwise we'll get "port is already allocated" error
                if active_container and active_container.status == "running":
                    self.console.print(f"[cyan]üõë Stopping old container '{active_container.name}' to free ports...[/cyan]")
                    try:
                        active_container.stop(timeout=10)
                        self.console.print(f"[green]‚úÖ Old container '{active_container.name}' stopped[/green]")
                        # Wait for ports to be released
                        time.sleep(2)
                    except Exception as e:
                        self.logger.warning(f"Failed to stop old container: {e}")
                        raise Exception(f"Cannot proceed: failed to stop old container: {e}")
                
                # Create final container with correct configuration
                final_normalized_volumes = self._normalize_volumes(config.volumes)
                self.logger.debug(f"Final normalized volumes: {final_normalized_volumes}")
                
                final_container_kwargs = {
                    'image': config.image_tag,
                    'name': target_container_name,
                    'detach': True,
                    'environment': config.environment,
                    'volumes': final_normalized_volumes,
                    'restart_policy': {"Name": config.restart_policy},
                }
                
                # Handle network and ports
                if config.network == 'host':
                    final_container_kwargs['network_mode'] = 'host'
                else:
                    if config.port_mapping and len(config.port_mapping) > 0:
                        final_container_kwargs['ports'] = config.port_mapping  # Final ports
                    if config.network and config.network != 'bridge':
                        final_container_kwargs['network'] = config.network
                
                # Add resource limits
                final_container_kwargs.update(self._get_resource_limits(config))
                
                # Add privileged mode if requested (needed for DB2 with bind mounts to support setuid)
                # Also auto-detect for infrastructure containers (minikube, kubernetes, etc.)
                requires_privileged = False
                if hasattr(config, 'privileged') and config.privileged:
                    requires_privileged = True
                else:
                    # Auto-detect infrastructure containers that require privileged mode
                    image_lower = config.image_tag.lower()
                    infrastructure_containers = ['minikube', 'kicbase', 'kubernetes', 'k8s', 'kind', 'k3s', 'k3d']
                    for infra_container in infrastructure_containers:
                        if infra_container in image_lower:
                            requires_privileged = True
                            self.logger.info(f"Auto-detected infrastructure container requiring privileged mode: {infra_container}")
                            break
                    
                    # Also check if active container has privileged mode enabled
                    if active_container:
                        try:
                            active_privileged = active_container.attrs.get('HostConfig', {}).get('Privileged', False)
                            if active_privileged:
                                requires_privileged = True
                                self.logger.info(f"Active container has privileged mode enabled, copying to final container")
                        except Exception as e:
                            self.logger.debug(f"Could not check active container privileged mode: {e}")
                
                if requires_privileged:
                    final_container_kwargs['privileged'] = True
                    self.logger.info(f"Final container {target_container_name} will run in privileged mode")
                
                # Add command if provided in config (for images that exit immediately without command)
                if hasattr(config, 'command') and config.command:
                    final_container_kwargs['command'] = config.command
                elif 'alpine' in config.image_tag.lower():
                    # Alpine needs a command to stay running
                    final_container_kwargs['command'] = ['sh', '-c', 'sleep 3600']
                
                # Create final container with retry on port conflict
                # Note: Final container uses the same volumes from config, so data migrated to target_container
                # will be available in final_container since they share the same volume definitions
                try:
                    final_container = self.client.containers.run(**final_container_kwargs)
                    self.logger.info("Final container created with migrated data (shares volumes with target)")
                except Exception as create_error:
                    error_msg = str(create_error)
                    # If port conflict and we have an active container, try to stop it and retry
                    if ('port is already allocated' in error_msg.lower() or 'bind for' in error_msg.lower()) and active_container:
                        self.console.print(f"[yellow]‚ö†Ô∏è Port conflict detected, stopping old container '{active_container.name}' and retrying...[/yellow]")
                        try:
                            # Force stop old container
                            active_container.stop(timeout=5)
                            active_container.remove()
                            time.sleep(3)  # Wait for port to be released
                            # Retry creating final container
                            final_container = self.client.containers.run(**final_container_kwargs)
                            self.console.print(f"[green]‚úÖ Final container created after stopping old container[/green]")
                        except Exception as retry_error:
                            self.logger.error(f"Failed to stop old container and retry: {retry_error}")
                            raise Exception(f"Port conflict: {error_msg}. Failed to resolve by stopping old container: {retry_error}")
                    else:
                        raise
                
                # Wait for final container to be ready
                time.sleep(3)
                
                # Final comprehensive validation before traffic switch
                # Check if container has port mapping for health checks
                has_ports = config.port_mapping and len(config.port_mapping) > 0
                
                if has_ports:
                    if config.network == 'host':
                        # With host network, use the original port directly
                        final_port = list(config.port_mapping.values())[0]
                    else:
                        final_port = list(config.port_mapping.values())[0]
                    
                    # Final health check
                    if not self._advanced_health_check(final_port, config.health_check_endpoint, 10, 5):
                        raise Exception("Final health check failed")
                    
                    # Final comprehensive validation - critical check before traffic switch
                    self.console.print(f"[yellow]üîç Final validation before traffic switch...[/yellow]")
                    is_valid, error_msg = self._comprehensive_container_validation(
                        final_container, config, final_port, target_name
                    )
                else:
                    # No ports - just verify container is running
                    self.console.print(f"[yellow]üîç Final validation before traffic switch (no ports)...[/yellow]")
                    try:
                        final_container.reload()
                        if final_container.status != "running":
                            raise Exception(f"Container is not running: {final_container.status}")
                        is_valid = True
                        error_msg = None
                    except Exception as e:
                        is_valid = False
                        error_msg = str(e)
                
                if not is_valid:
                    error_msg_full = f"Final validation failed before traffic switch: {error_msg}"
                    self.logger.error(error_msg_full)
                    
                    # Get detailed logs
                    try:
                        logs = final_container.logs(tail=100).decode('utf-8', errors='ignore')
                        self.logger.error(f"Final container logs:\n{logs}")
                    except:
                        pass
                    
                    # Cleanup and rollback
                    try:
                        final_container.stop()
                        final_container.remove()
                    except:
                        pass
                    
                    # Restart old container if it exists (rollback)
                    if active_container:
                        try:
                            self.console.print(f"[yellow]üîÑ Rolling back to previous container...[/yellow]")
                            active_container.start()
                            self.console.print(f"[green]‚úÖ Rollback successful - previous container restarted[/green]")
                        except Exception as e:
                            self.logger.error(f"Rollback failed: {e}")
                    
                    raise Exception(error_msg_full)
                
                self.console.print(f"[green]‚úÖ Final validation passed - deployment successful[/green]")
                
                # Old container was already stopped before creating final container
                # Now just remove it if it still exists
                if active_container:
                    try:
                        if active_container.status != 'exited':
                            active_container.stop(timeout=10)
                        active_container.remove()
                        self.console.print(f"[green]‚úÖ Old container '{active_container.name}' removed[/green]")
                    except docker.errors.NotFound:
                        # Already removed, that's fine
                        pass
                    except Exception as e:
                        self.logger.warning(f"Failed to remove old container: {e}")
                
                progress.update(switch_task, description="‚úÖ Traffic switched successfully")
                self._update_progress('traffic_switch', 95, '‚úÖ Traffic switch completed')
                
            except Exception as e:
                progress.update(switch_task, description="‚ùå Traffic switch failed")
                self._update_progress('traffic_switch', 0, f'‚ùå Traffic switch failed: {e}')
                self.logger.error(f"Traffic switch failed: {e}")
                return False
        
        deployment_end = datetime.now()
        duration = deployment_end - deployment_start
        
        self._record_deployment(deployment_id, config, "blue-green", True, duration)
        
        # Clear deployment tracking
        self._current_deployment_container = None
        
        self._update_progress('completed', 100, 'üéâ Deployment completed successfully!')
        self.console.print(f"\n[bold green]üéâ BLUE-GREEN DEPLOYMENT COMPLETED![/bold green]")
        self.console.print(f"[green]Active slot: {target_name}[/green]")
        self.console.print(f"[green]Duration: {duration.total_seconds():.1f}s[/green]")
        
        return True

    def quick_deploy(self, dockerfile_path: str = ".", image_tag: str = None, 
                    container_name: str = None, port_mapping: dict = None, 
                    environment: dict = None, volumes: dict = None,
                    yaml_config: str = None, cleanup_old_image: bool = True) -> bool:
        """
        Quick deployment: build -> stop old -> remove old container -> remove old image -> run new
        
        Args:
            dockerfile_path: Path to directory containing Dockerfile
            image_tag: Tag for the new image (e.g., 'myapp:v1.2')
            container_name: Name of the container
            port_mapping: Port mapping dict (e.g., {'80': '8080'})
            environment: Environment variables dict
            volumes: Volume mapping dict
            yaml_config: Optional path to YAML config file for container settings
            cleanup_old_image: Whether to remove old image after deployment
        
        Returns:
            bool: True if deployment successful
        """
        self.console.print(f"\n[bold cyan]‚ö° QUICK DEPLOY STARTED[/bold cyan]")
        
        deployment_start = datetime.now()
        old_image_id = None
        
        # Load configuration from YAML if provided
        if yaml_config and Path(yaml_config).exists():
            try:
                with open(yaml_config, 'r') as f:
                    config = yaml.safe_load(f)
                
                # Override with YAML settings if not explicitly provided
                image_tag = image_tag or config.get('image_tag')
                container_name = container_name or config.get('container_name')
                port_mapping = port_mapping or config.get('port_mapping', {})
                environment = environment or config.get('environment', {})
                volumes = volumes or config.get('volumes', {})
                
                self.console.print(f"[cyan]‚úì Loaded configuration from {yaml_config}[/cyan]")
            except Exception as e:
                self.logger.error(f"Failed to load YAML config: {e}")
                self.console.print(f"[yellow]‚ö†Ô∏è Could not load YAML config, using provided parameters[/yellow]")
        
        # Validate required parameters
        if not image_tag or not container_name:
            self.console.print("[red]‚ùå image_tag and container_name are required[/red]")
            return False
        
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=self.console
        ) as progress:
            
            # Step 1: Get old container info (for image cleanup)
            check_task = progress.add_task("üîç Checking existing deployment...", total=None)
            try:
                old_container = self.client.containers.get(container_name)
                old_image_id = old_container.image.id
                old_image_tags = old_container.image.tags
                progress.update(check_task, description=f"‚úÖ Found existing container (image: {old_image_tags[0] if old_image_tags else old_image_id[:12]})")
            except docker.errors.NotFound:
                progress.update(check_task, description="‚ÑπÔ∏è No existing container (first deployment)")
                old_image_id = None
            
            # Step 2: Build new image
            build_task = progress.add_task(f"üî® Building image {image_tag}...", total=None)
            try:
                dockerfile = Path(dockerfile_path) / "Dockerfile"
                if not dockerfile.exists():
                    progress.update(build_task, description="‚ùå Dockerfile not found")
                    self.console.print(f"[red]‚ùå Dockerfile not found at {dockerfile}[/red]")
                    return False
                
                image, build_logs = self.client.images.build(
                    path=dockerfile_path,
                    tag=image_tag,
                    rm=True,
                    pull=True
                )
                progress.update(build_task, description=f"‚úÖ Image {image_tag} built successfully")
                
            except docker.errors.BuildError as e:
                progress.update(build_task, description="‚ùå Build failed")
                self.logger.error(f"Build error: {e}")
                for log in e.build_log:
                    if 'stream' in log:
                        self.console.print(f"[red]{log['stream']}[/red]", end="")
                return False
            except Exception as e:
                progress.update(build_task, description="‚ùå Build failed")
                self.logger.error(f"Unexpected build error: {e}")
                return False
            
            # Step 3: Stop old container
            stop_task = progress.add_task("üõë Stopping old container...", total=None)
            try:
                old_container = self.client.containers.get(container_name)
                if old_container.status == "running":
                    old_container.stop(timeout=10)
                    progress.update(stop_task, description="‚úÖ Old container stopped")
                else:
                    progress.update(stop_task, description="‚ÑπÔ∏è Container was not running")
            except docker.errors.NotFound:
                progress.update(stop_task, description="‚ÑπÔ∏è No container to stop")
            except Exception as e:
                progress.update(stop_task, description="‚ùå Failed to stop container")
                self.logger.error(f"Stop failed: {e}")
                return False
            
            # Step 4: Remove old container
            remove_task = progress.add_task("üóëÔ∏è Removing old container...", total=None)
            try:
                old_container = self.client.containers.get(container_name)
                old_container.remove()
                progress.update(remove_task, description="‚úÖ Old container removed")
            except docker.errors.NotFound:
                progress.update(remove_task, description="‚ÑπÔ∏è No container to remove")
            except Exception as e:
                progress.update(remove_task, description="‚ùå Failed to remove container")
                self.logger.error(f"Remove failed: {e}")
                # Continue anyway
            
            # Step 5: Remove old image (if requested and exists)
            if cleanup_old_image and old_image_id:
                cleanup_task = progress.add_task("üßπ Cleaning up old image...", total=None)
                try:
                    # Check if old image is different from new one
                    new_image = self.client.images.get(image_tag)
                    if old_image_id != new_image.id:
                        # Check if any other containers are using the old image
                        containers_using_image = self.client.containers.list(
                            all=True,
                            filters={"ancestor": old_image_id}
                        )
                        
                        if len(containers_using_image) == 0:
                            try:
                                self.client.images.remove(old_image_id, force=False)
                                progress.update(cleanup_task, description="‚úÖ Old image removed")
                            except docker.errors.ImageNotFound:
                                progress.update(cleanup_task, description="‚ÑπÔ∏è Old image already removed")
                            except docker.errors.APIError as e:
                                if "image is being used" in str(e).lower():
                                    progress.update(cleanup_task, description="‚ö†Ô∏è Old image in use by other containers")
                                else:
                                    progress.update(cleanup_task, description=f"‚ö†Ô∏è Could not remove old image: {str(e)[:50]}")
                        else:
                            progress.update(cleanup_task, description=f"‚ö†Ô∏è Old image used by {len(containers_using_image)} other container(s)")
                    else:
                        progress.update(cleanup_task, description="‚ÑπÔ∏è Same image, no cleanup needed")
                except Exception as e:
                    progress.update(cleanup_task, description="‚ö†Ô∏è Image cleanup skipped")
                    self.logger.warning(f"Image cleanup failed: {e}")
            
            # Step 6: Run new container
            run_task = progress.add_task("üöÄ Starting new container...", total=None)
            try:
                new_container = self.client.containers.run(
                    image=image_tag,
                    name=container_name,
                    detach=True,
                    ports=port_mapping,
                    environment=environment,
                    volumes=volumes,
                    restart_policy={"Name": "unless-stopped"}
                )
                progress.update(run_task, description="‚úÖ New container started")
                
                # Grace period for startup
                time.sleep(3)
                
            except Exception as e:
                progress.update(run_task, description="‚ùå Failed to start container")
                self.logger.error(f"Container start failed: {e}")
                return False
            
            # Step 7: Optional health check
            if port_mapping:
                health_task = progress.add_task("ü©∫ Health check...", total=None)
                host_port = list(port_mapping.values())[0]
                
                if self._advanced_health_check(host_port, "/", timeout=10, max_retries=3):
                    progress.update(health_task, description="‚úÖ Health check passed")
                else:
                    progress.update(health_task, description="‚ö†Ô∏è Health check failed (container still running)")
                    self.logger.warning("Health check failed but deployment completed")
        
        # Deployment summary
        deployment_end = datetime.now()
        duration = deployment_end - deployment_start
        
        self.console.print(f"\n[bold green]üéâ QUICK DEPLOY COMPLETED SUCCESSFULLY![/bold green]")
        self.console.print(f"[green]Duration: {duration.total_seconds():.1f}s[/green]")
        self.console.print(f"[green]Container: {container_name}[/green]")
        self.console.print(f"[green]Image: {image_tag}[/green]")
        
        if port_mapping:
            for container_port, host_port in port_mapping.items():
                self.console.print(f"[green]Available at: http://localhost:{host_port}[/green]")
        
        # Record deployment
        self._record_deployment(
            f"quick_{int(deployment_start.timestamp())}",
            DeploymentConfig(
                image_tag=image_tag,
                container_name=container_name,
                port_mapping=port_mapping or {},
                environment=environment or {},
                volumes=volumes or {}
            ),
            "quick",
            True,
            duration
        )
        
        return True

    def _canary_deploy(self, config: DeploymentConfig, build_config: dict) -> bool:
        """Canary deployment with gradual traffic shifting"""
        self.console.print(f"\n[bold cyan]üê§ CANARY DEPLOYMENT STARTED[/bold cyan]")
        
        # This would require a load balancer integration
        # For now, we'll implement a simplified version
        
        deployment_start = datetime.now()
        
        with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}")) as progress:
            
            # Prepare image
            build_task = progress.add_task("üî® Preparing canary image...", total=None)
            try:
                success, message = self._prepare_image(config.image_tag, build_config)
                if not success:
                    progress.update(build_task, description=f"‚ùå {message}")
                    self.console.print(f"[bold red]‚ùå {message}[/bold red]")
                    return False
                progress.update(build_task, description=f"‚úÖ {message}")
            except Exception as e:
                progress.update(build_task, description="‚ùå Image preparation failed")
                self.logger.error(f"Image preparation failed: {e}")
                self.console.print(f"[bold red]‚ùå Image preparation failed: {e}[/bold red]")
                return False
            
            # Deploy canary container (5% traffic simulation)
            canary_name = f"{config.container_name}_canary"
            canary_task = progress.add_task("üöÄ Deploying canary (5% traffic)...", total=None)
            
            # Use different port for canary
            canary_port_mapping = {}
            for container_port, host_port in config.port_mapping.items():
                canary_port_mapping[container_port] = str(int(host_port) + 100)
            
            try:
                # Clean existing canary
                try:
                    old_canary = self.client.containers.get(canary_name)
                    old_canary.stop()
                    old_canary.remove()
                except docker.errors.NotFound:
                    pass
                
                canary_container = self.client.containers.run(
                    image=config.image_tag,
                    name=canary_name,
                    detach=True,
                    ports=canary_port_mapping,
                    environment={**config.environment, "CANARY": "true"},
                    volumes=self._normalize_volumes(config.volumes),
                    restart_policy={"Name": config.restart_policy},
                    **self._get_resource_limits(config)
                )
                
                progress.update(canary_task, description="‚úÖ Canary deployed")
                time.sleep(5)
                
            except Exception as e:
                progress.update(canary_task, description="‚ùå Canary deployment failed")
                return False
            
            # Monitor canary
            monitor_task = progress.add_task("üìä Monitoring canary performance...", total=None)
            
            canary_port = list(canary_port_mapping.values())[0]
            if not self._monitor_canary_performance(canary_port, duration=30):
                progress.update(monitor_task, description="‚ùå Canary monitoring failed")
                # Cleanup canary
                try:
                    canary_container.stop()
                    canary_container.remove()
                except:
                    pass
                return False
            
            progress.update(monitor_task, description="‚úÖ Canary performance acceptable")
            
            # Promote canary to full deployment
            promote_task = progress.add_task("‚¨ÜÔ∏è Promoting canary to full deployment...", total=None)
            
            try:
                # Stop main container
                try:
                    main_container = self.client.containers.get(config.container_name)
                    main_container.stop()
                    main_container.remove()
                except docker.errors.NotFound:
                    pass
                
                # Stop canary and redeploy as main
                canary_container.stop()
                canary_container.remove()
                
                # Deploy as main container
                main_container = self.client.containers.run(
                    image=config.image_tag,
                    name=config.container_name,
                    detach=True,
                    ports=config.port_mapping,
                    environment=config.environment,
                    volumes=self._normalize_volumes(config.volumes),
                    restart_policy={"Name": config.restart_policy},
                    **self._get_resource_limits(config)
                )
                
                progress.update(promote_task, description="‚úÖ Canary promoted successfully")
                
            except Exception as e:
                progress.update(promote_task, description="‚ùå Canary promotion failed")
                return False
        
        deployment_end = datetime.now()
        duration = deployment_end - deployment_start
        
        self._record_deployment(f"canary_{int(deployment_start.timestamp())}", config, "canary", True, duration)
        
        self.console.print(f"\n[bold green]üéâ CANARY DEPLOYMENT COMPLETED![/bold green]")
        self.console.print(f"[green]Duration: {duration.total_seconds():.1f}s[/green]")
        
        return True

    def _prepare_image(self, image_tag: str, build_config: dict = None):
        """Prepare image for deployment - check if exists, pull, or build.
        
        Args:
            image_tag: Docker image tag to prepare
            build_config: Optional build configuration dict
        
        Returns:
            tuple: (success: bool, message: str)
        """
        # Check if image already exists locally
        try:
            self.client.images.get(image_tag)
            self.logger.info(f"Image {image_tag} already exists locally")
            return True, "Image already exists"
        except docker.errors.ImageNotFound:
            pass
        
        # If build_config is provided and has dockerfile_path, try to build
        if build_config and build_config.get('dockerfile_path'):
            build_success = self._build_image_enhanced(image_tag, build_config)
            if build_success:
                return True, "Image built successfully"
            # If build failed, try to pull as fallback
            self.logger.warning(f"Build failed, trying to pull image {image_tag}")
        
        # Try to pull image from registry
        try:
            self.logger.info(f"Pulling image {image_tag} from registry...")
            self.client.images.pull(image_tag)
            return True, "Image pulled successfully"
        except Exception as pull_error:
            error_msg = f"Failed to pull image {image_tag}: {pull_error}"
            self.logger.error(error_msg)
            return False, error_msg

    def _build_image_enhanced(self, image_tag: str, build_config: dict) -> bool:
        """Enhanced image building with advanced features"""
        dockerfile_path = build_config.get('dockerfile_path', '.')
        context = build_config.get('context', '.')
        no_cache = build_config.get('no_cache', False)
        pull = build_config.get('pull', True)
        build_args = build_config.get('build_args', {})
        
        try:
            # Validate Dockerfile exists
            dockerfile = Path(dockerfile_path) / "Dockerfile"
            if not dockerfile.exists():
                self.console.print(f"[bold red]‚ùå Dockerfile not found at {dockerfile}[/bold red]")
                return False
            
            # Build with enhanced logging
            self.logger.info(f"Building image {image_tag} from {dockerfile_path}")
            
            build_kwargs = {
                'path': context,
                'tag': image_tag,
                'rm': True,
                'nocache': no_cache,
                'pull': pull,
                'buildargs': build_args
            }
            
            # Show loading indicator during build
            with self._with_loading("Building image"):
                image, build_logs = self.client.images.build(**build_kwargs)
            
            # Process build logs
            for log in build_logs:
                if 'stream' in log:
                    # Filter out verbose output for cleaner display
                    stream = log['stream'].strip()
                    if stream and not stream.startswith('Step'):
                        continue  # Only show steps in production
                
            return True
            
        except docker.errors.BuildError as e:
            self.logger.error(f"Build error: {e}")
            for log in e.build_log:
                if 'stream' in log:
                    self.console.print(f"[red]{log['stream']}[/red]", end="")
            return False
        except Exception as e:
            self.logger.error(f"Unexpected build error: {e}")
            return False
        
    def build_image_standalone(self, dockerfile_path: str, tag: str, no_cache: bool = False, pull: bool = True) -> bool:
        """Standalone image building function"""
        build_config = {
            'dockerfile_path': dockerfile_path,
            'context': dockerfile_path,
            'no_cache': no_cache,
            'pull': pull,
            'build_args': {}
        }

        self.console.print(f"[cyan]Building image {tag} from {dockerfile_path}...[/cyan]")

        success = self._build_image_enhanced(tag, build_config)

        if success:
            self.console.print(f"[green]‚úÖ Image {tag} built successfully[/green]")
        else:
            self.console.print(f"[red]‚ùå Failed to build image {tag}[/red]")

        return success

    def _detect_health_check_endpoint(self, image_tag: str) -> str:
        """Detect appropriate health check endpoint based on image name
        
        Uses configuration from:
        1. User config (self.config from YAML) - highest priority
        2. Default config file (health-checks-defaults.json) - fallback
        
        Returns:
            str: Health check endpoint path, or None for non-HTTP services
        """
        image_lower = image_tag.lower()
        
        # Load defaults from JSON file
        defaults = self._load_health_check_defaults()
        default_health_checks = defaults.get('health_checks', {})
        
        # User config overrides defaults
        user_health_checks = self.config.get('health_checks', {})
        
        # Merge: user config takes precedence over defaults
        non_http_services = user_health_checks.get(
            'non_http_services',
            default_health_checks.get('non_http_services', [])
        )
        
        endpoint_mappings = {
            **default_health_checks.get('endpoint_mappings', {}),  # Defaults first
            **user_health_checks.get('endpoint_mappings', {})      # User overrides
        }
        
        default_endpoint = user_health_checks.get(
            'default_endpoint',
            default_health_checks.get('default_endpoint', '/health')
        )
        
        # Check for non-HTTP services
        for service in non_http_services:
            if service in image_lower:
                self.logger.info(f"Detected non-HTTP service ({service}) - skipping HTTP health check")
                return None
        
        # Additional hardcoded non-HTTP services (infrastructure containers)
        infrastructure_services = ['minikube', 'kicbase', 'kubernetes', 'k8s', 'kind', 'k3s', 'k3d']
        for infra_service in infrastructure_services:
            if infra_service in image_lower:
                self.logger.info(f"Detected infrastructure service ({infra_service}) - skipping HTTP health check")
                return None
        
        # Try to find matching endpoint mapping
        for image_pattern, endpoint in endpoint_mappings.items():
            if image_pattern.lower() in image_lower:
                self.logger.info(f"Detected image pattern '{image_pattern}' -> endpoint '{endpoint}'")
                return endpoint
        
        # Use default endpoint
        self.logger.info(f"Using default health check endpoint: {default_endpoint}")
        return default_endpoint
    
    def _advanced_health_check(self, port: str, endpoint: str, timeout: int, max_retries: int) -> bool:
        """Advanced health check with detailed reporting
        
        Returns True if health check passes or if endpoint is None (skip check)
        """
        # Skip health check if endpoint is None (for non-HTTP services like SSH, Redis, etc.)
        if endpoint is None:
            self.logger.info("Skipping HTTP health check (non-HTTP service)")
            return True
        
        url = f"http://localhost:{port}{endpoint}"
        
        for attempt in range(max_retries):
            try:
                start_time = time.time()
                # Use longer timeout for first attempts (service may be starting)
                request_timeout = 10 if attempt < 3 else 5
                response = requests.get(url, timeout=request_timeout)
                response_time = time.time() - start_time
                
                # Accept 200-299 status codes as successful health checks
                if 200 <= response.status_code < 300:
                    self.logger.info(f"Health check passed (attempt {attempt + 1}): {response_time:.2f}s (status {response.status_code})")
                    return True
                else:
                    self.logger.warning(f"Health check returned {response.status_code} (attempt {attempt + 1})")
                    
            except requests.exceptions.RequestException as e:
                self.logger.warning(f"Health check failed (attempt {attempt + 1}): {e}")
            
            if attempt < max_retries - 1:
                # Longer wait between retries for first attempts
                wait_time = 5 if attempt < 3 else 3
                time.sleep(wait_time)
        
        return False

    def _comprehensive_container_validation(self, container, config: DeploymentConfig, 
                                          port: str, target_name: str) -> tuple:
        """
        Comprehensive validation of container before traffic switch.
        Returns (is_valid, error_message)
        
        Checks:
        1. Container status (must be running, not restarting)
        2. Health check endpoint response
        3. Container logs for errors
        4. Resource usage (CPU, memory)
        5. Volumes mounting
        6. Port availability
        7. Response time
        8. No crash loops
        """
        validation_errors = []
        
        # 1. Check container status
        try:
            container.reload()  # Refresh container state
            status = container.status
            
            if status != "running":
                validation_errors.append(f"Container status is '{status}', expected 'running'")
                return False, "; ".join(validation_errors)
            
            # Check restart count - be more lenient for databases that may restart during initialization
            restart_count = container.attrs.get('RestartCount', 0) if hasattr(container, 'attrs') else 0
            
            # Detect if this is a database service and get its configuration
            db_config = self._get_database_config(config.image_tag)
            is_database = len(db_config) > 0
            
            # Set restart threshold based on service type (from config or default)
            max_restarts = db_config.get('max_restart_count', 15) if is_database else 3
            
            if restart_count > max_restarts:
                validation_errors.append(f"Container has restarted {restart_count} times (possible crash loop, max allowed: {max_restarts})")
                return False, "; ".join(validation_errors)
            
            # For databases with high restart count, check if container is stable now
            # Wait a bit and check if it's still running (not restarting)
            if is_database and restart_count > 5:
                self.logger.info(f"Database container has {restart_count} restarts, checking stability...")
                time.sleep(5)  # Wait 5 seconds
                try:
                    container.reload()
                    if container.status != "running":
                        validation_errors.append(f"Container not stable after {restart_count} restarts (current status: {container.status})")
                        return False, "; ".join(validation_errors)
                    # Check if restart count increased (still restarting)
                    new_restart_count = container.attrs.get('RestartCount', 0) if hasattr(container, 'attrs') else 0
                    if new_restart_count > restart_count:
                        validation_errors.append(f"Container still restarting (restart count increased from {restart_count} to {new_restart_count})")
                        return False, "; ".join(validation_errors)
                    self.logger.info(f"Container appears stable after {restart_count} restarts")
                except Exception as e:
                    self.logger.warning(f"Could not verify container stability: {e}")
                
        except Exception as e:
            validation_errors.append(f"Failed to check container status: {e}")
            return False, "; ".join(validation_errors)
        
        # 2. Health check endpoint (skip for non-HTTP services)
        health_check_passed = False
        response_time = None
        
        if config.health_check_endpoint is None:
            # Non-HTTP service (SSH, Redis, etc.) - skip HTTP health check
            self.logger.info("Skipping HTTP health check for non-HTTP service")
            health_check_passed = True
        else:
            try:
                url = f"http://localhost:{port}{config.health_check_endpoint}"
                start_time = time.time()
                response = requests.get(url, timeout=10)
                response_time = time.time() - start_time
                
                if 200 <= response.status_code < 300:
                    health_check_passed = True
                    self.logger.info(f"Health check passed: {response_time:.2f}s response time")
                else:
                    validation_errors.append(f"Health check returned status {response.status_code}, expected 200-299")
            except requests.exceptions.Timeout:
                validation_errors.append(f"Health check timeout after 10s")
            except requests.exceptions.ConnectionError:
                validation_errors.append(f"Health check connection error - service may not be ready")
            except Exception as e:
                validation_errors.append(f"Health check failed: {e}")
            
            if not health_check_passed:
                return False, "; ".join(validation_errors)
        
        # 3. Check container logs for critical errors
        try:
            logs = container.logs(tail=100).decode('utf-8', errors='ignore')
            
            # For database services, check for normal initialization patterns
            db_config = self._get_database_config(config.image_tag)
            if db_config:
                log_patterns = db_config.get('log_patterns', {})
                loading_patterns = log_patterns.get('loading_shards', []) + log_patterns.get('loading', [])
                
                if loading_patterns:
                    for pattern in loading_patterns:
                        if pattern.lower() in logs.lower():
                            db_name = self._get_database_name(config.image_tag) or 'database'
                            self.logger.info(f"{db_name} is loading data - this is normal and may take time")
                            break
            
            # Common error patterns
            error_patterns = [
                'FATAL', 'CRITICAL', 'panic', 'segmentation fault',
                'out of memory', 'cannot bind', 'address already in use',
                'permission denied', 'access denied', 'failed to start'
            ]
            
            # Special handling for OOM - only treat as error if it's actual OOM, not OOM detection warnings
            # cadvisor and similar tools may log "OOM detection" warnings which are safe to ignore
            oom_safe_patterns = [
                'oom detection', 'configure.*oom', 'disabling oom', 'no oom',
                'could not configure.*oom', 'unable to configure.*oom'
            ]
            
            found_errors = []
            logs_lower = logs.lower()
            
            for pattern in error_patterns:
                if pattern.lower() in logs_lower:
                    found_errors.append(pattern)
            
            # Check for OOM separately - only flag if it's not a safe OOM detection warning
            if 'oom' in logs_lower:
                is_safe_oom = any(safe_pattern in logs_lower for safe_pattern in oom_safe_patterns)
                if not is_safe_oom:
                    # Check if it's actually an OOM error (not just detection-related)
                    oom_error_patterns = ['killed.*oom', 'out of memory', 'oom killer', 'memory limit exceeded']
                    if any(oom_err in logs_lower for oom_err in oom_error_patterns):
                        found_errors.append('OOM')
                else:
                    # It's a safe OOM detection warning, log but don't treat as error
                    self.logger.info("Found safe OOM detection warning (not a critical error)")
            
            if found_errors:
                validation_errors.append(f"Found critical errors in logs: {', '.join(found_errors)}")
                # Don't fail immediately, but log it
                self.logger.warning(f"Warning: Found error patterns in logs: {', '.join(found_errors)}")
        except Exception as e:
            self.logger.warning(f"Could not check container logs: {e}")
        
        # 4. Check resource usage (CPU, memory)
        try:
            stats = container.stats(stream=False)
            
            # Check memory usage
            if 'memory_stats' in stats:
                mem_usage = stats['memory_stats'].get('usage', 0)
                mem_limit = stats['memory_stats'].get('limit', 1)
                
                if mem_limit > 0:
                    mem_percent = (mem_usage / mem_limit) * 100.0
                    if mem_percent > 95:
                        validation_errors.append(f"Memory usage critical: {mem_percent:.1f}%")
                    elif mem_percent > 80:
                        self.logger.warning(f"Memory usage high: {mem_percent:.1f}%")
            
            # Check CPU usage (if available)
            if 'cpu_stats' in stats and 'precpu_stats' in stats:
                # CPU calculation would require more complex logic
                # For now, just check if stats are available
                pass
                
        except Exception as e:
            self.logger.warning(f"Could not check resource usage: {e}")
        
        # 5. Check volumes mounting
        try:
            mounts = container.attrs.get('Mounts', [])
            if config.volumes:
                expected_volumes = len(config.volumes) if isinstance(config.volumes, (dict, list)) else 0
                if len(mounts) < expected_volumes:
                    validation_errors.append(f"Expected {expected_volumes} volume(s), found {len(mounts)}")
        except Exception as e:
            self.logger.warning(f"Could not verify volumes: {e}")
        
        # 6. Check response time (should be reasonable)
        if response_time:
            if response_time > 5.0:
                validation_errors.append(f"Health check response time too slow: {response_time:.2f}s")
            elif response_time > 2.0:
                self.logger.warning(f"Health check response time slow: {response_time:.2f}s")
        
        # 7. Wait a bit and check again to ensure stability
        time.sleep(2)
        try:
            container.reload()
            if container.status != "running":
                validation_errors.append(f"Container status changed to '{container.status}' after validation")
                return False, "; ".join(validation_errors)
        except Exception as e:
            self.logger.warning(f"Could not re-check container status: {e}")
        
        # If we have critical errors, fail validation
        if validation_errors:
            return False, "; ".join(validation_errors)
        
        return True, "All validations passed"

    def _get_resource_limits(self, config: DeploymentConfig) -> dict:
        """Convert resource limits to Docker API format"""
        limits = {}
        
        if config.cpu_limit:
            # Convert CPU limit (e.g., "1.5" -> 1500000000 nanoseconds)
            try:
                cpu_limit = float(config.cpu_limit) * 1000000000
                limits['nano_cpus'] = int(cpu_limit)
            except:
                pass
        
        if config.memory_limit:
            # Convert memory limit (e.g., "1g" -> bytes)
            try:
                memory_str = config.memory_limit.lower()
                if memory_str.endswith('g'):
                    memory_bytes = int(float(memory_str[:-1]) * 1024 * 1024 * 1024)
                elif memory_str.endswith('m'):
                    memory_bytes = int(float(memory_str[:-1]) * 1024 * 1024)
                else:
                    memory_bytes = int(memory_str)
                
                limits['mem_limit'] = memory_bytes
            except:
                pass
        
        return limits

    def _normalize_volumes(self, volumes: Dict[str, str]) -> list:
        """Convert volumes from config format to Docker API format.
        
        Docker Python API's containers.run() expects volumes as a list of strings
        in the format: ['/host/path:/container/path', 'volume_name:/container/path']
        
        Supports:
        - Named volumes: 'volume_name': '/container/path' -> ['volume_name:/container/path']
        - Bind mounts: '/host/path': '/container/path' -> ['/host/path:/container/path']
        - Already formatted as list: ['volume:/path'] -> unchanged
        - Already formatted as dict with bind/mode: {'/host': {'bind': '/container', 'mode': 'rw'}} -> ['/host:/container:rw']
        """
        if not volumes:
            return []
        
        # Handle case where volumes might already be a list (from previous normalization)
        if isinstance(volumes, list):
            return volumes
        
        # Handle case where volumes might not be a dict (defensive programming)
        if not isinstance(volumes, dict):
            self.logger.warning(f"Volumes is not a dict or list, got {type(volumes)}: {volumes}")
            return []
        
        normalized = []
        for key, value in volumes.items():
            if isinstance(value, dict):
                # Already in correct format with bind and mode
                # Format: {'/host/path': {'bind': '/container/path', 'mode': 'rw'}}
                if 'bind' in value:
                    bind_path = value['bind']
                    mode = value.get('mode', 'rw')
                    normalized.append(f"{key}:{bind_path}:{mode}")
                else:
                    self.logger.warning(f"Volume dict for '{key}' missing 'bind', skipping: {value}")
            elif isinstance(value, str):
                # Check if it's a named volume (doesn't start with / or ./)
                # Named volumes don't have leading slash in Docker
                if not key.startswith('/') and not key.startswith('./') and not key.startswith('../'):
                    # Named volume: format as 'volume_name:/container/path'
                    normalized.append(f"{key}:{value}")
                else:
                    # Bind mount: format as '/host/path:/container/path'
                    normalized.append(f"{key}:{value}")
            else:
                # Unknown format, log warning and skip
                self.logger.warning(f"Unknown volume format for key '{key}': {type(value)} - {value}")
        
        return normalized

    def _should_run_parallel_tests(self) -> bool:
        """Determine if parallel tests should be run"""
        return self.config.get('testing', {}).get('parallel_tests_enabled', False)

    def _run_parallel_tests(self, port: str, config: DeploymentConfig) -> bool:
        """Run parallel tests against new deployment"""
        test_config = self.config.get('testing', {})
        test_endpoints = test_config.get('endpoints', ['/health'])
        
        base_url = f"http://localhost:{port}"
        
        for endpoint in test_endpoints:
            try:
                url = f"{base_url}{endpoint}"
                response = requests.get(url, timeout=5)
                
                if response.status_code != 200:
                    self.logger.error(f"Parallel test failed for {endpoint}: {response.status_code}")
                    return False
                    
            except Exception as e:
                self.logger.error(f"Parallel test error for {endpoint}: {e}")
                return False
        
        return True

    def _monitor_canary_performance(self, port: str, duration: int) -> bool:
        """Monitor canary deployment performance"""
        start_time = time.time()
        error_count = 0
        total_requests = 0
        
        while time.time() - start_time < duration:
            try:
                response = requests.get(f"http://localhost:{port}/health", timeout=2)
                total_requests += 1
                
                if response.status_code != 200:
                    error_count += 1
                
                # Stop if error rate is too high (>10%)
                if total_requests > 10 and (error_count / total_requests) > 0.1:
                    self.logger.error(f"Canary error rate too high: {error_count}/{total_requests}")
                    return False
                    
            except:
                error_count += 1
                total_requests += 1
            
            time.sleep(1)
        
        error_rate = error_count / total_requests if total_requests > 0 else 0
        self.logger.info(f"Canary monitoring complete: {error_count}/{total_requests} errors ({error_rate:.2%})")
        
        return error_rate < 0.05  # Accept if error rate < 5%

    def _record_deployment(self, deployment_id: str, config: DeploymentConfig, 
                          deployment_type: str, success: bool, duration: timedelta, target_env: str = None):
        """Record deployment in history"""
        deployment_record = {
            'id': deployment_id,
            'timestamp': datetime.now().isoformat(),
            'type': deployment_type,
            'image_tag': config.image_tag,
            'container_name': config.container_name,
            'success': success,
            'duration_seconds': duration.total_seconds()
        }
        
        # Add environment information if provided
        if target_env:
            deployment_record['environment'] = target_env
        
        self.deployment_history.append(deployment_record)
        
        # Save to file
        try:
            history_file = "deployment_history.json"
            history_data = []
            
            if Path(history_file).exists():
                with open(history_file, 'r') as f:
                    history_data = json.load(f)
            
            history_data.append(deployment_record)
            
            # Keep only last 100 deployments
            if len(history_data) > 100:
                history_data = history_data[-100:]
            
            with open(history_file, 'w') as f:
                json.dump(history_data, f, indent=2)
                
        except Exception as e:
            self.logger.error(f"Failed to save deployment history: {e}")

    def show_deployment_history(self, limit: int = 10):
        """Show deployment history"""
        history_file = "deployment_history.json"
        
        if not Path(history_file).exists():
            self.console.print("[yellow]‚ö†Ô∏è No deployment history found[/yellow]")
            return
        
        try:
            with open(history_file, 'r') as f:
                history_data = json.load(f)
            
            # Sort by timestamp, most recent first
            history_data.sort(key=lambda x: x['timestamp'], reverse=True)
            history_data = history_data[:limit]
            
            table = Table(title="üöÄ Deployment History", show_header=True)
            table.add_column("Date", style="cyan")
            table.add_column("ID", style="blue")
            table.add_column("Type", style="magenta")
            table.add_column("Image", style="yellow")
            table.add_column("Container", style="green")
            table.add_column("Status", style="bold")
            table.add_column("Duration", style="bright_blue")
            
            for record in history_data:
                timestamp = datetime.fromisoformat(record['timestamp']).strftime('%Y-%m-%d %H:%M')
                status = "[green]‚úÖ Success[/green]" if record['success'] else "[red]‚ùå Failed[/red]"
                duration = f"{record['duration_seconds']:.1f}s"
                
                table.add_row(
                    timestamp,
                    record['id'][:12],
                    record['type'],
                    record['image_tag'],
                    record['container_name'],
                    status,
                    duration
                )
            
            self.console.print(table)
            
        except Exception as e:
            self.logger.error(f"Failed to load deployment history: {e}")
            self.console.print(f"[red]‚ùå Error loading deployment history: {e}[/red]")

    # ==================== CLI INTERFACE ====================

    def create_cli_parser(self) -> argparse.ArgumentParser:
        """Create comprehensive CLI parser"""
        # Import version from __init__
        try:
            from . import __version__
        except ImportError:
            __version__ = "Enhanced"
        
        parser = argparse.ArgumentParser(
            description="Docker Pilot Enhanced - Professional Docker Management Tool",
            formatter_class=argparse.RawDescriptionHelpFormatter
        )
        
        parser.add_argument('--version', action='version', version=f'DockerPilot {__version__}')
        parser.add_argument('--config', '-c', type=str, help='Configuration file path')
        parser.add_argument('--log-level', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], 
                          default='INFO', help='Logging level')
        
        subparsers = parser.add_subparsers(dest='command', help='Available commands')
        
        # Container operations
        container_parser = subparsers.add_parser('container', help='Container operations')
        container_subparsers = container_parser.add_subparsers(dest='container_action')
        
        # List containers
        list_parser = container_subparsers.add_parser('list', help='List containers')
        list_parser.add_argument('--all', '-a', action='store_true', help='Show all containers')
        list_parser.add_argument('--format', choices=['table', 'json'], default='table')

       
        # List images
        images_parser = container_subparsers.add_parser('list-images', help='List Docker images')
        images_parser.add_argument('--all', '-a', action='store_true', help='Show all images')
        images_parser.add_argument('--format', choices=['table', 'json'], default='table')
        images_parser.add_argument('--hide-untagged', action='store_true', help='Hide images without tags (dangling images)')

        # Remove image
        remove_img_parser = container_subparsers.add_parser('remove-image', help='Remove Docker image(s)')
        remove_img_parser.add_argument('name', help='Image name(s) or ID(s), comma-separated (e.g., image1:tag,image2:tag)')
        remove_img_parser.add_argument('--force', '-f', action='store_true', help='Force removal')

        # Prune dangling images
        prune_img_parser = container_subparsers.add_parser('prune-images', help='Remove all dangling images (images without tags)')
        prune_img_parser.add_argument('--dry-run', action='store_true', help='Show what would be removed without actually removing')

        
        # Container actions
        for action in ['start', 'stop', 'restart', 'remove', 'pause', 'unpause']:
            action_parser = container_subparsers.add_parser(action, help=f'{action.title()} container(s)')
            action_parser.add_argument('name', help='Container name(s) or ID(s), comma-separated (e.g., app1,app2 or id1,id2)')
            if action in ['stop', 'restart']:
                action_parser.add_argument('--timeout', '-t', type=int, default=10, help='Timeout seconds')
            if action == 'remove':
                action_parser.add_argument('--force', '-f', action='store_true', help='Force removal')
        
        # Stop and remove in one operation
        stop_remove_parser = container_subparsers.add_parser('stop-remove', help='Stop and remove container(s) in one operation')
        stop_remove_parser.add_argument('name', help='Container name(s) or ID(s), comma-separated')
        stop_remove_parser.add_argument('--timeout', '-t', type=int, default=10, help='Timeout seconds')
        
        # Run new container
        run_parser = container_subparsers.add_parser('run', help='Run a new container from image')
        run_parser.add_argument('image', nargs='?', help='Docker image name/tag (e.g., nginx:latest)')
        run_parser.add_argument('--name', '-n', help='Container name')
        run_parser.add_argument('--port', '-p', action='append', help='Port mapping (format: container:host, e.g., 80:8080). Can be used multiple times')
        run_parser.add_argument('--env', '-e', action='append', help='Environment variable (format: KEY=VALUE). Can be used multiple times')
        run_parser.add_argument('--volume', '-v', action='append', help='Volume mapping (format: host:container or host:container:mode). Can be used multiple times')
        run_parser.add_argument('--command', '-c', help='Command to run in container')
        run_parser.add_argument('--restart', default='unless-stopped', choices=['no', 'on-failure', 'always', 'unless-stopped'], help='Restart policy')
        run_parser.add_argument('--network', help='Network name or "host" for host network')
        run_parser.add_argument('--privileged', action='store_true', help='Run container in privileged mode')
        run_parser.add_argument('--cpu-limit', help='CPU limit (e.g., 1.5 for 1.5 CPUs)')
        run_parser.add_argument('--memory-limit', '-m', help='Memory limit (e.g., 1g for 1GB, 512m for 512MB)')
        run_parser.add_argument('--interactive', '--more', '-i', action='store_true', help='Interactive mode: ask for all parameters one by one')
        
        # Exec non-interactive
        exec_simple_parser = container_subparsers.add_parser('exec-simple', help='Execute command non-interactively')
        exec_simple_parser.add_argument('name', help='Container name or ID')
        exec_simple_parser.add_argument('command', help='Command to execute (e.g., "ls -la")')
        
        # Exec into container
        exec_parser = container_subparsers.add_parser('exec', help='Execute interactive command in container(s)')
        exec_parser.add_argument('name', help='Container name(s) or ID(s), comma-separated (e.g., app1,app2)')
        exec_parser.add_argument('--command', '-c', default='/bin/bash', help='Command to execute (default: /bin/bash)')
        
        # View container logs
        logs_parser = container_subparsers.add_parser('logs', help='View container logs')
        logs_parser.add_argument('name', nargs='?', help='Container name(s) or ID(s), comma-separated (e.g., app1,app2)')
        logs_parser.add_argument('--tail', '-n', type=int, default=50, help='Number of lines to show (default: 50)')
        
        # Monitoring
        monitor_parser = subparsers.add_parser('monitor', help='Container monitoring')
        monitor_subparsers = monitor_parser.add_subparsers(dest='monitor_action')
        
        # Dashboard monitoring
        dashboard_parser = monitor_subparsers.add_parser('dashboard', help='Multi-container dashboard')
        dashboard_parser.add_argument('containers', nargs='*', help='Container names (empty for all running)')
        dashboard_parser.add_argument('--duration', '-d', type=int, default=300, help='Monitor duration in seconds')
        
        # Live monitoring (with screen clearing)
        live_parser = monitor_subparsers.add_parser('live', help='Live monitoring with screen clearing')
        live_parser.add_argument('container', help='Container name')
        live_parser.add_argument('--duration', '-d', type=int, default=30, help='Monitor duration in seconds')
        
        # One-time stats
        stats_parser = monitor_subparsers.add_parser('stats', help='Get one-time container statistics')
        stats_parser.add_argument('container', help='Container name')
        
        # Health check standalone
        health_parser = monitor_subparsers.add_parser('health', help='Test health check endpoint')
        health_parser.add_argument('port', type=int, help='Port number')
        health_parser.add_argument('--endpoint', '-e', default='/health', help='Health check endpoint')
        health_parser.add_argument('--retries', '-r', type=int, default=10, help='Maximum retries')
        
        # Deployment
        deploy_parser = subparsers.add_parser('deploy', help='Deployment operations')
        deploy_subparsers = deploy_parser.add_subparsers(dest='deploy_action')
        
        # Deploy from config
        config_deploy_parser = deploy_subparsers.add_parser('config', help='Deploy from configuration file')
        config_deploy_parser.add_argument('config_file', help='Deployment configuration file')
        config_deploy_parser.add_argument('--type', choices=['rolling', 'blue-green', 'canary'], 
                                        default='rolling', help='Deployment type')
        
        # Create config template
        template_parser = deploy_subparsers.add_parser('init', help='Create deployment configuration template')
        template_parser.add_argument('--output', '-o', default='deployment.yml', help='Output file name')
        
        # Deployment history
        history_parser = deploy_subparsers.add_parser('history', help='Show deployment history')
        history_parser.add_argument('--limit', '-l', type=int, default=10, help='Number of records to show')
        
        # Quick deploy
        quick_deploy_parser = deploy_subparsers.add_parser('quick', help='Quick deployment (build + replace)')
        quick_deploy_parser.add_argument('--dockerfile-path', '-d', default='.', help='Path to Dockerfile directory')
        quick_deploy_parser.add_argument('--image-tag', '-t', required=True, help='Image tag (e.g., myapp:v1.2)')
        quick_deploy_parser.add_argument('--container-name', '-n', required=True, help='Container name')
        quick_deploy_parser.add_argument('--port', '-p', help='Port mapping (format: container:host, e.g., 80:8080)')
        quick_deploy_parser.add_argument('--env', '-e', action='append', help='Environment variable (format: KEY=VALUE)')
        quick_deploy_parser.add_argument('--volume', '-v', action='append', help='Volume mapping (format: host:container)')
        quick_deploy_parser.add_argument('--yaml-config', '-y', help='YAML config file with container settings')
        quick_deploy_parser.add_argument('--no-cleanup', action='store_true', help='Do not remove old image')
        
        # New parsers added # 
        # System validation
        validate_parser = subparsers.add_parser('validate', help='Validate system requirements')

        # Backup operations
        backup_parser = subparsers.add_parser('backup', help='Backup and restore operations')
        backup_subparsers = backup_parser.add_subparsers(dest='backup_action')

        backup_create_parser = backup_subparsers.add_parser('create', help='Create deployment backup')
        backup_create_parser.add_argument('--path', '-p', help='Backup path')

        backup_restore_parser = backup_subparsers.add_parser('restore', help='Restore from backup')
        backup_restore_parser.add_argument('backup_path', help='Path to backup directory')
        
        # Container data backup (actual data from volumes)
        backup_data_parser = backup_subparsers.add_parser('container-data', help='Backup container data (volumes)')
        backup_data_parser.add_argument('container', help='Container name to backup')
        backup_data_parser.add_argument('--path', '-p', help='Backup path (auto-generated if not provided)')
        
        # Container data restore
        restore_data_parser = backup_subparsers.add_parser('restore-data', help='Restore container data from backup')
        restore_data_parser.add_argument('container', help='Container name to restore data to')
        restore_data_parser.add_argument('backup_path', help='Path to backup directory')

        # Configuration management
        config_parser = subparsers.add_parser('config', help='Configuration management')
        config_subparsers = config_parser.add_subparsers(dest='config_action')

        config_export_parser = config_subparsers.add_parser('export', help='Export configuration')
        config_export_parser.add_argument('--output', '-o', default='docker-pilot-config.tar.gz', help='Output archive name')

        config_import_parser = config_subparsers.add_parser('import', help='Import configuration')
        config_import_parser.add_argument('archive', help='Configuration archive path')

        # CI/CD pipeline
        pipeline_parser = subparsers.add_parser('pipeline', help='CI/CD pipeline operations')
        pipeline_subparsers = pipeline_parser.add_subparsers(dest='pipeline_action')

        pipeline_create_parser = pipeline_subparsers.add_parser('create', help='Create CI/CD pipeline')
        pipeline_create_parser.add_argument('--type', choices=['github', 'gitlab', 'jenkins'], default='github', help='Pipeline type')
        pipeline_create_parser.add_argument('--output', '-o', help='Output path')

        # Integration tests
        test_parser = subparsers.add_parser('test', help='Integration testing')
        test_parser.add_argument('--config', default='integration-tests.yml', help='Test configuration file')

        # Environment promotion
        promote_parser = subparsers.add_parser('promote', help='Environment promotion')
        promote_parser.add_argument('source', help='Source environment')
        promote_parser.add_argument('target', help='Target environment')
        promote_parser.add_argument('--config', help='Deployment configuration path')

        # Monitoring setup
        alerts_parser = subparsers.add_parser('alerts', help='Setup monitoring alerts')
        alerts_parser.add_argument('--config', default='alerts.yml', help='Alert configuration file')

        # Documentation
        docs_parser = subparsers.add_parser('docs', help='Generate documentation')
        docs_parser.add_argument('--output', '-o', default='docs', help='Output directory')

        # Build operations
        build_parser = subparsers.add_parser('build', help='Build Docker image from Dockerfile')
        build_parser.add_argument('dockerfile_path', help='Path to Dockerfile directory')
        build_parser.add_argument('tag', help='Image tag (e.g., myapp:latest)')
        build_parser.add_argument('--no-cache', action='store_true', help='Build without cache')
        build_parser.add_argument('--pull', action='store_true', default=True, help='Pull base image updates')

        # Production checklist
        checklist_parser = subparsers.add_parser('checklist', help='Generate production checklist')
        checklist_parser.add_argument('--output', '-o', default='production-checklist.md', help='Output file')

        return parser

    def run_cli(self):
        """Run CLI interface"""
        # Check if Docker is available before running CLI
        if not self.client or not self.container_manager:
            self.console.print("[bold red]‚ùå Docker is not available![/bold red]")
            self.console.print("[yellow]Please ensure Docker is running and accessible.[/yellow]")
            sys.exit(1)
        
        parser = self.create_cli_parser()
        args = parser.parse_args()
        
        # Check if we have a container action even if command is None (happens with subparsers)
        has_container_action = hasattr(args, 'container_action') and args.container_action is not None
        
        if not args.command and not has_container_action:
            # Interactive mode
            self._run_interactive_menu()
            return
        
        # Execute CLI command
        try:
            if args.command == 'container' or has_container_action:
                self._handle_container_cli(args)
            elif args.command == 'monitor':
                self._handle_monitor_cli(args)
            elif args.command == 'update_restart_policy':
                self.update_restart_policy(args.name, args.policy)
            elif args.command == 'run_image':
                self.run_image(args.image, args.name, args.ports, args.env, args.volumes, args.detach)  
            elif args.command == 'deploy':
                self._handle_deploy_cli(args)
            elif args.command == 'validate':
                success = self.validate_system_requirements()
                if not success:
                    sys.exit(1)
            elif args.command == 'backup':
                self._handle_backup_cli(args)
            elif args.command == 'config':
                self._handle_config_cli(args)
            elif args.command == 'pipeline':
                self._handle_pipeline_cli(args)
            elif args.command == 'test':
                success = self.run_integration_tests(args.config)
                if not success:
                    sys.exit(1)
            elif args.command == 'promote':
                config_path = getattr(args, 'config', None)
                skip_backup = getattr(args, 'skip_backup', False)
                success = self.environment_promotion(args.source, args.target, config_path, skip_backup)
                if not success:
                    sys.exit(1)
            elif args.command == 'alerts':
                success = self.setup_monitoring_alerts(args.config)
                if not success:
                    sys.exit(1)
            elif args.command == 'docs':
                success = self.generate_documentation(args.output)
                if not success:
                    sys.exit(1)
            elif args.command == 'checklist':
                success = self.create_production_checklist(args.output)
                if not success:
                    sys.exit(1)
            elif args.command == 'build':
                success = self.build_image_standalone(args.dockerfile_path, args.tag, args.no_cache, args.pull)
                if not success:
                    sys.exit(1)
            else:
                parser.print_help()
        except Exception as e:
            self.logger.error(f"CLI command failed: {e}")
            self.console.print(f"[red]‚ùå Command failed: {e}[/red]")
            sys.exit(1)

    def _run_container_interactive(self, args):
        """Interactive mode for running containers - asks for all parameters one by one"""
        self.console.print("\n[bold cyan]üöÄ Interactive Container Run Mode[/bold cyan]")
        self.console.print("[dim]Press Enter to use default value or leave empty to skip[/dim]\n")
        
        # Image (required)
        image_name = args.image if args.image else None
        if not image_name:
            image_name = Prompt.ask("Docker image name/tag", default="")
            if not image_name:
                self.console.print("[red]‚ùå Image name is required[/red]")
                sys.exit(1)
        
        # Container name (required)
        container_name = args.name if args.name else None
        if not container_name:
            container_name = Prompt.ask("Container name", default="")
            if not container_name:
                self.console.print("[red]‚ùå Container name is required[/red]")
                sys.exit(1)
        
        # Port mappings
        ports = {}
        if args.port:
            # Parse existing ports
            for port_mapping in args.port:
                if ':' in port_mapping:
                    container_port, host_port = port_mapping.split(':')
                    ports[container_port.strip()] = host_port.strip()
        
        self.console.print("\n[cyan]Port mappings (format: container:host, e.g., 80:8080)[/cyan]")
        while True:
            port_input = Prompt.ask("Port mapping (empty to finish)", default="").strip()
            if not port_input:
                break
            if ':' in port_input:
                try:
                    container_port, host_port = port_input.split(':')
                    ports[container_port.strip()] = host_port.strip()
                    self.console.print(f"[green]‚úì Added port mapping: {container_port} -> {host_port}[/green]")
                except ValueError:
                    self.console.print("[yellow]‚ö†Ô∏è Invalid format. Use container:host[/yellow]")
            else:
                self.console.print("[yellow]‚ö†Ô∏è Invalid format. Use container:host[/yellow]")
        
        # Environment variables
        environment = {}
        if args.env:
            # Parse existing env vars
            for env_var in args.env:
                if '=' in env_var:
                    key, value = env_var.split('=', 1)
                    environment[key.strip()] = value.strip()
        
        self.console.print("\n[cyan]Environment variables (format: KEY=VALUE)[/cyan]")
        while True:
            env_input = Prompt.ask("Environment variable (empty to finish)", default="").strip()
            if not env_input:
                break
            if '=' in env_input:
                key, value = env_input.split('=', 1)
                environment[key.strip()] = value.strip()
                self.console.print(f"[green]‚úì Added environment variable: {key}[/green]")
            else:
                self.console.print("[yellow]‚ö†Ô∏è Invalid format. Use KEY=VALUE[/yellow]")
        
        # Volumes
        volumes = {}
        if args.volume:
            # Parse existing volumes
            for volume_mapping in args.volume:
                if ':' in volume_mapping:
                    parts = volume_mapping.split(':')
                    if len(parts) == 2:
                        host_path, container_path = parts
                        volumes[host_path.strip()] = container_path.strip()
                    elif len(parts) == 3:
                        host_path, container_path, mode = parts
                        volumes[host_path.strip()] = {
                            'bind': container_path.strip(),
                            'mode': mode.strip()
                        }
        
        self.console.print("\n[cyan]Volume mappings (format: host:container or host:container:mode)[/cyan]")
        while True:
            vol_input = Prompt.ask("Volume mapping (empty to finish)", default="").strip()
            if not vol_input:
                break
            if ':' in vol_input:
                parts = vol_input.split(':')
                if len(parts) == 2:
                    host_path, container_path = parts
                    volumes[host_path.strip()] = container_path.strip()
                    self.console.print(f"[green]‚úì Added volume: {host_path} -> {container_path}[/green]")
                elif len(parts) == 3:
                    host_path, container_path, mode = parts
                    volumes[host_path.strip()] = {
                        'bind': container_path.strip(),
                        'mode': mode.strip()
                    }
                    self.console.print(f"[green]‚úì Added volume: {host_path} -> {container_path} ({mode})[/green]")
                else:
                    self.console.print("[yellow]‚ö†Ô∏è Invalid format. Use host:container or host:container:mode[/yellow]")
            else:
                self.console.print("[yellow]‚ö†Ô∏è Invalid format. Use host:container[/yellow]")
        
        # Command
        command = args.command if args.command else None
        if not command:
            command = Prompt.ask("Command to run (empty for default)", default="").strip()
            command = command if command else None
        
        # Restart policy
        restart_policy = args.restart if args.restart else 'unless-stopped'
        restart_policy = Prompt.ask("Restart policy", default=restart_policy, choices=['no', 'on-failure', 'always', 'unless-stopped'])
        
        # Network
        network = args.network if args.network else None
        if not network:
            network = Prompt.ask("Network name (or 'host' for host network, empty for default)", default="").strip()
            network = network if network else None
        
        # Privileged mode
        privileged = args.privileged if args.privileged else False
        if not privileged:
            privileged = Confirm.ask("Run in privileged mode?", default=False)
        
        # CPU limit
        cpu_limit = args.cpu_limit if args.cpu_limit else None
        if not cpu_limit:
            cpu_input = Prompt.ask("CPU limit (e.g., 1.5 for 1.5 CPUs, empty to skip)", default="").strip()
            cpu_limit = cpu_input if cpu_input and cpu_input.lower() not in ['n', 'no'] else None
        
        # Memory limit
        memory_limit = args.memory_limit if args.memory_limit else None
        if not memory_limit:
            memory_input = Prompt.ask("Memory limit (e.g., 1g for 1GB, 512m for 512MB, empty to skip)", default="").strip()
            memory_limit = memory_input if memory_input and memory_input.lower() not in ['n', 'no'] else None
        
        # Summary
        self.console.print("\n[bold cyan]üìã Configuration Summary:[/bold cyan]")
        self.console.print(f"  Image: {image_name}")
        self.console.print(f"  Container name: {container_name}")
        if ports:
            self.console.print(f"  Ports: {ports}")
        if environment:
            self.console.print(f"  Environment variables: {len(environment)} set")
        if volumes:
            self.console.print(f"  Volumes: {len(volumes)} mounted")
        if command:
            self.console.print(f"  Command: {command}")
        self.console.print(f"  Restart policy: {restart_policy}")
        if network:
            self.console.print(f"  Network: {network}")
        if privileged:
            self.console.print(f"  Privileged mode: enabled")
        if cpu_limit:
            self.console.print(f"  CPU limit: {cpu_limit}")
        if memory_limit:
            self.console.print(f"  Memory limit: {memory_limit}")
        
        # Confirm
        if not Confirm.ask("\n[bold]Proceed with container creation?[/bold]", default=True):
            self.console.print("[yellow]‚ùå Cancelled by user[/yellow]")
            sys.exit(0)
        
        # Run container
        success = self.run_new_container(
            image_name=image_name,
            name=container_name,
            ports=ports if ports else None,
            command=command,
            environment=environment if environment else None,
            volumes=volumes if volumes else None,
            restart_policy=restart_policy,
            network=network,
            privileged=privileged,
            cpu_limit=cpu_limit,
            memory_limit=memory_limit
        )
        if not success:
            sys.exit(1)
    
    def _handle_container_cli(self, args):
        """Handle container CLI commands with support for multiple targets"""
        if args.container_action == 'list':
            self.list_containers(show_all=args.all, format_output=args.format)
        elif args.container_action == 'stop-remove':
            # Stop and remove in one operation
            containers = self._parse_multi_target(args.name)
            
            if not containers:
                self.console.print("[red]‚ùå No container names provided[/red]")
                sys.exit(1)
            
            timeout = args.timeout if hasattr(args, 'timeout') else 10
            
            all_success = True
            for container in containers:
                self.console.print(f"\n[cyan]Processing container: {container}[/cyan]")
                success = self.stop_and_remove_container(container, timeout)
                if not success:
                    all_success = False
            
            if not all_success:
                self.console.print("\n[yellow]‚ö†Ô∏è Some operations failed[/yellow]")
                sys.exit(1)
            else:
                self.console.print("\n[green]‚úÖ All operations completed successfully[/green]")
        
        elif args.container_action == 'exec-simple':
            # Non-interactive exec
            success = self.exec_command_non_interactive(args.name, args.command)
            if not success:
                sys.exit(1)
        
        elif args.container_action in ['start', 'stop', 'restart', 'remove', 'pause', 'unpause']:
            # Parse multiple container names/IDs
            containers = self._parse_multi_target(args.name)
            
            if not containers:
                self.console.print("[red]‚ùå No container names provided[/red]")
                sys.exit(1)
            
            kwargs = {}
            if hasattr(args, 'timeout'):
                kwargs['timeout'] = args.timeout
            if hasattr(args, 'force'):
                kwargs['force'] = args.force
            
            # Execute operation on each container
            all_success = True
            for container in containers:
                self.console.print(f"\n[cyan]Processing container: {container}[/cyan]")
                success = self.container_operation(args.container_action, container, **kwargs)
                if not success:
                    all_success = False
            
            if not all_success:
                self.console.print("\n[yellow]‚ö†Ô∏è Some operations failed[/yellow]")
                sys.exit(1)
            else:
                self.console.print("\n[green]‚úÖ All operations completed successfully[/green]")
                
        elif args.container_action == 'exec':
            # Parse multiple container names/IDs
            containers = self._parse_multi_target(args.name)
            
            if not containers:
                self.console.print("[red]‚ùå No container names provided[/red]")
                sys.exit(1)
            
            command = args.command if hasattr(args, 'command') else '/bin/bash'
            
            # Execute command in each container sequentially
            for container in containers:
                self.console.print(f"\n[cyan]Executing in container: {container}[/cyan]")
                success = self.exec_container(container, command)
                if not success:
                    self.console.print(f"[yellow]‚ö†Ô∏è Failed to exec in {container}, continuing...[/yellow]")
        
        elif args.container_action == 'logs':
            tail = args.tail if hasattr(args, 'tail') else 50
            if args.name:
                self.view_container_logs(args.name, tail)
            else:
                # Interactive mode when no container name provided
                self.view_container_logs(None, tail)
                    
        elif args.container_action == 'run':
            # Check if interactive mode or missing required parameters
            interactive = getattr(args, 'interactive', False)
            missing_required = not args.image or not args.name
            
            if interactive or missing_required:
                # Interactive mode: ask for all parameters
                self._run_container_interactive(args)
            else:
                # Non-interactive mode: use provided arguments
                ports = {}
                if hasattr(args, 'port') and args.port:
                    for port_mapping in args.port:
                        try:
                            if ':' in port_mapping:
                                container_port, host_port = port_mapping.split(':')
                                ports[container_port.strip()] = host_port.strip()
                            else:
                                self.console.print(f"[yellow]Invalid port format: {port_mapping}. Use container:host[/yellow]")
                        except ValueError:
                            self.console.print(f"[yellow]Invalid port format: {port_mapping}[/yellow]")
                
                # Parse environment variables
                environment = {}
                if hasattr(args, 'env') and args.env:
                    for env_var in args.env:
                        if '=' in env_var:
                            key, value = env_var.split('=', 1)
                            environment[key.strip()] = value.strip()
                        else:
                            self.console.print(f"[yellow]Invalid env format: {env_var}. Use KEY=VALUE[/yellow]")
                
                # Parse volumes
                volumes = {}
                if hasattr(args, 'volume') and args.volume:
                    for volume_mapping in args.volume:
                        if ':' in volume_mapping:
                            parts = volume_mapping.split(':')
                            if len(parts) == 2:
                                # Simple format: host:container
                                host_path, container_path = parts
                                volumes[host_path.strip()] = container_path.strip()
                            elif len(parts) == 3:
                                # Format with mode: host:container:mode
                                host_path, container_path, mode = parts
                                volumes[host_path.strip()] = {
                                    'bind': container_path.strip(),
                                    'mode': mode.strip()
                                }
                            else:
                                self.console.print(f"[yellow]Invalid volume format: {volume_mapping}[/yellow]")
                        else:
                            self.console.print(f"[yellow]Invalid volume format: {volume_mapping}[/yellow]")
                
                # Run container
                success = self.run_new_container(
                    image_name=args.image,
                    name=args.name,
                    ports=ports if ports else None,
                    command=getattr(args, 'command', None),
                    environment=environment if environment else None,
                    volumes=volumes if volumes else None,
                    restart_policy=getattr(args, 'restart', 'unless-stopped'),
                    network=getattr(args, 'network', None),
                    privileged=getattr(args, 'privileged', False),
                    cpu_limit=getattr(args, 'cpu_limit', None),
                    memory_limit=getattr(args, 'memory_limit', None)
                )
                if not success:
                    sys.exit(1)
        elif args.container_action == 'list-images':
            hide_untagged = getattr(args, 'hide_untagged', False)
            self.list_images(show_all=args.all, format_output=args.format, hide_untagged=hide_untagged)
        elif args.container_action == 'remove-image':
            # Parse multiple image names/IDs
            images = self._parse_multi_target(args.name)
            
            if not images:
                self.console.print("[red]‚ùå No image names provided[/red]")
                sys.exit(1)
            
            # Remove each image
            all_success = True
            for image in images:
                self.console.print(f"\n[cyan]Processing image: {image}[/cyan]")
                success = self.remove_image(image, args.force)
                if not success:
                    all_success = False
            
            if not all_success:
                self.console.print("\n[yellow]‚ö†Ô∏è Some operations failed[/yellow]")
                sys.exit(1)
            else:
                self.console.print("\n[green]‚úÖ All operations completed successfully[/green]")
        
        elif args.container_action == 'prune-images':
            dry_run = getattr(args, 'dry_run', False)
            result = self.prune_dangling_images(dry_run=dry_run)
            
            if not dry_run and result['images_deleted'] > 0:
                self.console.print(f"\n[green]‚úÖ Cleanup completed! Removed {result['images_deleted']} images[/green]")
            elif dry_run:
                if result['images_deleted'] > 0:
                    self.console.print(f"\n[cyan]‚ÑπÔ∏è Use without --dry-run to actually remove {result['images_deleted']} images[/cyan]")
                else:
                    self.console.print("\n[yellow]‚ÑπÔ∏è No dangling images to remove[/yellow]")
            else:
                if result['images_deleted'] == 0:
                    self.console.print("\n[yellow]‚ÑπÔ∏è No dangling images were removed[/yellow]")

    def _handle_monitor_cli(self, args):
        """Handle monitoring CLI commands"""
        if args.monitor_action == 'dashboard' or not args.monitor_action:
            # Default to dashboard if no action specified (backward compatibility)
            containers = args.containers if hasattr(args, 'containers') and args.containers else None
            duration = args.duration if hasattr(args, 'duration') else 300
            self.monitor_containers_dashboard(containers, duration)
        
        elif args.monitor_action == 'live':
            # Live monitoring with screen clearing
            success = self.monitor_container_live(args.container, args.duration)
            if not success:
                sys.exit(1)
        
        elif args.monitor_action == 'stats':
            # One-time stats
            success = self.get_container_stats_once(args.container)
            if not success:
                sys.exit(1)
        
        elif args.monitor_action == 'health':
            # Health check
            success = self.health_check_standalone(
                args.port, 
                args.endpoint, 
                timeout=30,
                max_retries=args.retries
            )
            if not success:
                sys.exit(1)

    def _handle_deploy_cli(self, args):
        """Handle deployment CLI commands"""
        if args.deploy_action == 'config':
            success = self.deploy_from_config(args.config_file, args.type)
            if not success:
                sys.exit(1)
        elif args.deploy_action == 'init':
            output = getattr(args, 'output', 'deployment.yml')
            success = self.create_deployment_config(output)
            if not success:
                sys.exit(1)
        elif args.deploy_action == 'history':
            self.show_deployment_history(limit=getattr(args, 'limit', 10))
        elif args.deploy_action == 'quick':
            # Parse port mapping
            port_mapping = None
            if args.port:
                try:
                    container_port, host_port = args.port.split(':')
                    port_mapping = {container_port: host_port}
                except ValueError:
                    self.console.print("[red]Invalid port format. Use container:host (e.g., 80:8080)[/red]")
                    sys.exit(1)
            
            # Parse environment variables
            environment = {}
            if args.env:
                for env_var in args.env:
                    try:
                        key, value = env_var.split('=', 1)
                        environment[key] = value
                    except ValueError:
                        self.console.print(f"[red]Invalid env format: {env_var}. Use KEY=VALUE[/red]")
                        sys.exit(1)
            
            # Parse volumes
            volumes = {}
            if args.volume:
                for volume in args.volume:
                    try:
                        host_path, container_path = volume.split(':')
                        volumes[host_path] = {'bind': container_path, 'mode': 'rw'}
                    except ValueError:
                        self.console.print(f"[red]Invalid volume format: {volume}. Use host:container[/red]")
                        sys.exit(1)
            
            success = self.quick_deploy(
                dockerfile_path=args.dockerfile_path,
                image_tag=args.image_tag,
                container_name=args.container_name,
                port_mapping=port_mapping,
                environment=environment if environment else None,
                volumes=volumes if volumes else None,
                yaml_config=args.yaml_config,
                cleanup_old_image=not args.no_cleanup
            )
            
            if not success:
                sys.exit(1)
        else:
            self.console.print("[yellow]‚ö†Ô∏è Unknown deploy action[/yellow]")

    def _handle_backup_cli(self, args):
        """Handle backup CLI commands"""
        if args.backup_action == 'create':
            backup_path = getattr(args, 'path', None)
            success = self.backup_deployment_state(backup_path)
            if not success:
                sys.exit(1)
        elif args.backup_action == 'restore':
            success = self.restore_deployment_state(args.backup_path)
            if not success:
                sys.exit(1)
        elif args.backup_action == 'container-data':
            backup_path = getattr(args, 'path', None)
            success = self.backup_container_data(args.container, backup_path)
            if not success:
                sys.exit(1)
        elif args.backup_action == 'restore-data':
            success = self.restore_container_data(args.container, args.backup_path)
            if not success:
                sys.exit(1)
        else:
            self.console.print("[yellow]‚ö†Ô∏è Unknown backup action[/yellow]")

    def _handle_config_cli(self, args):
        """Handle configuration CLI commands"""
        if args.config_action == 'export':
            success = self.export_configuration(args.output)
            if not success:
                sys.exit(1)
        elif args.config_action == 'import':
            success = self.import_configuration(args.archive)
            if not success:
                sys.exit(1)
        else:
            self.console.print("[yellow]‚ö†Ô∏è Unknown config action[/yellow]")

    def _handle_pipeline_cli(self, args):
        """Handle pipeline CLI commands"""
        if args.pipeline_action == 'create':
            success = self.create_pipeline_config(args.type, args.output)
            if not success:
                sys.exit(1)
        else:
            self.console.print("[yellow]‚ö†Ô∏è Unknown pipeline action[/yellow]")


    def _run_interactive_menu(self):
        """Simple interactive menu for quick operations"""
        try:
            while True:
                choice = Prompt.ask(
                    "\n[bold cyan]Docker Pilot - Interactive Menu[/bold cyan]\n"
                    "Container: list, list-img, start, stop, restart, remove, pause, unpause, stop-remove, exec, exec-simple, policy, run_image, logs, remove-image, prune-images, json, build\n"
                    "Monitor: monitor, live-monitor, stats, health-check\n"
                    "Deploy: quick-deploy, deploy-init, deploy-config, history, promote\n"
                    "System: validate, backup-create, backup-restore, alerts, test, pipeline, docs, checklist\n"
                    "Config: export-config, import-config\n"
                    "Select",
                    default="list"
                ).strip().lower()

                if choice == "exit":
                    self.console.print("[green]Bye![/green]")
                    break

                if choice == "list":
                    self.list_containers(show_all=True, format_output="table")

                elif choice == "list-img":
                    hide_untagged = Confirm.ask("Hide untagged images (dangling)?", default=False)
                    self.list_images(show_all=True, format_output="table", hide_untagged=hide_untagged)

                elif choice in ("start", "stop", "restart", "remove", "pause", "unpause"):
                    self.list_containers()
                    names_input = Prompt.ask("Container name(s) or ID(s) (comma-separated for multiple, e.g., app1,app2)")
                    containers = self._parse_multi_target(names_input)
                    
                    if not containers:
                        self.console.print("[red]No container names provided[/red]")
                        continue
                    
                    kwargs = {}
                    if choice in ("stop", "restart"):
                        kwargs['timeout'] = int(Prompt.ask("Timeout seconds", default="10"))
                    if choice == "remove":
                        kwargs['force'] = Confirm.ask("Force removal?", default=False)
                    
                    # Execute operation on each container
                    all_success = True
                    for container in containers:
                        self.console.print(f"\n[cyan]Processing container: {container}[/cyan]")
                        success = self.container_operation(choice, container, **kwargs)
                        if not success:
                            all_success = False
                    
                    if not all_success:
                        self.console.print(f"[yellow]‚ö†Ô∏è Some operations failed[/yellow]")
                    else:
                        self.console.print(f"[green]‚úÖ All operations completed successfully[/green]")

                elif choice == "exec":
                    self.list_containers()
                    names_input = Prompt.ask("Container name(s) or ID(s) (comma-separated for multiple, e.g., app1,app2)")
                    containers = self._parse_multi_target(names_input)
                    
                    if not containers:
                        self.console.print("[red]No container names provided[/red]")
                        continue
                    
                    command = Prompt.ask("Command to execute", default="/bin/bash")
                    
                    # Execute command in each container sequentially
                    for container in containers:
                        self.console.print(f"\n[cyan]Executing in container: {container}[/cyan]")
                        success = self.exec_container(container, command)
                        if not success:
                            self.console.print(f"[yellow]‚ö†Ô∏è Failed to exec in {container}, continuing...[/yellow]")

                elif choice == "stop-remove":
                    self.list_containers()
                    names_input = Prompt.ask("Container name(s) or ID(s) (comma-separated for multiple)")
                    containers = self._parse_multi_target(names_input)
                    
                    if not containers:
                        self.console.print("[red]No container names provided[/red]")
                        continue
                    
                    timeout = int(Prompt.ask("Timeout seconds", default="10"))
                    
                    all_success = True
                    for container in containers:
                        self.console.print(f"\n[cyan]Processing container: {container}[/cyan]")
                        success = self.stop_and_remove_container(container, timeout)
                        if not success:
                            all_success = False
                    
                    if not all_success:
                        self.console.print(f"[yellow]‚ö†Ô∏è Some operations failed[/yellow]")
                    else:
                        self.console.print(f"[green]‚úÖ All operations completed successfully[/green]")
                
                elif choice == "exec-simple":
                    self.list_containers()
                    container_name = Prompt.ask("Container name or ID")
                    command = Prompt.ask("Command to execute (e.g., 'ls -la')")
                    self.exec_command_non_interactive(container_name, command)
                
                elif choice == "monitor":
                    self.list_containers()
                    containers_input = Prompt.ask("Containers (comma separated, empty = all running)", default="").strip()
                    containers = [c.strip() for c in containers_input.split(",")] if containers_input else None
                    duration = int(Prompt.ask("Duration seconds", default="60"))
                    self.monitor_containers_dashboard(containers, duration)
                
                elif choice == "live-monitor":
                    self.list_containers()
                    container_name = Prompt.ask("Container name")
                    duration = int(Prompt.ask("Duration seconds", default="30"))
                    self.monitor_container_live(container_name, duration)
                
                elif choice == "stats":
                    self.list_containers()
                    container_name = Prompt.ask("Container name")
                    self.get_container_stats_once(container_name)
                
                elif choice == "health-check":
                    port = int(Prompt.ask("Port number"))
                    endpoint = Prompt.ask("Health check endpoint", default="/health")
                    max_retries = int(Prompt.ask("Maximum retries", default="10"))
                    self.health_check_standalone(port, endpoint, max_retries=max_retries)

                elif choice == "run_image":
                    image_name = Prompt.ask("Image name (e.g., nginx:latest)")
                    container_name = Prompt.ask("Container name")
                    
                    # Port mapping
                    ports = {}
                    ports_input = Prompt.ask("Port mapping (format: container:host, e.g., 80:8080, or multiple: 80:8080,443:8443, empty for none)", default="").strip()
                    if ports_input:
                        try:
                            # Support multiple ports: "80:8080,443:8443"
                            for port_pair in ports_input.split(','):
                                port_pair = port_pair.strip()
                                if ':' in port_pair:
                                    container_port, host_port = port_pair.split(':')
                                    ports[container_port.strip()] = host_port.strip()
                        except ValueError:
                            self.console.print("[red]Invalid port format. Use container:host (e.g., 80:8080)[/red]")
                            continue
                    
                    # Environment variables
                    environment = {}
                    if Confirm.ask("Add environment variables?", default=False):
                        while True:
                            env_input = Prompt.ask("Environment variable (KEY=VALUE, empty to finish)", default="").strip()
                            if not env_input:
                                break
                            if '=' in env_input:
                                key, value = env_input.split('=', 1)
                                environment[key.strip()] = value.strip()
                            else:
                                self.console.print("[yellow]Invalid format. Use KEY=VALUE[/yellow]")
                    
                    # Volumes
                    volumes = {}
                    if Confirm.ask("Add volume mappings?", default=False):
                        while True:
                            vol_input = Prompt.ask("Volume mapping (host:container or host:container:mode, empty to finish)", default="").strip()
                            if not vol_input:
                                break
                            if ':' in vol_input:
                                parts = vol_input.split(':')
                                if len(parts) == 2:
                                    # Simple format: host:container
                                    host_path, container_path = parts
                                    volumes[host_path.strip()] = container_path.strip()
                                elif len(parts) == 3:
                                    # Format with mode: host:container:mode
                                    host_path, container_path, mode = parts
                                    volumes[host_path.strip()] = {
                                        'bind': container_path.strip(),
                                        'mode': mode.strip()
                                    }
                                else:
                                    self.console.print("[yellow]Invalid format. Use host:container or host:container:mode[/yellow]")
                            else:
                                self.console.print("[yellow]Invalid format. Use host:container[/yellow]")
                    
                    # Optional command
                    command = Prompt.ask("Command to run (empty for default)", default="").strip()
                    command = command if command else None
                    
                    # Restart policy
                    restart_policy = Prompt.ask("Restart policy (no/on-failure/always/unless-stopped)", default="unless-stopped")
                    
                    # Network
                    network = None
                    if Confirm.ask("Use custom network?", default=False):
                        network = Prompt.ask("Network name (or 'host' for host network)", default="")
                        network = network if network else None
                    
                    # Privileged mode
                    privileged = Confirm.ask("Run in privileged mode?", default=False)
                    
                    # Resource limits
                    cpu_limit = None
                    if Confirm.ask("Set CPU limit?", default=False):
                        cpu_limit = Prompt.ask("CPU limit (e.g., 1.5 for 1.5 CPUs)", default="")
                        cpu_limit = cpu_limit if cpu_limit else None
                    
                    memory_limit = None
                    if Confirm.ask("Set memory limit?", default=False):
                        memory_limit = Prompt.ask("Memory limit (e.g., 1g for 1GB, 512m for 512MB)", default="")
                        memory_limit = memory_limit if memory_limit else None
                    
                    # Run container
                    success = self.run_new_container(
                        image_name=image_name,
                        name=container_name,
                        ports=ports if ports else None,
                        command=command,
                        environment=environment if environment else None,
                        volumes=volumes if volumes else None,
                        restart_policy=restart_policy,
                        network=network,
                        privileged=privileged,
                        cpu_limit=cpu_limit,
                        memory_limit=memory_limit
                    )
                    if not success:
                        self.console.print("[red]Failed to run container[/red]")
                
                elif choice == "build":
                    dockerfile_path = Prompt.ask("Dockerfile path", default=".")
                    image_tag = Prompt.ask("Image tag (e.g., myapp:latest)")
                    no_cache = Confirm.ask("Build without cache?", default=False)
                    pull = Confirm.ask("Pull base image updates?", default=True)
                    success = self.build_image_standalone(dockerfile_path, image_tag, no_cache, pull)
                    if not success:
                        self.console.print("[red]Image build failed[/red]")

                elif choice == "json":
                    self.list_containers()
                    container_name = Prompt.ask("Container name or ID")
                    self.view_container_json(container_name)    

                elif choice == "logs":
                    self.list_containers()
                    containers_input = Prompt.ask("Container name(s) or ID(s) (comma-separated for multiple, empty for interactive select)", default="").strip()
                    if containers_input:
                        self.view_container_logs(containers_input)
                    else:
                        self.view_container_logs()

                
                elif choice == "remove-image":
                    self.list_images()
                    images_input = Prompt.ask("Image name(s) or ID(s) to remove (comma-separated for multiple, e.g., img1:tag,img2:tag)")
                    images = self._parse_multi_target(images_input)
                    
                    if not images:
                        self.console.print("[red]No image names provided[/red]")
                        continue
                    
                    force = Confirm.ask("Force removal?", default=False)
                    
                    # Remove each image
                    all_success = True
                    for image in images:
                        self.console.print(f"\n[cyan]Processing image: {image}[/cyan]")
                        success = self.remove_image(image, force)
                        if not success:
                            all_success = False
                    
                    if not all_success:
                        self.console.print("[yellow]‚ö†Ô∏è Some operations failed[/yellow]")
                    else:
                        self.console.print("[green]‚úÖ All operations completed successfully[/green]")

                elif choice == "prune-images":
                    self.console.print("[cyan]üßπ Cleaning up dangling images (images without tags)...[/cyan]")
                    dry_run = Confirm.ask("Dry run (show what would be removed)?", default=True)
                    result = self.prune_dangling_images(dry_run=dry_run)
                    
                    if not dry_run and result['images_deleted'] > 0:
                        self.console.print(f"[green]‚úÖ Cleanup completed! Removed {result['images_deleted']} images[/green]")
                    elif dry_run:
                        if result['images_deleted'] > 0:
                            proceed = Confirm.ask("Proceed with removal?", default=False)
                            if proceed:
                                result = self.prune_dangling_images(dry_run=False)
                                if result['images_deleted'] > 0:
                                    self.console.print(f"[green]‚úÖ Cleanup completed! Removed {result['images_deleted']} images[/green]")
                        else:
                            self.console.print("[yellow]‚ÑπÔ∏è No dangling images to remove[/yellow]")

                elif choice == "quick-deploy":
                    dockerfile_path = Prompt.ask("Dockerfile directory path", default=".")
                    image_tag = Prompt.ask("Image tag (e.g., myapp:v1.2)")
                    container_name = Prompt.ask("Container name")
                    
                    # Optional YAML config
                    use_yaml = Confirm.ask("Load settings from YAML config?", default=False)
                    yaml_config = None
                    if use_yaml:
                        yaml_config = Prompt.ask("YAML config file path")
                    
                    # Port mapping (if not using YAML)
                    port_mapping = None
                    if not use_yaml:
                        port_input = Prompt.ask("Port mapping (format: container:host, e.g., 80:8080, empty to skip)", default="").strip()
                        if port_input:
                            try:
                                container_port, host_port = port_input.split(':')
                                port_mapping = {container_port: host_port}
                            except ValueError:
                                self.console.print("[red]Invalid port format[/red]")
                                continue
                    
                    # Environment variables
                    environment = None
                    if not use_yaml and Confirm.ask("Add environment variables?", default=False):
                        environment = {}
                        while True:
                            env_var = Prompt.ask("Environment variable (KEY=VALUE, empty to finish)", default="").strip()
                            if not env_var:
                                break
                            try:
                                key, value = env_var.split('=', 1)
                                environment[key] = value
                            except ValueError:
                                self.console.print("[red]Invalid format. Use KEY=VALUE[/red]")
                    
                    # Volumes
                    volumes = None
                    if not use_yaml and Confirm.ask("Add volume mappings?", default=False):
                        volumes = {}
                        while True:
                            volume = Prompt.ask("Volume mapping (host:container, empty to finish)", default="").strip()
                            if not volume:
                                break
                            try:
                                host_path, container_path = volume.split(':')
                                volumes[host_path] = {'bind': container_path, 'mode': 'rw'}
                            except ValueError:
                                self.console.print("[red]Invalid format. Use host:container[/red]")
                    
                    # Cleanup old image
                    cleanup_old_image = Confirm.ask("Remove old image after deployment?", default=True)
                    
                    success = self.quick_deploy(
                        dockerfile_path=dockerfile_path,
                        image_tag=image_tag,
                        container_name=container_name,
                        port_mapping=port_mapping,
                        environment=environment,
                        volumes=volumes,
                        yaml_config=yaml_config,
                        cleanup_old_image=cleanup_old_image
                    )
                    
                    if not success:
                        self.console.print("[red]Quick deploy failed[/red]")

                elif choice == "deploy-init":
                    output = Prompt.ask("Output file", default="deployment.yml")
                    self.create_deployment_config(output)

                elif choice == "deploy-config":
                    config_file = Prompt.ask("Config file path", default="deployment.yml")
                    deploy_type = Prompt.ask("Type (rolling/blue-green/canary)", default="rolling")
                    success = self.deploy_from_config(config_file, deploy_type)
                    if not success:
                        self.console.print("[red]Deployment failed[/red]")

                elif choice == "history":
                    limit = int(Prompt.ask("Number of records", default="10"))
                    self.show_deployment_history(limit=limit)

                elif choice == "validate":
                    success = self.validate_system_requirements()
                    if not success:
                        self.console.print("[red]System validation failed[/red]")

                elif choice == "backup-create":
                    backup_path = Prompt.ask("Backup path (empty for auto)", default="").strip()
                    backup_path = backup_path if backup_path else None
                    self.backup_deployment_state(backup_path)

                elif choice == "backup-restore":
                    backup_path = Prompt.ask("Backup path")
                    success = self.restore_deployment_state(backup_path)
                    if not success:
                        self.console.print("[red]Restore failed[/red]")

                elif choice == "export-config":
                    output = Prompt.ask("Output archive name", default="docker-pilot-config.tar.gz")
                    self.export_configuration(output)

                elif choice == "import-config":
                    archive = Prompt.ask("Archive path")
                    success = self.import_configuration(archive)
                    if not success:
                        self.console.print("[red]Import failed[/red]")

                elif choice == "pipeline":
                    pipeline_type = Prompt.ask("Pipeline type (github/gitlab/jenkins)", default="github")
                    output = Prompt.ask("Output path (empty for default)", default="").strip()
                    output = output if output else None
                    self.create_pipeline_config(pipeline_type, output)

                elif choice == "test":
                    test_config = Prompt.ask("Test config file", default="integration-tests.yml")
                    success = self.run_integration_tests(test_config)
                    if not success:
                        self.console.print("[red]Integration tests failed[/red]")

                elif choice == "promote":
                    source = Prompt.ask("Source environment")
                    target = Prompt.ask("Target environment") 
                    config_path = Prompt.ask("Config file (empty for auto)", default="").strip()
                    config_path = config_path if config_path else None
                    success = self.environment_promotion(source, target, config_path)
                    if not success:
                        self.console.print("[red]Environment promotion failed[/red]")

                elif choice == "alerts":
                    config_path = Prompt.ask("Alert config file", default="alerts.yml")
                    success = self.setup_monitoring_alerts(config_path)
                    if not success:
                        self.console.print("[red]Alert setup failed[/red]")

                elif choice == "policy":
                    self.list_containers()
                    name = Prompt.ask("Container name or ID")
                    policy = Prompt.ask("Restart policy (no/on-failure/always/unless-stopped)", default="always")
                    success = self.update_restart_policy(name, policy)
                    if not success:
                        self.console.print("[red]Failed to update restart policy[/red]")

                elif choice == "docs":
                    output = Prompt.ask("Output directory", default="docs")
                    success = self.generate_documentation(output)
                    if not success:
                        self.console.print("[red]Documentation generation failed[/red]")

                elif choice == "checklist":
                    output = Prompt.ask("Output file", default="production-checklist.md")
                    success = self.create_production_checklist(output)
                    if not success:
                        self.console.print("[red]Checklist generation failed[/red]")

                else:
                    self.console.print("[yellow]Unknown option, try again[/yellow]")

        except KeyboardInterrupt:
            self.console.print("\n[yellow]Interrupted, exiting interactive mode[/yellow]")
        except Exception as e:
            self.logger.error(f"Interactive menu error: {e}")
            self.console.print(f"[red]Error: {e}[/red]")

# ==================== CI/CD PIPELINE INTEGRATION ====================

    def integrate_with_git(self, repo_path: str = ".") -> bool:
        """Integrate with Git for automated deployments"""
        try:
            import git
            repo = git.Repo(repo_path)
            
            # Get current branch and commit info
            current_branch = repo.active_branch.name
            commit_hash = repo.head.commit.hexsha[:8]
            commit_message = repo.head.commit.message.strip()
            
            self.console.print(f"[cyan]Git Integration:[/cyan] {current_branch}@{commit_hash}")
            self.console.print(f"[cyan]Latest commit:[/cyan] {commit_message}")
            
            return True
        except ImportError:
            self.console.print("[yellow]GitPython not installed. Run: pip install GitPython[/yellow]")
            return False
        except Exception as e:
            self.logger.error(f"Git integration failed: {e}")
            return False

    def create_pipeline_config(self, pipeline_type: str = "github", output_path: str = None) -> bool:
        """Generate CI/CD pipeline configuration files"""
        
        if pipeline_type.lower() == "github":
            return self._create_github_actions_config(output_path)
        elif pipeline_type.lower() == "gitlab":
            return self._create_gitlab_ci_config(output_path)
        elif pipeline_type.lower() == "jenkins":
            return self._create_jenkins_config(output_path)
        else:
            self.console.print(f"[red]Unsupported pipeline type: {pipeline_type}[/red]")
            return False

    def _create_github_actions_config(self, output_path: str = None) -> bool:
        """Create GitHub Actions workflow"""
        if not output_path:
            output_path = ".github/workflows"
        
        os.makedirs(output_path, exist_ok=True)
        
        # Load template from configs directory
        template_path = Path(__file__).parent / "configs" / "github-actions.yml.template"
        
        try:
            if not template_path.exists():
                self.logger.error(f"Template file not found: {template_path}")
                self.console.print(f"[red]Template file not found: {template_path}[/red]")
                return False
            
            with open(template_path, 'r') as f:
                workflow_content = f.read()
        
            config_file = Path(output_path) / "docker-pilot.yml"
            with open(config_file, 'w') as f:
                f.write(workflow_content)
            
            self.console.print(f"[green]GitHub Actions workflow created: {config_file}[/green]")
            return True
        except Exception as e:
            self.logger.error(f"Failed to create GitHub Actions config: {e}")
            return False

    def _create_gitlab_ci_config(self, output_path: str = None) -> bool:
        """Create GitLab CI configuration"""
        # Load template from configs directory
        template_path = Path(__file__).parent / "configs" / "gitlab-ci.yml.template"
        
        try:
            if not template_path.exists():
                self.logger.error(f"Template file not found: {template_path}")
                self.console.print(f"[red]Template file not found: {template_path}[/red]")
                return False
            
            with open(template_path, 'r') as f:
                config_content = f.read()
            
            config_file = ".gitlab-ci.yml" if not output_path else Path(output_path) / ".gitlab-ci.yml"
            with open(config_file, 'w') as f:
                f.write(config_content)
            
            self.console.print(f"[green]GitLab CI configuration created: {config_file}[/green]")
            return True
        except Exception as e:
            self.logger.error(f"Failed to create GitLab CI config: {e}")
            return False

    def _create_jenkins_config(self, output_path: str = None) -> bool:
        """Create Jenkins pipeline configuration"""
        # Load template from configs directory
        template_path = Path(__file__).parent / "configs" / "jenkinsfile.template"
        
        try:
            if not template_path.exists():
                self.logger.error(f"Template file not found: {template_path}")
                self.console.print(f"[red]Template file not found: {template_path}[/red]")
                return False
            
            with open(template_path, 'r') as f:
                pipeline_content = f.read()
            
            config_file = "Jenkinsfile" if not output_path else Path(output_path) / "Jenkinsfile"
            with open(config_file, 'w') as f:
                f.write(pipeline_content)
            
            self.console.print(f"[green]Jenkins pipeline created: {config_file}[/green]")
            return True
        except Exception as e:
            self.logger.error(f"Failed to create Jenkins config: {e}")
            return False

    def run_integration_tests(self, test_config_path: str = "integration-tests.yml") -> bool:
        """Run comprehensive integration tests"""
        self.console.print("[cyan]Running integration tests...[/cyan]")
        
        try:
            if Path(test_config_path).exists():
                with open(test_config_path, 'r') as f:
                    test_config = yaml.safe_load(f)
            else:
                # Load default test configuration from template
                template_path = Path(__file__).parent / "configs" / "integration-tests.yml.template"
                
                if template_path.exists():
                    with open(template_path, 'r') as f:
                        test_config = yaml.safe_load(f)
                else:
                    # Fallback to default test configuration
                    test_config = {
                        'tests': [
                            {
                                'name': 'Health Check',
                                'type': 'http',
                                'url': 'http://localhost:8080/health',
                                'expected_status': 200,
                                'timeout': 5
                            },
                            {
                                'name': 'API Endpoint',
                                'type': 'http',
                                'url': 'http://localhost:8080/api/status',
                                'expected_status': 200,
                                'timeout': 10
                            }
                        ]
                    }
            
            test_results = []
            
            for test in test_config.get('tests', []):
                result = self._run_single_integration_test(test)
                test_results.append(result)
            
            # Generate test report
            self._generate_test_report(test_results)
            
            # Return True if all tests passed
            return all(result['passed'] for result in test_results)
            
        except Exception as e:
            self.logger.error(f"Integration tests failed: {e}")
            return False

    def _run_single_integration_test(self, test_config: dict) -> dict:
        """Run a single integration test"""
        test_name = test_config.get('name', 'Unknown Test')
        test_type = test_config.get('type', 'http')
        
        start_time = time.time()
        
        try:
            if test_type == 'http':
                return self._run_http_test(test_config, start_time)
            elif test_type == 'database':
                return self._run_database_test(test_config, start_time)
            elif test_type == 'custom':
                return self._run_custom_test(test_config, start_time)
            else:
                return {
                    'name': test_name,
                    'passed': False,
                    'duration': 0,
                    'error': f'Unknown test type: {test_type}'
                }
        except Exception as e:
            return {
                'name': test_name,
                'passed': False,
                'duration': time.time() - start_time,
                'error': str(e)
            }

    def _run_http_test(self, test_config: dict, start_time: float) -> dict:
        """Run HTTP-based integration test"""
        url = test_config['url']
        expected_status = test_config.get('expected_status', 200)
        timeout = test_config.get('timeout', 5)
        method = test_config.get('method', 'GET').upper()
        headers = test_config.get('headers', {})
        data = test_config.get('data')
        
        try:
            if method == 'GET':
                response = requests.get(url, headers=headers, timeout=timeout)
            elif method == 'POST':
                response = requests.post(url, headers=headers, json=data, timeout=timeout)
            else:
                response = requests.request(method, url, headers=headers, json=data, timeout=timeout)
            
            passed = response.status_code == expected_status
            
            return {
                'name': test_config.get('name', 'HTTP Test'),
                'passed': passed,
                'duration': time.time() - start_time,
                'status_code': response.status_code,
                'expected_status': expected_status,
                'response_time': response.elapsed.total_seconds()
            }
            
        except requests.exceptions.RequestException as e:
            return {
                'name': test_config.get('name', 'HTTP Test'),
                'passed': False,
                'duration': time.time() - start_time,
                'error': str(e)
            }

    def _run_database_test(self, test_config: dict, start_time: float) -> dict:
        """Run database connectivity test"""
        # This would require database-specific libraries
        # For now, return a placeholder implementation
        return {
            'name': test_config.get('name', 'Database Test'),
            'passed': True,  # Placeholder
            'duration': time.time() - start_time,
            'note': 'Database testing requires specific database drivers'
        }

    def _run_custom_test(self, test_config: dict, start_time: float) -> dict:
        """Run custom test script"""
        script_path = test_config.get('script')
        if not script_path or not Path(script_path).exists():
            return {
                'name': test_config.get('name', 'Custom Test'),
                'passed': False,
                'duration': time.time() - start_time,
                'error': 'Custom test script not found'
            }
        
        try:
            import subprocess
            result = subprocess.run(
                ['python', script_path],
                capture_output=True,
                text=True,
                timeout=test_config.get('timeout', 30)
            )
            
            return {
                'name': test_config.get('name', 'Custom Test'),
                'passed': result.returncode == 0,
                'duration': time.time() - start_time,
                'stdout': result.stdout,
                'stderr': result.stderr
            }
            
        except subprocess.TimeoutExpired:
            return {
                'name': test_config.get('name', 'Custom Test'),
                'passed': False,
                'duration': time.time() - start_time,
                'error': 'Test script timed out'
            }

    def _generate_test_report(self, test_results: List[dict]):
        """Generate comprehensive test report"""
        total_tests = len(test_results)
        passed_tests = sum(1 for result in test_results if result['passed'])
        failed_tests = total_tests - passed_tests
        
        # Create test report table
        table = Table(title="Integration Test Results", show_header=True)
        table.add_column("Test Name", style="cyan")
        table.add_column("Status", style="bold")
        table.add_column("Duration", style="blue")
        table.add_column("Details", style="yellow")
        
        for result in test_results:
            status = "[green]PASS[/green]" if result['passed'] else "[red]FAIL[/red]"
            duration = f"{result['duration']:.2f}s"
            
            details = ""
            if 'status_code' in result:
                details = f"HTTP {result['status_code']}"
            if 'error' in result:
                details = result['error'][:50] + "..." if len(result['error']) > 50 else result['error']
            
            table.add_row(result['name'], status, duration, details)
        
        self.console.print(table)
        
        # Summary
        summary_color = "green" if failed_tests == 0 else "red"
        summary = f"[{summary_color}]{passed_tests}/{total_tests} tests passed[/{summary_color}]"
        self.console.print(Panel(summary, title="Test Summary"))
        
        # Save detailed report
        self._save_test_report(test_results, passed_tests, failed_tests)

    def _save_test_report(self, test_results: List[dict], passed: int, failed: int):
        """Save test report to file"""
        try:
            report_data = {
                'timestamp': datetime.now().isoformat(),
                'summary': {
                    'total': len(test_results),
                    'passed': passed,
                    'failed': failed,
                    'success_rate': (passed / len(test_results)) * 100 if test_results else 0
                },
                'tests': test_results
            }
            
            with open('integration-test-report.json', 'w') as f:
                json.dump(report_data, f, indent=2)
                
            self.logger.info("Integration test report saved to integration-test-report.json")
            
        except Exception as e:
            self.logger.error(f"Failed to save test report: {e}")

    def environment_promotion(self, source_env: str, target_env: str, 
                            config_path: str = None, skip_backup: bool = False) -> bool:
        """Promote deployment between environments (dev -> staging -> prod)
        
        Args:
            source_env: Source environment (dev/staging)
            target_env: Target environment (staging/prod)
            config_path: Path to deployment config (optional)
            skip_backup: Skip data backup before deployment (faster but risky)
        """
        self.console.print(f"[cyan]Promoting from {source_env} to {target_env}...[/cyan]")
        
        # Environment-specific configurations
        env_configs = {
            'dev': {
                'replicas': 1,
                'resources': {'cpu': '0.5', 'memory': '512Mi'},
                'image_tag_suffix': '-dev'
            },
            'staging': {
                'replicas': 2,
                'resources': {'cpu': '1.0', 'memory': '1Gi'},
                'image_tag_suffix': '-staging'
            },
            'prod': {
                'replicas': 3,
                'resources': {'cpu': '2.0', 'memory': '2Gi'},
                'image_tag_suffix': ''
            }
        }
        
        if source_env not in env_configs or target_env not in env_configs:
            self.console.print(f"[red]Invalid environment: {source_env} or {target_env}[/red]")
            return False
        
        try:
            # Load base configuration
            if not config_path:
                config_path = f"deployment-{target_env}.yml"
            
            if Path(config_path).exists():
                with open(config_path, 'r') as f:
                    config = yaml.safe_load(f)
            else:
                self.console.print(f"[red]Configuration file not found: {config_path}[/red]")
                return False
            
            # Apply environment-specific settings
            target_config = env_configs[target_env]
            
            # Keep original image tag for all environments
            # Don't modify image tags during promotion - same image should be used across environments
            # Environment-specific tags should be set manually in deployment configs if needed
            original_image_tag = config['deployment']['image_tag']
            config['deployment']['image_tag'] = original_image_tag
            self.logger.info(f"Using original image tag for promotion: {original_image_tag}")
            
            # Update resources
            config['deployment']['cpu_limit'] = target_config['resources']['cpu']
            config['deployment']['memory_limit'] = target_config['resources']['memory']
            
            # For STAGING environment, check if container already exists in PROD
            # If it does, don't create duplicate - just save config and mark as promoted
            original_container_name = config['deployment']['container_name']
            if target_env == 'staging':
                # Check if container with base name or variants already exists (likely in PROD)
                existing_prod_container = None
                try:
                    existing_prod_container = self.client.containers.get(original_container_name)
                except docker.errors.NotFound:
                    # Also check for blue/green variants
                    for suffix in ['_blue', '_green']:
                        try:
                            existing_prod_container = self.client.containers.get(f"{original_container_name}{suffix}")
                            break
                        except docker.errors.NotFound:
                            pass
                
                if existing_prod_container and existing_prod_container.status == 'running':
                    # Container already exists in PROD - don't create duplicate for STAGING
                    # Just save config and record promotion without deploying
                    self.console.print(f"[yellow]‚ö†Ô∏è Container '{original_container_name}' already exists in PROD[/yellow]")
                    self.console.print(f"[cyan]Saving STAGING configuration without creating new container...[/cyan]")
                    
                    # Save config for staging (will be used for future deployments)
                    container_name = config['deployment']['container_name']
                    image_tag = config['deployment']['image_tag']
                    config_dir = Path(config_path).parent
                    target_config_path = config_dir / f'deployment-{target_env}.yml'
                    with open(target_config_path, 'w', encoding='utf-8') as f:
                        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
                    self.logger.info(f"Saved deployment config for {target_env} environment to {target_config_path}")
                    
                    # Record promotion without deploying
                    deployment_id = f"promote_{source_env}_to_{target_env}_{int(time.time())}"
                    duration = timedelta(seconds=1)
                    
                    # Create a deployment record for staging
                    deployment_config = DeploymentConfig(**config['deployment'])
                    self._record_deployment(deployment_id, deployment_config, f'promotion-{target_env}', True, duration, target_env=target_env)
                    
                    self.console.print(f"[green]‚úì STAGING configuration saved. Container already running in PROD, no new deployment needed.[/green]")
                    return True
            
            # Run pre-promotion checks
            if not self._run_pre_promotion_checks(source_env, target_env):
                self.console.print("[red]Pre-promotion checks failed[/red]")
                return False
            
            # Normalize deployment config to ensure all fields are in correct format
            deployment = config['deployment']
            # Ensure volumes is a dict
            if 'volumes' not in deployment or not isinstance(deployment.get('volumes'), dict):
                deployment['volumes'] = {}
            # Ensure port_mapping is a dict
            if 'port_mapping' not in deployment or not isinstance(deployment.get('port_mapping'), dict):
                deployment['port_mapping'] = {}
            # Ensure environment is a dict
            if 'environment' not in deployment or not isinstance(deployment.get('environment'), dict):
                deployment['environment'] = {}
            
            # Execute deployment
            deployment_config = DeploymentConfig(**deployment)
            build_config = config.get('build', {})
            
            # Use appropriate deployment strategy based on target environment
            deployment_type = 'blue-green' if target_env == 'prod' else 'rolling'
            
            if deployment_type == 'blue-green':
                success = self._blue_green_deploy_enhanced(deployment_config, build_config)
            else:
                success = self._rolling_deploy(deployment_config, build_config)
            
            if success:
                # Run post-promotion validation
                if self._run_post_promotion_validation(target_env, deployment_config):
                    # Save deployment config for target environment
                    # Extract container name from config
                    container_name = config['deployment']['container_name']
                    image_tag = config['deployment']['image_tag']
                    
                    # Save config to unified deployment directory structure
                    # Save directly next to source config
                    config_dir = Path(config_path).parent
                    target_config_path = config_dir / f'deployment-{target_env}.yml'
                    with open(target_config_path, 'w', encoding='utf-8') as f:
                        yaml.dump(config, f, default_flow_style=False, allow_unicode=True)
                    self.logger.info(f"Saved deployment config for {target_env} environment to {target_config_path}")
                    
                    # Update metadata.json if it exists
                    metadata_path = config_dir / 'metadata.json'
                    if metadata_path.exists():
                        try:
                            with open(metadata_path, 'r', encoding='utf-8') as f:
                                metadata = json.load(f)
                            metadata['last_updated'] = datetime.now().isoformat()
                            metadata[f'env_{target_env}_config'] = str(target_config_path)
                            with open(metadata_path, 'w', encoding='utf-8') as f:
                                json.dump(metadata, f, indent=2)
                            self.logger.info(f"Updated metadata.json with {target_env} config path")
                        except Exception as e:
                            self.logger.warning(f"Could not update metadata.json: {e}")
                    
                    # Record deployment with environment information
                    deployment_id = f"promote_{source_env}_to_{target_env}_{int(time.time())}"
                    duration = timedelta(seconds=5)  # Approximate duration
                    self._record_deployment(deployment_id, deployment_config, f'promotion-{target_env}', True, duration, target_env=target_env)
                    
                    self.console.print(f"[green]Successfully promoted to {target_env}[/green]")
                    return True
                else:
                    self.console.print(f"[yellow]Deployment succeeded but validation failed in {target_env}[/yellow]")
                    return False
            else:
                self.console.print(f"[red]Deployment failed in {target_env}[/red]")
                return False
                
        except Exception as e:
            self.logger.error(f"Environment promotion failed: {e}")
            return False

    def _run_pre_promotion_checks(self, source_env: str, target_env: str) -> bool:
        """Run checks before promoting between environments"""
        checks = [
            f"Source environment ({source_env}) is healthy",
            f"Target environment ({target_env}) is ready",
            "All required tests have passed",
            "No blocking issues in monitoring systems"
        ]
        
        # For demo purposes, we'll simulate these checks
        # In real implementation, these would check actual systems
        
        for check in checks:
            # Simulate check (replace with real logic)
            time.sleep(1)
            self.console.print(f"[green]‚úì[/green] {check}")
        
        return True

    def _run_post_promotion_validation(self, environment: str, config: DeploymentConfig) -> bool:
        """Validate deployment after promotion"""
        validation_checks = [
            "Application is responding to health checks",
            "All services are running correctly",
            "Performance metrics are within acceptable ranges",
            "No error spikes in logs"
        ]
        
        # Run actual health checks
        if config.port_mapping:
            port = list(config.port_mapping.values())[0]
            if not self._advanced_health_check(port, config.health_check_endpoint, 30, 5):
                return False
        
        # Additional validation checks would go here
        for check in validation_checks:
            time.sleep(1)
            self.console.print(f"[green]‚úì[/green] {check}")
        
        return True

    def setup_monitoring_alerts(self, alert_config_path: str = "alerts.yml") -> bool:
        """Setup monitoring and alerting configuration from template"""
        
        try:
            if not Path(alert_config_path).exists():
                # Load template from configs directory
                template_path = Path(__file__).parent / "configs" / "alerts.yml.template"
                
                if not template_path.exists():
                    self.logger.error(f"Template file not found: {template_path}")
                    self.console.print(f"[red]Template file not found: {template_path}[/red]")
                    return False
                
                with open(template_path, 'r', encoding='utf-8') as f:
                    template_content = f.read()
                
                with open(alert_config_path, 'w', encoding='utf-8') as f:
                    f.write(template_content)
                
                self.console.print(f"[green]Alert configuration template created: {alert_config_path}[/green]")
            else:
                self.console.print(f"[yellow]Alert configuration already exists: {alert_config_path}[/yellow]")
            
            # Initialize alert monitoring
            return self._initialize_alert_monitoring(alert_config_path)
            
        except Exception as e:
            self.logger.error(f"Failed to setup monitoring alerts: {e}")
            return False

    def _initialize_alert_monitoring(self, alert_config_path: str) -> bool:
        """Initialize alert monitoring system"""
        try:
            with open(alert_config_path, 'r') as f:
                alert_config = yaml.safe_load(f)
            
            self.alert_rules = alert_config.get('alerts', [])
            self.notification_channels = alert_config.get('notification_channels', [])
            
            self.console.print(f"[green]Initialized {len(self.alert_rules)} alert rules[/green]")
            self.console.print(f"[green]Configured {len(self.notification_channels)} notification channels[/green]")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to initialize alert monitoring: {e}")
            return False

    def check_alerts(self, container_stats: ContainerStats, container_name: str):
        """Check if any alerts should be triggered"""
        if not hasattr(self, 'alert_rules'):
            return
        
        current_time = datetime.now()
        
        for rule in self.alert_rules:
            condition = rule['condition']
            
            # Simple condition evaluation (would need more sophisticated parsing in production)
            if 'cpu_percent >' in condition:
                threshold = float(condition.split('>')[-1].strip())
                if container_stats.cpu_percent > threshold:
                    self._trigger_alert(rule, container_name, f"CPU: {container_stats.cpu_percent:.1f}%")
            
            elif 'memory_percent >' in condition:
                threshold = float(condition.split('>')[-1].strip())
                if container_stats.memory_percent > threshold:
                    self._trigger_alert(rule, container_name, f"Memory: {container_stats.memory_percent:.1f}%")

    def _trigger_alert(self, rule: dict, container_name: str, details: str):
        """Trigger an alert notification"""
        alert_message = f"ALERT: {rule['name']} - Container: {container_name} - {details} - {rule['message']}"
        
        self.logger.warning(f"Alert triggered: {alert_message}")
        self.console.print(f"[red]üö® ALERT: {rule['name']} - {container_name}[/red]")
        
        # Send notifications
        for channel in getattr(self, 'notification_channels', []):
            self._send_notification(channel, alert_message)

    def _send_notification(self, channel: dict, message: str):
        """Send notification through configured channel"""
        try:
            if channel['type'] == 'slack':
                # Slack webhook notification
                webhook_url = channel.get('webhook_url')
                if webhook_url:
                    payload = {
                        'text': message,
                        'channel': channel.get('channel', '#general'),
                        'username': 'Docker Pilot',
                        'icon_emoji': ':warning:'
                    }
                    requests.post(webhook_url, json=payload, timeout=5)
            
            elif channel['type'] == 'email':
                # Email notification (would require email libraries)
                self.logger.info(f"Email notification would be sent: {message}")
                
        except Exception as e:
            self.logger.error(f"Failed to send notification: {e}")

    def create_production_checklist(self, output_file: str = "production-checklist.md") -> bool:
        """Generate production deployment checklist from template"""
        try:
            # Load template from configs directory
            template_path = Path(__file__).parent / "configs" / "production-checklist.md.template"
            
            if not template_path.exists():
                self.logger.error(f"Template not found: {template_path}")
                self.console.print(f"[red]Template file not found: {template_path}[/red]")
                return False
            
            with open(template_path, 'r', encoding='utf-8') as f:
                checklist_content = f.read()
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(checklist_content)
            
            self.console.print(f"[green]Production checklist created: {output_file}[/green]")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to create production checklist: {e}")
            return False

    def generate_documentation(self, output_dir: str = "docs") -> bool:
        """Generate comprehensive project documentation from templates"""
        try:
            docs_path = Path(output_dir)
            docs_path.mkdir(exist_ok=True)
            
            # Get templates directory
            templates_dir = Path(__file__).parent / "configs"
            
            # Documentation files to generate
            doc_files = [
                ("docs-readme.md.template", "README.md"),
                ("docs-api.md.template", "API.md"),
                ("docs-troubleshooting.md.template", "TROUBLESHOOTING.md")
            ]
            
            # Generate each documentation file from template
            for template_name, output_name in doc_files:
                template_path = templates_dir / template_name
                
                if not template_path.exists():
                    self.logger.warning(f"Template not found: {template_path}")
                    continue
                
                with open(template_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                with open(docs_path / output_name, 'w', encoding='utf-8') as f:
                    f.write(content)
                
                self.logger.info(f"Generated {output_name}")
            
            self.console.print(f"[green]Documentation generated in {output_dir}/[/green]")
            return True
            
        except Exception as e:
            self.logger.error(f"Failed to generate documentation: {e}")
            return False

    def _check_sudo_required_for_backup(self, container_name: str) -> tuple[bool, list[str], dict]:
        """Check if backup will require sudo access and get mount information
        
        Returns:
            tuple: (requires_sudo: bool, privileged_paths: list[str], mount_info: dict)
                   mount_info contains: {'large_mounts': list, 'total_size_gb': float, 'mounts': list}
        """
        try:
            import shutil
            
            container = self.client.containers.get(container_name)
            mounts = container.attrs.get('Mounts', [])
            
            privileged_paths = []
            large_mounts = []  # Mounts > 1TB
            mount_info_list = []  # List of all mounts with size info
            total_size_bytes = 0
            
            for mount in mounts:
                source = mount.get('Source')
                if source:
                    source_path = Path(source)
                    requires_sudo = (
                        str(source).startswith('/var/lib/docker/volumes/') or
                        str(source).startswith('/var/lib/docker/') or
                        str(source).startswith('/root/') or
                        not os.access(source, os.R_OK)
                    )
                    
                    if requires_sudo:
                        privileged_paths.append(source)
                    
                    # Check mount size (if it's a directory)
                    mount_size_bytes = 0
                    mount_size_gb = 0
                    mount_total_capacity_gb = 0  # Total disk capacity
                    is_large = False
                    
                    try:
                        if source_path.exists() and source_path.is_dir():
                            # First, check total disk capacity using df (faster than du for large dirs)
                            try:
                                df_result = subprocess.run(
                                    ['df', '-B1', str(source_path)],  # -B1 = block size 1 byte
                                    capture_output=True,
                                    timeout=5,
                                    text=True
                                )
                                if df_result.returncode == 0:
                                    # Parse df output: Filesystem Size Used Avail Use% Mounted
                                    lines = df_result.stdout.strip().split('\n')
                                    if len(lines) > 1:
                                        parts = lines[1].split()
                                        if len(parts) >= 2:
                                            mount_total_capacity_gb = int(parts[1]) / (1024 ** 3)
                            except (subprocess.TimeoutExpired, ValueError, IndexError, FileNotFoundError):
                                # df failed, continue with du
                                pass
                            
                            # Use du command for actual used size (faster than Python for large dirs)
                            try:
                                result = subprocess.run(
                                    ['du', '-sb', str(source_path)],
                                    capture_output=True,
                                    timeout=30,  # Increased timeout for large directories
                                    text=True
                                )
                                if result.returncode == 0:
                                    mount_size_bytes = int(result.stdout.split()[0])
                                    mount_size_gb = mount_size_bytes / (1024 ** 3)
                                    total_size_bytes += mount_size_bytes
                                    
                                    # Consider large if:
                                    # - Used space > 500GB (more reasonable threshold)
                                    # - OR total disk capacity > 1TB (even if not fully used)
                                    is_large = mount_size_gb > 500 or mount_total_capacity_gb > 1024
                                    
                                    if is_large:
                                        large_mounts.append({
                                            'path': source,
                                            'size_gb': mount_size_gb,
                                            'size_tb': mount_size_gb / 1024,
                                            'total_capacity_gb': mount_total_capacity_gb,
                                            'total_capacity_tb': mount_total_capacity_gb / 1024
                                        })
                            except (subprocess.TimeoutExpired, ValueError, IndexError):
                                # If du fails or times out, check if we have capacity info from df
                                if mount_total_capacity_gb > 1024:
                                    is_large = True
                                    large_mounts.append({
                                        'path': source,
                                        'size_gb': 0,  # Unknown actual size
                                        'size_tb': 0,
                                        'total_capacity_gb': mount_total_capacity_gb,
                                        'total_capacity_tb': mount_total_capacity_gb / 1024,
                                        'note': 'Size check timed out, but disk capacity is large'
                                    })
                                # Don't try shutil fallback for large dirs - it's too slow
                    except Exception as e:
                        self.logger.debug(f"Could not check size for {source}: {e}")
                    
                    mount_info_list.append({
                        'path': source,
                        'mount_point': mount.get('Destination'),
                        'requires_sudo': requires_sudo,
                        'size_gb': mount_size_gb,
                        'total_capacity_gb': mount_total_capacity_gb,
                        'is_large': is_large
                    })
            
            total_size_gb = total_size_bytes / (1024 ** 3)
            
            mount_info = {
                'large_mounts': large_mounts,
                'total_size_gb': total_size_gb,
                'total_size_tb': total_size_gb / 1024,
                'mounts': mount_info_list
            }
            
            return len(privileged_paths) > 0, privileged_paths, mount_info
            
        except Exception as e:
            self.logger.warning(f"Could not check sudo requirements: {e}")
            return False, [], {'large_mounts': [], 'total_size_gb': 0, 'total_size_tb': 0, 'mounts': []}
    
    def find_existing_backup(self, container_name: str, max_age_hours: int = 24) -> Optional[Path]:
        """
        Find existing backup for container that is recent enough.
        
        Args:
            container_name: Name of the container to find backup for
            max_age_hours: Maximum age of backup in hours (default: 24)
        
        Returns:
            Path to backup directory if found, None otherwise
        """
        try:
            # Search in current directory and common backup locations
            search_paths = [
                Path('.'),
                Path.home() / '.dockerpilot_extras' / 'backups',
                Path('/tmp'),
            ]
            
            # Also search in parent directories for backups
            current_dir = Path.cwd()
            for parent in [current_dir] + list(current_dir.parents)[:3]:  # Check up to 3 levels up
                search_paths.append(parent)
            
            best_backup = None
            best_backup_time = None
            
            for search_path in search_paths:
                if not search_path.exists():
                    continue
                
                # Look for backup directories matching pattern
                pattern = f"backup_{container_name}_*"
                for backup_dir in search_path.glob(pattern):
                    if not backup_dir.is_dir():
                        continue
                    
                    # Check if backup metadata exists
                    metadata_file = backup_dir / 'backup_metadata.json'
                    if not metadata_file.exists():
                        continue
                    
                    try:
                        with open(metadata_file, 'r') as f:
                            metadata = json.load(f)
                        
                        # Verify it's for the right container
                        if metadata.get('container_name') != container_name:
                            continue
                        
                        # Check backup age
                        backup_time_str = metadata.get('backup_time')
                        if backup_time_str:
                            backup_time = datetime.fromisoformat(backup_time_str.replace('Z', '+00:00'))
                            if backup_time.tzinfo is None:
                                # Assume local time if no timezone
                                backup_time = backup_time.replace(tzinfo=datetime.now().astimezone().tzinfo)
                            
                            age_hours = (datetime.now(backup_time.tzinfo) - backup_time).total_seconds() / 3600
                            
                            if age_hours <= max_age_hours:
                                # Check if all backup files exist
                                volumes = metadata.get('volumes', [])
                                all_files_exist = True
                                for vol in volumes:
                                    backup_file = Path(vol.get('backup_file', ''))
                                    if not backup_file.is_absolute():
                                        backup_file = backup_dir / backup_file.name
                                    if not backup_file.exists():
                                        all_files_exist = False
                                        break
                                
                                if all_files_exist:
                                    # This is a valid backup, check if it's newer than current best
                                    if best_backup is None or (backup_time > best_backup_time):
                                        best_backup = backup_dir
                                        best_backup_time = backup_time
                    except Exception as e:
                        self.logger.debug(f"Error reading backup metadata {metadata_file}: {e}")
                        continue
            
            return best_backup
            
        except Exception as e:
            self.logger.warning(f"Error searching for existing backup: {e}")
            return None
    
    def backup_container_data(self, container_name: str, backup_path: str = None, reuse_existing: bool = True, max_backup_age_hours: int = 24) -> bool:
        """
        Backup ALL data from container volumes (actual data, not just metadata).
        Creates a complete backup of all volumes mounted to the container.
        If reuse_existing is True, will check for existing recent backup first.
        
        Args:
            container_name: Name of the container to backup
            backup_path: Optional path for backup (auto-generated if not provided)
            reuse_existing: If True, reuse existing backup if found (default: True)
            max_backup_age_hours: Maximum age of backup to reuse in hours (default: 24)
        
        Returns:
            bool: True if backup successful or existing backup reused
        """
        try:
            container = self.client.containers.get(container_name)
            
            # Check for existing backup first if reuse_existing is True
            # Even if backup_path is provided, we check for existing backup first
            if reuse_existing:
                existing_backup = self.find_existing_backup(container_name, max_backup_age_hours)
                if existing_backup:
                    self.logger.info(f"Found existing backup for {container_name}: {existing_backup}")
                    self.console.print(f"[green]‚úÖ Found existing backup: {existing_backup}[/green]")
                    
                    # Verify backup is complete
                    metadata_file = existing_backup / 'backup_metadata.json'
                    if metadata_file.exists():
                        try:
                            with open(metadata_file, 'r') as f:
                                metadata = json.load(f)
                            
                            volumes = metadata.get('volumes', [])
                            total_size_mb = metadata.get('total_size', 0) / (1024 * 1024)
                            backup_time = metadata.get('backup_time', 'unknown')
                            
                            self.console.print(f"[green]Backup created: {backup_time}[/green]")
                            self.console.print(f"[green]Total size: {total_size_mb:.2f} MB[/green]")
                            self.console.print(f"[green]Volumes backed up: {len(volumes)}[/green]")
                            self.console.print(f"[cyan]‚ÑπÔ∏è Reusing existing backup instead of creating new one[/cyan]")
                            
                            # Set backup_path to the existing backup path for consistency
                            backup_path = str(existing_backup)
                            return True
                        except Exception as e:
                            self.logger.warning(f"Error reading existing backup metadata: {e}")
                            # Continue to create new backup
            
            if not backup_path:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_path = f"backup_{container_name}_{timestamp}"
            
            backup_dir = Path(backup_path)
            backup_dir.mkdir(exist_ok=True, parents=True)
            
            # Pre-check: will we need sudo?
            requires_sudo, privileged_paths, mount_info = self._check_sudo_required_for_backup(container_name)
            
            if requires_sudo:
                self.console.print(f"[yellow]‚ö†Ô∏è  BACKUP REQUIRES SUDO ACCESS[/yellow]")
                self.console.print(f"[yellow]Privileged paths ({len(privileged_paths)}):[/yellow]")
                for path in privileged_paths[:3]:  # Show first 3
                    self.console.print(f"[dim]  - {path}[/dim]")
                if len(privileged_paths) > 3:
                    self.console.print(f"[dim]  ... and {len(privileged_paths) - 3} more[/dim]")
                self.console.print(f"[yellow]You may be prompted for sudo password during backup.[/yellow]")
                self.console.print(f"[yellow]To skip backup: use --skip-backup flag[/yellow]")
                
                # Give user 3 seconds to cancel
                import sys
                for i in range(3, 0, -1):
                    sys.stdout.write(f"\rContinuing in {i}s... (Ctrl+C to cancel)")
                    sys.stdout.flush()
                    time.sleep(1)
                sys.stdout.write("\r" + " " * 50 + "\r")  # Clear line
            
            self.console.print(f"[cyan]üì¶ Creating data backup for container '{container_name}'...[/cyan]")
            
            # Get container mounts
            mounts = container.attrs.get('Mounts', [])
            
            if not mounts:
                self.console.print(f"[yellow]‚ö†Ô∏è No volumes mounted to container '{container_name}'[/yellow]")
                return True  # Not an error, just no data to backup
            
            # System paths that should be skipped during backup (they can hang or are too large)
            system_paths_to_skip = [
                '/',  # Root filesystem - NEVER backup this!
                '/lib/modules',
                '/proc',
                '/sys',
                '/dev',
                '/run',
                '/tmp',
                '/var/run',
                '/boot',
                '/usr',
                '/bin',
                '/sbin',
                '/etc',
                '/var/lib',
                '/var/log',
            ]
            
            # Show loading indicator during backup
            with self._with_loading("Backing up container data"):
                # Backup each volume
                backed_up_volumes = []
                total_mounts = len([m for m in mounts if m.get('Source') or m.get('Name')])
                processed_mounts = 0
                
                for mount in mounts:
                    # Check for cancellation before each mount backup
                    if self._check_cancel_flag(container_name):
                        self.logger.warning(f"Backup cancelled by user for {container_name}")
                        self.console.print(f"[yellow]‚ö†Ô∏è Backup cancelled by user[/yellow]")
                        return False
                    
                    volume_name = mount.get('Name')
                    mount_point = mount.get('Destination')  # Path inside container
                    source = mount.get('Source')  # Path on host (for bind mounts)
                    
                    # Skip system paths and root filesystem FIRST
                    if source:
                        source_path = Path(source)
                        
                        # ALWAYS skip root filesystem - this is critical!
                        if str(source_path) == '/' or str(source_path).resolve() == Path('/'):
                            self.logger.warning(f"Skipping root filesystem bind mount: {source} -> {mount_point} (CRITICAL: root filesystem should never be backed up)")
                            self.console.print(f"[red]‚ö†Ô∏è SKIPPING root filesystem bind mount '{source}' (root filesystem should never be backed up!)[/red]")
                            continue
                    
                    # For migration: Skip external data mounts (not container-specific data)
                    # Named volumes are always backed up (they're container-specific)
                    # Bind mounts to external storage (/mnt/*, /media/*) should be skipped
                    # Bind mounts to application directories (/opt/*, /var/www/*) should be backed up
                    if source and not volume_name:
                        # This is a bind mount (not a named volume)
                        source_path = Path(source)
                        
                        # Skip external storage mounts (these are not container data, just mounted storage)
                        external_storage_patterns = [
                            '/mnt/',  # External mounts like /mnt/sdc_share, /mnt/sda3, etc.
                            '/media/',  # Removable media
                        ]
                        
                        # Check if this is an external storage mount
                        is_external_storage = any(
                            str(source_path).startswith(pattern) 
                            for pattern in external_storage_patterns
                        )
                        
                        # Exception: application directories in /opt, /var/www, etc. should be backed up
                        application_patterns = [
                            '/opt/',  # Application installations
                            '/var/www/',  # Web applications
                            '/var/lib/',  # Application data (but not /var/lib/docker)
                            '/srv/',  # Service data
                        ]
                        
                        is_application_data = any(
                            str(source_path).startswith(pattern) 
                            for pattern in application_patterns
                        ) and not str(source_path).startswith('/var/lib/docker')
                        
                        if is_external_storage and not is_application_data:
                            self.logger.info(f"Skipping external storage bind mount: {source} -> {mount_point} (external storage, not container data)")
                            self.console.print(f"[cyan]‚ÑπÔ∏è Skipping external disk '{source}' (this is not container data, just a mounted disk)[/cyan]")
                            continue
                    
                    # Skip system paths
                    if source:
                        source_path = Path(source)
                        
                        # Check if source is a system path to skip
                        skip_mount = False
                        for system_path in system_paths_to_skip:
                            if str(source_path) == system_path or str(source_path).startswith(system_path + '/'):
                                self.logger.warning(f"Skipping system bind mount: {source} -> {mount_point}")
                                self.console.print(f"[yellow]‚ö†Ô∏è Skipping system bind mount '{source}' (system path)[/yellow]")
                                skip_mount = True
                                break
                        
                        if skip_mount:
                            continue
                        
                        # Check if mount is very large (> 1TB) - warn but don't skip automatically
                        # User should have been warned in the modal, but double-check here
                        try:
                            # Quick size check using du (with timeout to avoid hanging)
                            result = subprocess.run(
                                ['du', '-sb', str(source_path)],
                                capture_output=True,
                                timeout=5,  # 5 second timeout for size check
                                text=True
                            )
                            if result.returncode == 0:
                                size_bytes = int(result.stdout.split()[0])
                                size_tb = size_bytes / (1024 ** 4)
                                if size_tb > 1:
                                    self.logger.warning(f"Large mount detected: {source} ({size_tb:.2f} TB) - this will take a very long time to backup")
                                    self.console.print(f"[yellow]‚ö†Ô∏è Large mount detected: {source} ({size_tb:.2f} TB)[/yellow]")
                                    self.console.print(f"[yellow]   This backup may take many hours. Consider skipping this mount.[/yellow]")
                        except (subprocess.TimeoutExpired, ValueError, IndexError, FileNotFoundError):
                            # If size check fails or times out, continue anyway
                            # (might be a network mount or permission issue)
                            pass
                    
                    if volume_name:
                        # Named volume - backup using Docker container (no sudo needed!)
                        self.console.print(f"[cyan]Backing up named volume: {volume_name} -> {mount_point}[/cyan]")
                        try:
                            backup_file = backup_dir / f"{volume_name}.tar.gz"
                            
                            # Update progress for volume backup
                            if container_name:
                                progress_pct = 5 + int((processed_mounts / max(total_mounts, 1)) * 15)  # 5-20% range
                                self._update_progress('backup', progress_pct, f'üì¶ Creating backup of volume: {volume_name}...')
                            
                            # Use Docker to backup volume (runs as root inside container)
                            # This avoids permission issues without requiring sudo
                            success = self._backup_volume_using_docker(volume_name, backup_file, container_name)
                            
                            # Check for cancellation after backup
                            if self._check_cancel_flag(container_name):
                                self.logger.warning(f"Backup cancelled by user for {container_name}")
                                self.console.print(f"[yellow]‚ö†Ô∏è Backup cancelled by user[/yellow]")
                                return False
                            
                            if success:
                                processed_mounts += 1
                                backed_up_volumes.append({
                                    'type': 'named_volume',
                                    'name': volume_name,
                                    'mount_point': mount_point,
                                    'backup_file': str(backup_file),
                                    'size': backup_file.stat().st_size if backup_file.exists() else 0
                                })
                                self.console.print(f"[green]‚úÖ Backed up volume '{volume_name}' to {backup_file}[/green]")
                                
                                # Update progress after successful backup
                                if container_name:
                                    progress_pct = 5 + int((processed_mounts / max(total_mounts, 1)) * 15)  # 5-20% range
                                    self._update_progress('backup', progress_pct, f'‚úÖ Zbackupowano volume: {volume_name} ({processed_mounts}/{total_mounts})')
                            else:
                                self.logger.warning(f"Failed to backup volume {volume_name}, continuing...")
                                self.console.print(f"[yellow]‚ö†Ô∏è Failed to backup volume '{volume_name}', continuing...[/yellow]")
                                # Don't return False - continue with other volumes
                        except Exception as e:
                            self.logger.error(f"Failed to backup volume {volume_name}: {e}")
                            self.console.print(f"[yellow]‚ö†Ô∏è Failed to backup volume '{volume_name}': {e}, continuing...[/yellow]")
                            # Don't return False - continue with other volumes
                
                    elif source:
                        # Bind mount - backup using Docker container (faster and no sudo needed for many paths)
                        self.console.print(f"[cyan]Backing up bind mount: {source} -> {mount_point}[/cyan]")
                        # Update progress for bind mount backup
                        if container_name:
                            source_name = Path(source).name
                            progress_pct = 5 + int((processed_mounts / max(total_mounts, 1)) * 15)  # 5-20% range
                            self._update_progress('backup', progress_pct, f'üì¶ Creating backup of bind mount: {source_name}...')
                        try:
                            if Path(source).exists():
                                # Create safe filename from path
                                safe_name = source.replace('/', '_').replace('\\', '_').strip('_')
                                backup_file = backup_dir / f"bind_{safe_name}.tar.gz"
                                
                                # Use Docker container for backup (faster, no sudo needed, better for large directories)
                                success = self._backup_bind_mount_using_docker(source, backup_file, container_name)
                                
                                # Check for cancellation after backup
                                if self._check_cancel_flag(container_name):
                                    self.logger.warning(f"Backup cancelled by user for {container_name}")
                                    self.console.print(f"[yellow]‚ö†Ô∏è Backup cancelled by user[/yellow]")
                                    return False
                                
                                if success:
                                    processed_mounts += 1
                                    backed_up_volumes.append({
                                        'type': 'bind_mount',
                                        'source': source,
                                        'mount_point': mount_point,
                                        'backup_file': str(backup_file),
                                        'size': backup_file.stat().st_size if backup_file.exists() else 0
                                    })
                                    self.console.print(f"[green]‚úÖ Backed up bind mount '{source}' to {backup_file}[/green]")
                                    
                                    # Update progress after successful backup
                                    if container_name:
                                        progress_pct = 5 + int((processed_mounts / max(total_mounts, 1)) * 15)  # 5-20% range
                                        self._update_progress('backup', progress_pct, f'‚úÖ Zbackupowano bind mount: {source_name} ({processed_mounts}/{total_mounts})')
                                else:
                                    self.logger.warning(f"Failed to backup bind mount {source}, continuing...")
                                    self.console.print(f"[yellow]‚ö†Ô∏è Failed to backup bind mount '{source}', continuing...[/yellow]")
                                    # Don't return False - continue with other volumes
                            else:
                                self.logger.warning(f"Bind mount source does not exist: {source}")
                                self.console.print(f"[yellow]‚ö†Ô∏è Bind mount source not found: {source}[/yellow]")
                        except Exception as e:
                            self.logger.error(f"Failed to backup bind mount {source}: {e}")
                            self.console.print(f"[yellow]‚ö†Ô∏è Failed to backup bind mount '{source}': {e}, continuing...[/yellow]")
                            # Don't return False - continue with other volumes
                
                # Save backup metadata (inside loading context)
                if container_name:
                    self._update_progress('backup', 18, 'üíæ Saving backup metadata...')
                
                backup_metadata = {
                    'container_name': container_name,
                    'backup_time': datetime.now().isoformat(),
                    'container_image': container.image.tags[0] if container.image.tags else container.image.id,
                    'volumes': backed_up_volumes,
                    'total_size': sum(v.get('size', 0) for v in backed_up_volumes)
                }
                
                metadata_file = backup_dir / 'backup_metadata.json'
                with open(metadata_file, 'w') as f:
                    json.dump(backup_metadata, f, indent=2)
                
                # Final progress update
                if container_name:
                    self._update_progress('backup', 20, '‚úÖ Backup completed')
            
            # Show results after loading completes
            total_size_mb = sum(v.get('size', 0) for v in backed_up_volumes) / (1024 * 1024)
            self.console.print(f"[bold green]‚úÖ Data backup completed![/bold green]")
            self.console.print(f"[green]Backup location: {backup_path}[/green]")
            self.console.print(f"[green]Total size: {total_size_mb:.2f} MB[/green]")
            self.console.print(f"[green]Volumes backed up: {len(backed_up_volumes)}[/green]")
            
            return True
            
        except docker.errors.NotFound:
            self.console.print(f"[red]‚ùå Container '{container_name}' not found[/red]")
            return False
        except Exception as e:
            self.logger.error(f"Container data backup failed: {e}")
            self.console.print(f"[red]‚ùå Backup failed: {e}[/red]")
            return False
    
    def _backup_volume_using_docker(self, volume_name: str, backup_file: Path, container_name: str = None) -> bool:
        """Backup Docker volume using a temporary container (no sudo needed!)
        
        This method uses Docker itself to backup volumes, avoiding permission issues.
        The container runs as root and can access the volume data.
        
        Args:
            volume_name: Name of the Docker volume to backup
            backup_file: Path to the backup file to create
            container_name: Container name for cancel flag checking
        """
        try:
            import subprocess
            import signal
            
            # Get current user UID and GID for ownership fix
            uid = os.getuid()
            gid = os.getgid()
            
            # Use docker run to create tar backup of volume
            # This runs as root inside container, so no permission issues
            # We also fix ownership of the backup file
            process = subprocess.Popen(
                [
                    'docker', 'run', '--rm',
                    '-v', f'{volume_name}:/volume:ro',  # Mount volume as read-only
                    '-v', f'{backup_file.parent.absolute()}:/backup',  # Mount backup dir
                    'alpine:latest',  # Lightweight image
                    'sh', '-c',
                    f'tar -czf /backup/{backup_file.name} -C /volume . 2>/dev/null'
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # Wait with periodic cancel checks and progress updates
            timeout = 600  # 10 minutes timeout (large volumes like influxdb2)
            start_time = time.time()
            check_interval = 2  # Check cancel flag every 2 seconds
            last_size = 0
            last_log_time = start_time
            log_interval = 10  # Log progress every 10 seconds
            
            self.logger.info(f"Starting backup of volume '{volume_name}' (timeout: {timeout}s)")
            
            last_progress_update = 0
            progress_update_interval = 5  # Update progress every 5 seconds
            
            while True:
                elapsed = time.time() - start_time
                if elapsed > timeout:
                    process.terminate()
                    try:
                        process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        process.kill()
                    self.logger.error(f"Volume backup timed out for {volume_name} after {elapsed:.1f}s")
                    if container_name:
                        self._update_progress('backup', 95, f'‚ùå Backup timeout for volume: {volume_name}')
                    # Clean up any orphaned backup containers
                    self._cleanup_backup_containers()
                    return False
                
                # Check for cancellation
                if container_name and self._check_cancel_flag(container_name):
                    self.logger.warning(f"Backup cancelled during volume backup: {volume_name}")
                    process.terminate()
                    try:
                        process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        process.kill()
                    if container_name:
                        progress_pct = min(90, int((elapsed / timeout) * 100))
                        self._update_progress('backup', progress_pct, f'‚ö†Ô∏è Backup cancelled: {volume_name}')
                    # Clean up any orphaned backup containers
                    self._cleanup_backup_containers()
                    return False
                
                # Check if process finished
                if process.poll() is not None:
                    self.logger.info(f"Volume backup completed for '{volume_name}' in {elapsed:.1f}s")
                    break
                
                # Update progress periodically (for web interface)
                if container_name and elapsed - last_progress_update >= progress_update_interval:
                    current_size = backup_file.stat().st_size if backup_file.exists() else 0
                    size_mb = current_size / (1024 * 1024) if current_size > 0 else 0
                    progress_pct = min(90, int((elapsed / timeout) * 100))
                    self._update_progress('backup', progress_pct, f'üì¶ Creating backup of volume: {volume_name}... ({int(elapsed)}s, {size_mb:.1f} MB)')
                    last_progress_update = elapsed
                
                # Log progress periodically (for console)
                if time.time() - last_log_time >= log_interval:
                    current_size = backup_file.stat().st_size if backup_file.exists() else 0
                    size_mb = current_size / (1024 * 1024) if current_size > 0 else 0
                    progress_pct = min(95, int((elapsed / timeout) * 100))
                    
                    if current_size > last_size:
                        self.logger.info(f"Backup progress: {progress_pct}% | Elapsed: {elapsed:.1f}s | Size: {size_mb:.1f} MB | Volume: {volume_name}")
                        last_size = current_size
                    else:
                        self.logger.info(f"Backup progress: {progress_pct}% | Elapsed: {elapsed:.1f}s | Volume: {volume_name}")
                    last_log_time = time.time()
                
                # Wait a bit before next check
                time.sleep(check_interval)
            
            # Get result
            stdout, stderr = process.communicate()
            returncode = process.returncode
            
            # Fix ownership of backup file after container finishes
            if returncode == 0:
                if backup_file.exists():
                    try:
                        # Try to fix ownership - first try without sudo, then with sudo if needed
                        try:
                            # Try direct chown first (might work if file is already accessible)
                            os.chown(backup_file, uid, gid)
                            self.logger.debug(f"Fixed ownership of {backup_file} without sudo")
                        except (PermissionError, OSError):
                            # If direct chown fails, try with sudo if password is available
                            if hasattr(self, '_sudo_password') and self._sudo_password:
                                self._run_sudo_command(['chown', f'{uid}:{gid}', str(backup_file)], timeout=10)
                                self.logger.debug(f"Fixed ownership of {backup_file} with sudo")
                            else:
                                # No sudo password available - log warning but don't fail
                                # Backup was successful, ownership is just a convenience
                                self.logger.warning(f"Could not fix ownership of {backup_file} - no sudo password available")
                                self.logger.warning("Backup file may be owned by root - this is not critical, backup was successful")
                    except Exception as e:
                        # Any other error - log but don't fail (backup was successful)
                        self.logger.warning(f"Could not fix ownership of {backup_file}: {e} - may require manual chown")
                
                self.logger.info(f"Volume {volume_name} backed up successfully using Docker")
                if container_name:
                    self._update_progress('backup', 90, f'‚úÖ Zbackupowano volume: {volume_name}')
                return True
            else:
                # Log stderr but don't fail on socket warnings
                if stderr and 'socket ignored' not in stderr:
                    self.logger.warning(f"Docker volume backup warnings: {stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"Docker volume backup failed: {e}")
            return False
    
    def _cleanup_backup_containers(self):
        """Clean up any leftover backup containers (alpine:latest) that may have been orphaned
        
        This method finds and removes containers using alpine:latest image that are in exited state
        or have been running for too long. These are typically backup containers that weren't
        properly cleaned up due to process interruption.
        """
        try:
            if not self.client:
                return
            
            # Find all containers using alpine:latest image
            # These are likely backup containers that weren't cleaned up
            all_containers = self.client.containers.list(all=True, filters={'ancestor': 'alpine:latest'})
            
            cleaned = 0
            for container in all_containers:
                try:
                    container.reload()  # Refresh container state
                    
                    # Remove exited containers (these are definitely orphaned)
                    if container.status == 'exited':
                        container.remove()
                        cleaned += 1
                        self.logger.debug(f"Cleaned up exited backup container: {container.id[:12]}")
                    elif container.status == 'running':
                        # Check how long it's been running
                        # Normal backup containers should finish in seconds/minutes
                        try:
                            created_str = container.attrs.get('Created', '')
                            if created_str:
                                # Docker timestamps are in ISO format: "2025-12-21T23:22:24.123456789Z"
                                # Remove microseconds and timezone for simpler parsing
                                created_str_clean = created_str.split('.')[0].replace('Z', '')
                                created_time = datetime.strptime(created_str_clean, '%Y-%m-%dT%H:%M:%S')
                                running_time = (datetime.now() - created_time).total_seconds()
                                
                                # If running for more than 10 minutes, it's likely orphaned
                                if running_time > 600:
                                    container.stop(timeout=5)
                                    container.remove()
                                    cleaned += 1
                                    self.logger.debug(f"Cleaned up orphaned backup container (running {running_time:.0f}s): {container.id[:12]}")
                        except (ValueError, TypeError, KeyError) as e:
                            # If we can't parse the timestamp, check if container has been running too long
                            # by checking its uptime attribute if available
                            try:
                                uptime_str = container.attrs.get('State', {}).get('StartedAt', '')
                                if uptime_str:
                                    uptime_clean = uptime_str.split('.')[0].replace('Z', '')
                                    started_time = datetime.strptime(uptime_clean, '%Y-%m-%dT%H:%M:%S')
                                    running_time = (datetime.now() - started_time).total_seconds()
                                    if running_time > 600:
                                        container.stop(timeout=5)
                                        container.remove()
                                        cleaned += 1
                                        self.logger.debug(f"Cleaned up orphaned backup container (running {running_time:.0f}s): {container.id[:12]}")
                            except (ValueError, TypeError, KeyError):
                                # If we still can't determine, just log and skip
                                self.logger.debug(f"Could not determine container age, skipping: {container.id[:12]}")
                except docker.errors.NotFound:
                    # Container was already removed, skip
                    pass
                except Exception as e:
                    self.logger.debug(f"Could not clean up container {container.id[:12]}: {e}")
            
            if cleaned > 0:
                self.logger.info(f"Cleaned up {cleaned} orphaned backup container(s)")
        except Exception as e:
            self.logger.debug(f"Error cleaning up backup containers: {e}")
    
    def _backup_bind_mount_using_docker(self, source_path: str, backup_file: Path, container_name: str = None) -> bool:
        """Backup bind mount directory using a temporary Docker container (no sudo needed!)
        
        This method uses Docker container to backup directories, which is faster and avoids
        permission issues. The container runs as root and can access the directory data.
        
        Args:
            source_path: Path to the directory on host to backup
            backup_file: Path to the backup file to create
            container_name: Container name for cancel flag checking
        """
        try:
            import subprocess
            
            source = Path(source_path)
            if not source.exists():
                self.logger.warning(f"Source path does not exist: {source_path}")
                return False
            
            # Get current user UID and GID for ownership fix
            uid = os.getuid()
            gid = os.getgid()
            
            # Use docker run to create tar backup of directory
            # This runs as root inside container, so no permission issues
            # Mount the parent directory and backup the child directory name
            source_parent = str(source.parent.absolute())
            source_name = source.name
            
            process = subprocess.Popen(
                [
                    'docker', 'run', '--rm',
                    '-v', f'{source_parent}:/source:ro',  # Mount parent dir as read-only
                    '-v', f'{backup_file.parent.absolute()}:/backup',  # Mount backup dir
                    'alpine:latest',  # Lightweight image
                    'sh', '-c',
                    f'tar -czf /backup/{backup_file.name} -C /source {source_name} 2>/dev/null || true'
                ],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # Wait with periodic cancel checks and progress updates
            timeout = 600  # 10 minutes timeout (large directories like influxdb)
            start_time = time.time()
            check_interval = 2  # Check cancel flag every 2 seconds
            last_progress_update = 0
            progress_update_interval = 5  # Update progress every 5 seconds
            
            while True:
                elapsed = time.time() - start_time
                if elapsed > timeout:
                    process.terminate()
                    try:
                        process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        process.kill()
                    self.logger.error(f"Bind mount backup timed out for {source_path}")
                    # Clean up any orphaned backup containers
                    self._cleanup_backup_containers()
                    if container_name:
                        self._update_progress('backup', 95, f'‚ùå Backup timeout for {source.name}')
                    return False
                
                # Check for cancellation
                if container_name and self._check_cancel_flag(container_name):
                    self.logger.warning(f"Backup cancelled during bind mount backup: {source_path}")
                    process.terminate()
                    try:
                        process.wait(timeout=5)
                    except subprocess.TimeoutExpired:
                        process.kill()
                    # Clean up any orphaned backup containers
                    self._cleanup_backup_containers()
                    if container_name:
                        progress_pct = min(90, int((elapsed / timeout) * 100))
                        self._update_progress('backup', progress_pct, f'‚ö†Ô∏è Backup cancelled: {source.name}')
                    return False
                
                # Update progress periodically during backup
                if container_name and elapsed - last_progress_update >= progress_update_interval:
                    progress_pct = min(90, int((elapsed / timeout) * 100))
                    self._update_progress('backup', progress_pct, f'üì¶ Creating backup of {source.name}... ({int(elapsed)}s)')
                    last_progress_update = elapsed
                
                # Check if process finished
                if process.poll() is not None:
                    break
                
                # Wait a bit before next check
                time.sleep(check_interval)
            
            # Get result
            stdout, stderr = process.communicate()
            returncode = process.returncode
            
            # Fix ownership of backup file after container finishes
            if returncode == 0 and backup_file.exists():
                try:
                    # Try to fix ownership - first try without sudo, then with sudo if needed
                    try:
                        # Try direct chown first (might work if file is already accessible)
                        os.chown(backup_file, uid, gid)
                        self.logger.debug(f"Fixed ownership of {backup_file} without sudo")
                    except (PermissionError, OSError):
                        # If direct chown fails, try with sudo if password is available
                        if hasattr(self, '_sudo_password') and self._sudo_password:
                            self._run_sudo_command(['chown', f'{uid}:{gid}', str(backup_file)], timeout=10)
                            self.logger.debug(f"Fixed ownership of {backup_file} with sudo")
                        else:
                            # No sudo password available - log warning but don't fail
                            # Backup was successful, ownership is just a convenience
                            self.logger.warning(f"Could not fix ownership of {backup_file} - no sudo password available")
                            self.logger.warning("Backup file may be owned by root - this is not critical, backup was successful")
                except Exception as e:
                    # Any other error - log but don't fail (backup was successful)
                    self.logger.warning(f"Could not fix ownership of {backup_file}: {e} - may require manual chown")
                
                self.logger.info(f"Bind mount {source_path} backed up successfully using Docker")
                return True
            else:
                # If Docker method failed, fall back to direct tar with timeout
                self.logger.info(f"Docker backup method failed, falling back to direct tar for {source_path}")
                return self._backup_directory(source_path, backup_file, container_name)
                
        except Exception as e:
            self.logger.error(f"Docker bind mount backup failed: {e}, falling back to direct method")
            return self._backup_directory(source_path, backup_file, container_name)
    
    def _backup_directory(self, source_path: str, backup_file: Path, container_name: str = None) -> bool:
        """Backup a directory to tar.gz file using tar command with timeout
        
        Uses sudo for paths that require elevated privileges (like /var/lib/docker/volumes/).
        Uses tar command instead of tarfile module to avoid hanging on large directories.
        """
        try:
            import subprocess
            import threading
            
            source = Path(source_path)
            if not source.exists():
                self.logger.warning(f"Source path does not exist: {source_path}")
                return False
            
            # Check if path requires sudo (Docker volume paths or system paths)
            requires_sudo = (
                str(source_path).startswith('/var/lib/docker/volumes/') or
                str(source_path).startswith('/var/lib/docker/') or
                str(source_path).startswith('/root/') or
                not os.access(source_path, os.R_OK)  # Check if we can read without sudo
            )
            
            timeout = 600  # 10 minutes timeout for large directories
            tar_cmd = ['tar', '-czf', str(backup_file), '-C', str(source.parent), source.name]
            
            if requires_sudo:
                self.logger.info(f"Using sudo for backup of privileged path: {source_path}")
                # Use _run_sudo_command to pass password if available
                if hasattr(self, '_sudo_password') and self._sudo_password:
                    # For long-running operations like tar, use Popen with password passing
                    # instead of communicate() which may not work well for long operations
                    password_bytes = (self._sudo_password + '\n').encode('utf-8')
                    sudo_cmd = ['sudo', '-S'] + tar_cmd  # -S reads password from stdin
                    
                    try:
                        process = subprocess.Popen(
                            sudo_cmd,
                            stdin=subprocess.PIPE,
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE,
                            text=False  # Use binary mode for better performance
                        )
                        # Send password immediately
                        process.stdin.write(password_bytes)
                        process.stdin.close()
                        
                        # Wait with periodic cancel checks and progress updates
                        start_time = time.time()
                        check_interval = 2
                        last_progress_update = 0
                        progress_update_interval = 5  # Update progress every 5 seconds
                        
                        while True:
                            elapsed = time.time() - start_time
                            if elapsed > timeout:
                                process.terminate()
                                try:
                                    process.wait(timeout=5)
                                except subprocess.TimeoutExpired:
                                    process.kill()
                                self.logger.error(f"Backup timed out for {source_path}")
                                if container_name:
                                    self._update_progress('backup', 95, f'‚ùå Backup timeout for {Path(source_path).name}')
                                return False
                            
                            # Check for cancellation
                            if container_name and self._check_cancel_flag(container_name):
                                self.logger.warning(f"Backup cancelled during directory backup: {source_path}")
                                process.terminate()
                                try:
                                    process.wait(timeout=5)
                                except subprocess.TimeoutExpired:
                                    process.kill()
                                if container_name:
                                    self._update_progress('backup', int((elapsed / timeout) * 100), f'‚ö†Ô∏è Backup cancelled: {Path(source_path).name}')
                                return False
                            
                            # Update progress periodically during backup
                            if container_name and elapsed - last_progress_update >= progress_update_interval:
                                progress_pct = min(90, int((elapsed / timeout) * 100))
                                self._update_progress('backup', progress_pct, f'üì¶ Creating backup of {Path(source_path).name}... ({int(elapsed)}s)')
                                last_progress_update = elapsed
                            
                            # Check if process finished
                            if process.poll() is not None:
                                break
                            
                            # Wait a bit before next check
                            time.sleep(check_interval)
                        
                        # Get result
                        stdout, stderr = process.communicate()
                        returncode = process.returncode
                        
                        if returncode == 0:
                            # Fix ownership of created backup file
                            if backup_file.exists():
                                try:
                                    self._run_sudo_command(['chown', f"{os.getuid()}:{os.getgid()}", str(backup_file)], timeout=10)
                                except:
                                    pass  # Ignore chown errors
                            return True
                        else:
                            error_msg = stderr.decode('utf-8', errors='ignore').strip() if stderr else "Unknown error"
                            # Check if sudo prompted for password (this shouldn't happen with -S)
                            if 'password' in error_msg.lower() or '[sudo] password' in error_msg.lower():
                                self.logger.error(f"Sudo password prompt appeared during backup - password may be incorrect or not passed correctly")
                                self.logger.error(f"Command: {' '.join(sudo_cmd)}")
                                self.logger.error(f"Error: {error_msg}")
                                self.logger.error("This should not happen - password should be passed via stdin with -S flag")
                            self.logger.error(f"Tar backup failed for {source_path}: {error_msg}")
                            return False
                    except Exception as e:
                        self.logger.error(f"Failed to run sudo tar backup: {e}")
                        self.logger.error("If you see a sudo prompt, it means sudo was called directly without using _run_sudo_command")
                        return False
                else:
                    # No password available - but we need sudo
                    # This should not happen in web interface, but if it does, log warning and fail
                    self.logger.error(f"Sudo password required for backup of {source_path} but password not available")
                    self.logger.error("This should not happen in web interface - password should be set from session")
                    self.logger.error("If you see a sudo prompt, it means sudo was called directly without using _run_sudo_command")
                    self.console.print(f"[red]‚ùå Sudo password required but not available. Cannot backup {source_path}[/red]")
                    self.console.print(f"[red]Please provide sudo password via the web interface modal[/red]")
                    return False
            else:
                self.logger.info(f"Using direct tar for backup of: {source_path}")
            
            # Show progress bar for non-sudo backup as well
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                TimeElapsedColumn(),
                TimeRemainingColumn(),
                console=self.console
            ) as progress:
                backup_task = progress.add_task(
                    f"üì¶ Backing up {source.name}...",
                    total=100
                )
                
                # Use Popen for better cancellation support (for non-sudo or when password not available)
                process = subprocess.Popen(
                    tar_cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )
                
                # Wait with periodic cancel checks and progress updates
                start_time = time.time()
                check_interval = 2  # Check cancel flag every 2 seconds
                last_size = 0
                last_progress_update = 0
                progress_update_interval = 5  # Update progress every 5 seconds
                
                while True:
                    elapsed = time.time() - start_time
                    if elapsed > timeout:
                        process.terminate()
                        try:
                            process.wait(timeout=5)
                        except subprocess.TimeoutExpired:
                            process.kill()
                        progress.update(backup_task, description="‚ùå Backup timed out")
                        self.logger.error(f"Backup timed out for {source_path}")
                        if container_name:
                            self._update_progress('backup', 95, f'‚ùå Backup timeout for {source.name}')
                        return False
                    
                    # Check for cancellation
                    if container_name and self._check_cancel_flag(container_name):
                        self.logger.warning(f"Backup cancelled during directory backup: {source_path}")
                        process.terminate()
                        try:
                            process.wait(timeout=5)
                        except subprocess.TimeoutExpired:
                            process.kill()
                        progress.update(backup_task, description="‚ö†Ô∏è Backup cancelled")
                        if container_name:
                            progress_pct = min(90, int((elapsed / timeout) * 100))
                            self._update_progress('backup', progress_pct, f'‚ö†Ô∏è Backup cancelled: {source.name}')
                        return False
                    
                    # Update progress periodically during backup (for web interface)
                    if container_name and elapsed - last_progress_update >= progress_update_interval:
                        progress_pct = min(90, int((elapsed / timeout) * 100))
                        self._update_progress('backup', progress_pct, f'üì¶ Creating backup of {source.name}... ({int(elapsed)}s)')
                        last_progress_update = elapsed
                    
                    # Check if process finished
                    if process.poll() is not None:
                        progress.update(backup_task, completed=100, description="‚úÖ Backup completed")
                        break
                    
                    # Update progress based on file size growth (for console)
                    if backup_file.exists():
                        current_size = backup_file.stat().st_size
                        if current_size > last_size:
                            # Estimate progress based on time elapsed vs timeout
                            # This is a rough estimate since we don't know total size
                            progress_pct = min(95, int((elapsed / timeout) * 100))
                            progress.update(backup_task, completed=progress_pct)
                            last_size = current_size
                    
                    # Wait a bit before next check
                    time.sleep(check_interval)
            
                # Get result
                stdout, stderr = process.communicate()
                returncode = process.returncode
                
                if returncode == 0:
                    if requires_sudo and backup_file.exists():
                        # Fix ownership of created backup file
                        try:
                            self._run_sudo_command(['chown', f"{os.getuid()}:{os.getgid()}", str(backup_file)], timeout=10)
                        except:
                            pass  # Ignore chown errors
                    return True
                else:
                    error_msg = stderr.strip() if stderr else "Unknown error"
                    self.logger.error(f"Tar backup failed for {source_path}: {error_msg}")
                    return False
                    
        except Exception as e:
            self.logger.error(f"Failed to create tar backup: {e}")
            return False
    
    def _run_sudo_command(self, command_args, timeout=10, check=False):
        """Run sudo command with password if available
        
        Args:
            command_args: List of command arguments (without 'sudo')
            timeout: Command timeout in seconds
            check: If True, raise exception on non-zero return code
        
        Raises:
            RuntimeError: If sudo password is required but not available
            subprocess.TimeoutExpired: If command times out and check=True
            subprocess.CalledProcessError: If command fails and check=True
        """
        sudo_cmd = ['sudo'] + command_args
        
        # If password is available (from web session), use it
        if hasattr(self, '_sudo_password') and self._sudo_password:
            # Use subprocess with stdin to pass password to sudo -S (read from stdin)
            # -S makes sudo read password from stdin
            # We pass password + newline to stdin
            password_bytes = (self._sudo_password + '\n').encode('utf-8')
            
            try:
                sudo_process = subprocess.Popen(
                    sudo_cmd + ['-S'],  # -S reads password from stdin
                    stdin=subprocess.PIPE,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE
                )
                stdout, stderr = sudo_process.communicate(input=password_bytes, timeout=timeout)
                returncode = sudo_process.returncode
                
                # Check if sudo prompted for password (this shouldn't happen with -S)
                if stderr and b'password' in stderr.lower() and returncode != 0:
                    error_msg = stderr.decode('utf-8', errors='ignore').strip()
                    self.logger.error(f"Sudo command failed - password may be incorrect or sudo prompt appeared: {error_msg}")
                    if check:
                        raise RuntimeError(f"Sudo command failed: {error_msg}")
            except subprocess.TimeoutExpired:
                sudo_process.kill()
                stdout, stderr = sudo_process.communicate()
                returncode = -1
                if check:
                    raise subprocess.TimeoutExpired(sudo_cmd, timeout)
        else:
            # No password available - this should not happen in web interface
            # Log warning and return error instead of prompting
            self.logger.error(f"Sudo password required for command {command_args} but password not available")
            self.logger.error("This should not happen in web interface - password should be set from session")
            self.logger.error("If you see a sudo prompt, it means sudo was called directly without using _run_sudo_command")
            raise RuntimeError(f"Sudo password required but not available. Cannot execute: {' '.join(command_args)}")
        
        if check and returncode != 0:
            error_msg = stderr.decode('utf-8', errors='ignore').strip() if stderr else "Unknown error"
            raise subprocess.CalledProcessError(returncode, sudo_cmd, stdout, stderr)
        
        return subprocess.CompletedProcess(sudo_cmd, returncode, stdout, stderr)
    
    def restore_container_data(self, container_name: str, backup_path: str) -> bool:
        """
        Restore container data from backup.
        
        Args:
            container_name: Name of the container to restore data to
            backup_path: Path to backup directory
        
        Returns:
            bool: True if restore successful
        """
        try:
            backup_dir = Path(backup_path)
            if not backup_dir.exists():
                self.console.print(f"[red]‚ùå Backup directory not found: {backup_path}[/red]")
                return False
            
            metadata_file = backup_dir / 'backup_metadata.json'
            if not metadata_file.exists():
                self.console.print(f"[red]‚ùå Backup metadata not found: {metadata_file}[/red]")
                return False
            
            with open(metadata_file, 'r') as f:
                backup_metadata = json.load(f)
            
            self.console.print(f"[cyan]üì¶ Restoring data for container '{container_name}' from backup...[/cyan]")
            self.console.print(f"[cyan]Backup created: {backup_metadata.get('backup_time', 'unknown')}[/cyan]")
            
            container = self.client.containers.get(container_name)
            mounts = container.attrs.get('Mounts', [])
            
            # Show loading indicator during restore
            with self._with_loading("Restoring container data"):
                # Restore each volume
                for volume_info in backup_metadata.get('volumes', []):
                    backup_file = Path(volume_info['backup_file'])
                    if not backup_file.exists():
                        # Try relative to backup_dir
                        backup_file = backup_dir / backup_file.name
                    
                    if not backup_file.exists():
                        self.console.print(f"[yellow]‚ö†Ô∏è Backup file not found: {volume_info['backup_file']}[/yellow]")
                        continue
                    
                    if volume_info['type'] == 'named_volume':
                        volume_name = volume_info['name']
                        self.console.print(f"[cyan]Restoring named volume: {volume_name}[/cyan]")
                        
                        try:
                            volume = self.client.volumes.get(volume_name)
                            volume_path = volume.attrs['Mountpoint']
                            
                            # Extract backup to volume
                            self._restore_from_tar(backup_file, volume_path)
                            self.console.print(f"[green]‚úÖ Restored volume '{volume_name}'[/green]")
                        except Exception as e:
                            self.logger.error(f"Failed to restore volume {volume_name}: {e}")
                            self.console.print(f"[red]‚ùå Failed to restore volume '{volume_name}': {e}[/red]")
                            return False
                    
                    elif volume_info['type'] == 'bind_mount':
                        source_path = volume_info['source']
                        self.console.print(f"[cyan]Restoring bind mount: {source_path}[/cyan]")
                        
                        try:
                            if Path(source_path).exists():
                                # Backup existing data first
                                existing_backup = Path(source_path).parent / f"{Path(source_path).name}.backup_{int(time.time())}"
                                if Path(source_path).is_dir():
                                    import shutil
                                    shutil.move(str(source_path), str(existing_backup))
                                    Path(source_path).mkdir(parents=True, exist_ok=True)
                                
                                # Extract backup
                                self._restore_from_tar(backup_file, source_path)
                                self.console.print(f"[green]‚úÖ Restored bind mount '{source_path}'[/green]")
                            else:
                                self.console.print(f"[yellow]‚ö†Ô∏è Bind mount path does not exist: {source_path}[/yellow]")
                        except Exception as e:
                            self.logger.error(f"Failed to restore bind mount {source_path}: {e}")
                            self.console.print(f"[red]‚ùå Failed to restore bind mount '{source_path}': {e}[/red]")
                            return False
            
            self.console.print(f"[bold green]‚úÖ Data restore completed![/bold green]")
            return True
            
        except docker.errors.NotFound:
            self.console.print(f"[red]‚ùå Container '{container_name}' not found[/red]")
            return False
        except Exception as e:
            self.logger.error(f"Container data restore failed: {e}")
            self.console.print(f"[red]‚ùå Restore failed: {e}[/red]")
            return False
    
    def _restore_from_tar(self, tar_file: Path, destination: str) -> bool:
        """Extract tar.gz file to destination"""
        try:
            import tarfile
            
            destination_path = Path(destination)
            destination_path.mkdir(parents=True, exist_ok=True)
            
            with tarfile.open(tar_file, 'r:gz') as tar:
                tar.extractall(path=destination_path.parent)
            
            return True
        except Exception as e:
            self.logger.error(f"Failed to extract tar backup: {e}")
            return False
    
    def _migrate_container_data(self, source_container, target_container, config: DeploymentConfig) -> bool:
        """Migrate data from source container to target container during blue-green deployment
        
        This function copies data from the active container to the new container to ensure
        data persistence during blue-green deployment.
        
        Args:
            source_container: The active (source) container to migrate data from
            target_container: The new (target) container to migrate data to
            config: Deployment configuration
            
        Returns:
            bool: True if migration successful or not needed, False on error
        """
        try:
            if not source_container:
                self.logger.info("No source container to migrate data from")
                return True
            
            self.console.print(f"[cyan]üì¶ Migrating data from '{source_container.name}' to '{target_container.name}'...[/cyan]")
            
            # Get mounts from both containers
            source_mounts = source_container.attrs.get('Mounts', [])
            target_mounts = target_container.attrs.get('Mounts', [])
            
            if not source_mounts:
                self.logger.info("No mounts in source container to migrate")
                return True
            
            # Create mapping of mount points
            source_volumes = {}
            for mount in source_mounts:
                volume_name = mount.get('Name')
                mount_point = mount.get('Destination')
                source_path = mount.get('Source')
                
                if volume_name:
                    source_volumes[mount_point] = {'type': 'named_volume', 'name': volume_name, 'source': None}
                elif source_path:
                    source_volumes[mount_point] = {'type': 'bind_mount', 'name': None, 'source': source_path}
            
            target_volumes = {}
            for mount in target_mounts:
                volume_name = mount.get('Name')
                mount_point = mount.get('Destination')
                source_path = mount.get('Source')
                
                if volume_name:
                    target_volumes[mount_point] = {'type': 'named_volume', 'name': volume_name, 'source': None}
                elif source_path:
                    target_volumes[mount_point] = {'type': 'bind_mount', 'name': None, 'source': source_path}
            
            # System paths to skip
            system_paths_to_skip = [
                '/lib/modules', '/proc', '/sys', '/dev', '/run', '/tmp', '/var/run', '/boot'
            ]
            
            migrated_count = 0
            skipped_count = 0
            
            # Migrate each volume
            for mount_point, source_info in source_volumes.items():
                # Skip system paths
                skip = False
                if source_info['source']:
                    for system_path in system_paths_to_skip:
                        if str(source_info['source']).startswith(system_path):
                            skip = True
                            break
                
                if skip:
                    skipped_count += 1
                    continue
                
                # Check if target has the same mount point
                if mount_point not in target_volumes:
                    self.logger.warning(f"Mount point '{mount_point}' not found in target container, skipping")
                    continue
                
                target_info = target_volumes[mount_point]
                
                # Handle named volumes - copy data between volumes
                if source_info['type'] == 'named_volume' and target_info['type'] == 'named_volume':
                    source_volume_name = source_info['name']
                    target_volume_name = target_info['name']
                    
                    # If volumes are the same, no migration needed
                    if source_volume_name == target_volume_name:
                        self.logger.info(f"Volume '{source_volume_name}' is shared, no migration needed")
                        continue
                    
                    self.console.print(f"[cyan]Migrating named volume: {source_volume_name} -> {target_volume_name}[/cyan]")
                    
                    # Copy data using Docker container
                    success = self._copy_volume_data(source_volume_name, target_volume_name, config.container_name)
                    if success:
                        migrated_count += 1
                        self.console.print(f"[green]‚úÖ Migrated volume '{source_volume_name}' to '{target_volume_name}'[/green]")
                    else:
                        self.logger.warning(f"Failed to migrate volume '{source_volume_name}', continuing...")
                
                # Handle bind mounts - check if same source path (data is already shared)
                elif source_info['type'] == 'bind_mount' and target_info['type'] == 'bind_mount':
                    source_path = source_info['source']
                    target_path = target_info['source']
                    
                    # If same path, data is already available
                    if source_path == target_path:
                        self.logger.info(f"Bind mount '{source_path}' is shared, no migration needed")
                        continue
                    
                    # If different paths, copy data
                    self.console.print(f"[cyan]Migrating bind mount: {source_path} -> {target_path}[/cyan]")
                    
                    if Path(source_path).exists():
                        success = self._copy_bind_mount_data(source_path, target_path, config.container_name)
                        if success:
                            migrated_count += 1
                            self.console.print(f"[green]‚úÖ Migrated bind mount '{source_path}' to '{target_path}'[/green]")
                        else:
                            self.logger.warning(f"Failed to migrate bind mount '{source_path}', continuing...")
                    else:
                        self.logger.warning(f"Source bind mount path does not exist: {source_path}")
            
            # Copy internal configuration files for databases
            db_config = self._get_database_config(config.image_tag)
            
            if db_config:
                self.console.print(f"[cyan]üìã Detected database container, migrating configuration files...[/cyan]")
                
                # Get config paths from database configuration
                config_paths = db_config.get('config_paths', [])
                
                for config_path in config_paths:
                    success = self._copy_container_files(source_container, target_container, config_path, config.container_name)
                    if success:
                        self.console.print(f"[green]‚úÖ Migrated config from '{config_path}'[/green]")
            
            self.console.print(f"[green]‚úÖ Data migration completed: {migrated_count} volumes migrated, {skipped_count} skipped[/green]")
            return True
            
        except Exception as e:
            self.logger.error(f"Data migration failed: {e}")
            self.console.print(f"[yellow]‚ö†Ô∏è Data migration failed: {e}, continuing deployment...[/yellow]")
            # Don't fail deployment if migration fails - just log warning
            return True  # Return True to not block deployment
    
    def _copy_volume_data(self, source_volume_name: str, target_volume_name: str, container_name: str = None) -> bool:
        """Copy data from source named volume to target named volume using Docker"""
        try:
            import subprocess
            
            # Use Docker container to copy data between volumes
            # This runs as root inside container, so no permission issues
            result = subprocess.run(
                [
                    'docker', 'run', '--rm',
                    '-v', f'{source_volume_name}:/source:ro',  # Mount source volume as read-only
                    '-v', f'{target_volume_name}:/target',      # Mount target volume
                    'alpine:latest',  # Lightweight image
                    'sh', '-c',
                    'cp -a /source/. /target/ 2>/dev/null || true'
                ],
                capture_output=True,
                text=True,
                timeout=600  # 10 minutes timeout for large volumes
            )
            
            if result.returncode == 0:
                self.logger.info(f"Successfully copied data from volume '{source_volume_name}' to '{target_volume_name}'")
                return True
            else:
                self.logger.warning(f"Volume copy warnings: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            self.logger.error(f"Volume copy timed out for {source_volume_name} -> {target_volume_name}")
            return False
        except Exception as e:
            self.logger.error(f"Failed to copy volume data: {e}")
            return False
    
    def _copy_bind_mount_data(self, source_path: str, target_path: str, container_name: str = None) -> bool:
        """Copy data from source bind mount path to target bind mount path"""
        try:
            import subprocess
            import shutil
            
            source = Path(source_path)
            target = Path(target_path)
            
            if not source.exists():
                self.logger.warning(f"Source path does not exist: {source_path}")
                return False
            
            # Create target directory if it doesn't exist
            target.mkdir(parents=True, exist_ok=True)
            
            # Use rsync if available, otherwise use cp
            if shutil.which('rsync'):
                result = subprocess.run(
                    ['rsync', '-a', '--info=progress2', f'{source_path}/', f'{target_path}/'],
                    capture_output=True,
                    text=True,
                    timeout=600
                )
            else:
                result = subprocess.run(
                    ['cp', '-a', f'{source_path}/.', f'{target_path}/'],
                    capture_output=True,
                    text=True,
                    timeout=600
                )
            
            if result.returncode == 0:
                self.logger.info(f"Successfully copied bind mount data from '{source_path}' to '{target_path}'")
                return True
            else:
                self.logger.warning(f"Bind mount copy failed: {result.stderr}")
                return False
                
        except Exception as e:
            self.logger.error(f"Failed to copy bind mount data: {e}")
            return False
    
    def _copy_container_files(self, source_container, target_container, source_path: str, container_name: str = None) -> bool:
        """Copy files from source container to target container using docker cp"""
        try:
            import subprocess
            import tempfile
            
            # Create temporary tar file
            with tempfile.NamedTemporaryFile(suffix='.tar', delete=False) as tmp_tar:
                tmp_tar_path = tmp_tar.name
            
            try:
                # Copy from source container to tar
                result1 = subprocess.run(
                    ['docker', 'cp', f'{source_container.name}:{source_path}', '-'],
                    stdout=open(tmp_tar_path, 'wb'),
                    stderr=subprocess.PIPE,
                    timeout=300
                )
                
                if result1.returncode != 0:
                    self.logger.warning(f"Failed to copy from source container: {result1.stderr.decode()}")
                    return False
                
                # Extract tar to target container
                result2 = subprocess.run(
                    ['docker', 'cp', '-', f'{target_container.name}:{Path(source_path).parent}/'],
                    stdin=open(tmp_tar_path, 'rb'),
                    stderr=subprocess.PIPE,
                    timeout=300
                )
                
                if result2.returncode == 0:
                    self.logger.info(f"Successfully copied files from '{source_path}' between containers")
                    return True
                else:
                    self.logger.warning(f"Failed to copy to target container: {result2.stderr.decode()}")
                    return False
                    
            finally:
                # Cleanup temp file
                try:
                    Path(tmp_tar_path).unlink()
                except:
                    pass
                    
        except Exception as e:
            self.logger.error(f"Failed to copy container files: {e}")
            return False

    def backup_deployment_state(self, backup_path: str = None) -> bool:
        """Create backup of current deployment state"""
        if not backup_path:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = f"backup_{timestamp}"
        
        backup_dir = Path(backup_path)
        backup_dir.mkdir(exist_ok=True)
        
        try:
            # Backup running containers info
            containers = self.client.containers.list(all=True)
            containers_backup = []
            
            for container in containers:
                container_info = {
                    'name': container.name,
                    'image': container.image.tags[0] if container.image.tags else container.image.id,
                    'status': container.status,
                    'ports': container.ports,
                    'environment': container.attrs.get('Config', {}).get('Env', []),
                    'volumes': container.attrs.get('Mounts', []),
                    'command': container.attrs.get('Config', {}).get('Cmd'),
                    'created': container.attrs.get('Created'),
                    'restart_policy': container.attrs.get('HostConfig', {}).get('RestartPolicy', {})
                }
                containers_backup.append(container_info)
            
            # Save containers backup
            with open(backup_dir / 'containers.json', 'w') as f:
                json.dump(containers_backup, f, indent=2)
            
            # Backup Docker images
            images = self.client.images.list()
            images_backup = []
            
            for image in images:
                if image.tags:  # Only backup tagged images
                    image_info = {
                        'tags': image.tags,
                        'id': image.id,
                        'created': image.attrs.get('Created'),
                        'size': image.attrs.get('Size')
                    }
                    images_backup.append(image_info)
            
            with open(backup_dir / 'images.json', 'w') as f:
                json.dump(images_backup, f, indent=2)
            
            # Backup networks
            networks = self.client.networks.list()
            networks_backup = []
            
            for network in networks:
                if not network.name.startswith(('bridge', 'host', 'none')):  # Skip default networks
                    network_info = {
                        'name': network.name,
                        'driver': network.attrs.get('Driver'),
                        'options': network.attrs.get('Options', {}),
                        'labels': network.attrs.get('Labels', {}),
                        'created': network.attrs.get('Created')
                    }
                    networks_backup.append(network_info)
            
            with open(backup_dir / 'networks.json', 'w') as f:
                json.dump(networks_backup, f, indent=2)
            
            # Backup volumes
            volumes = self.client.volumes.list()
            volumes_backup = []
            
            for volume in volumes:
                volume_info = {
                    'name': volume.name,
                    'driver': volume.attrs.get('Driver'),
                    'mountpoint': volume.attrs.get('Mountpoint'),
                    'labels': volume.attrs.get('Labels', {}),
                    'created': volume.attrs.get('CreatedAt')
                }
                volumes_backup.append(volume_info)
            
            with open(backup_dir / 'volumes.json', 'w') as f:
                json.dump(volumes_backup, f, indent=2)
            
            # Create backup summary
            summary = {
                'backup_time': datetime.now().isoformat(),
                'containers_count': len(containers_backup),
                'images_count': len(images_backup),
                'networks_count': len(networks_backup),
                'volumes_count': len(volumes_backup),
                'docker_version': self.client.version()['Version']
            }
            
            with open(backup_dir / 'summary.json', 'w') as f:
                json.dump(summary, f, indent=2)
            
            self.console.print(f"[green]Deployment state backed up to {backup_path}/[/green]")
            self.console.print(f"[cyan]Backup contains: {len(containers_backup)} containers, {len(images_backup)} images[/cyan]")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Backup failed: {e}")
            return False

    def restore_deployment_state(self, backup_path: str) -> bool:
        """Restore deployment state from backup"""
        backup_dir = Path(backup_path)
        
        if not backup_dir.exists():
            self.console.print(f"[red]Backup directory not found: {backup_path}[/red]")
            return False
        
        try:
            # Load backup summary
            with open(backup_dir / 'summary.json', 'r') as f:
                summary = json.load(f)
            
            self.console.print(f"[cyan]Restoring backup from {summary['backup_time']}[/cyan]")
            
            # Restore networks first
            if (backup_dir / 'networks.json').exists():
                with open(backup_dir / 'networks.json', 'r') as f:
                    networks = json.load(f)
                
                for network_info in networks:
                    try:
                        self.client.networks.create(
                            name=network_info['name'],
                            driver=network_info['driver'],
                            options=network_info.get('options', {}),
                            labels=network_info.get('labels', {})
                        )
                        self.console.print(f"[green]Restored network: {network_info['name']}[/green]")
                    except docker.errors.APIError as e:
                        if "already exists" in str(e):
                            continue
                        self.logger.warning(f"Failed to restore network {network_info['name']}: {e}")
            
            # Restore volumes
            if (backup_dir / 'volumes.json').exists():
                with open(backup_dir / 'volumes.json', 'r') as f:
                    volumes = json.load(f)
                
                for volume_info in volumes:
                    try:
                        self.client.volumes.create(
                            name=volume_info['name'],
                            driver=volume_info['driver'],
                            labels=volume_info.get('labels', {})
                        )
                        self.console.print(f"[green]Restored volume: {volume_info['name']}[/green]")
                    except docker.errors.APIError as e:
                        if "already exists" in str(e):
                            continue
                        self.logger.warning(f"Failed to restore volume {volume_info['name']}: {e}")
            
            # Note: Images and containers would need more complex restoration logic
            # This is a simplified implementation
            self.console.print("[yellow]Note: Complete container restoration requires image availability[/yellow]")
            self.console.print("[yellow]Consider using docker save/load for complete image backup[/yellow]")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Restore failed: {e}")
            return False

    def validate_system_requirements(self) -> bool:
        """Validate system requirements and dependencies"""
        self.console.print("[cyan]Validating system requirements...[/cyan]")
        
        requirements_met = True
        
        # Check Python version
        python_version = sys.version_info
        if python_version < (3, 8):
            self.console.print("[red]‚ùå Python 3.8+ required[/red]")
            requirements_met = False
        else:
            self.console.print(f"[green]‚úì Python {python_version.major}.{python_version.minor}[/green]")
        
        # Check Docker connectivity
        try:
            docker_version = self.client.version()
            self.console.print(f"[green]‚úì Docker {docker_version['Version']}[/green]")
        except Exception as e:
            self.console.print(f"[red]‚ùå Docker connection failed: {e}[/red]")
            requirements_met = False
        
        # Check required modules
        required_modules = [
            'docker', 'yaml', 'requests', 'rich', 'pathlib'
        ]
        
        for module in required_modules:
            try:
                __import__(module)
                self.console.print(f"[green]‚úì Module {module}[/green]")
            except ImportError:
                self.console.print(f"[red]‚ùå Module {module} not found[/red]")
                requirements_met = False
        
        # Check disk space
        try:
            import shutil
            disk_usage = shutil.disk_usage('.')
            free_gb = disk_usage.free / (1024**3)
            
            if free_gb < 1:  # Require at least 1GB free space
                self.console.print(f"[red]‚ùå Insufficient disk space: {free_gb:.1f}GB[/red]")
                requirements_met = False
            else:
                self.console.print(f"[green]‚úì Disk space: {free_gb:.1f}GB available[/green]")
                
        except Exception:
            self.console.print("[yellow]‚ö†Ô∏è Could not check disk space[/yellow]")
        
        # Check Docker daemon permissions
        try:
            self.client.ping()
            self.console.print("[green]‚úì Docker daemon accessible[/green]")
        except Exception:
            self.console.print("[red]‚ùå Docker daemon permission denied[/red]")
            self.console.print("[yellow]Try: sudo usermod -aG docker $USER[/yellow]")
            requirements_met = False
        
        if requirements_met:
            self.console.print("\n[bold green]‚úÖ All system requirements met![/bold green]")
        else:
            self.console.print("\n[bold red]‚ùå Some requirements not met. Please fix and retry.[/bold red]")
        
        return requirements_met

    def export_configuration(self, config_name: str = "docker-pilot-config.tar.gz") -> bool:
        """Export all configuration files as a backup"""
        try:
            import tarfile
            
            config_files = [
                "deployment.yml",
                "alerts.yml", 
                "integration-tests.yml",
                "docker_pilot.log",
                "docker_metrics.json",
                "deployment_history.json"
            ]
            
            with tarfile.open(config_name, "w:gz") as tar:
                for config_file in config_files:
                    if Path(config_file).exists():
                        tar.add(config_file)
                        self.console.print(f"[green]Added {config_file}[/green]")
            
            self.console.print(f"[bold green]Configuration exported to {config_name}[/bold green]")
            return True
            
        except Exception as e:
            self.logger.error(f"Configuration export failed: {e}")
            return False

    def import_configuration(self, config_archive: str) -> bool:
        """Import configuration from backup archive"""
        try:
            import tarfile
            
            if not Path(config_archive).exists():
                self.console.print(f"[red]Archive not found: {config_archive}[/red]")
                return False
            
            with tarfile.open(config_archive, "r:gz") as tar:
                tar.extractall(".")
                self.console.print("[green]Configuration files imported[/green]")
                
                # List imported files
                for member in tar.getmembers():
                    if member.isfile():
                        self.console.print(f"[cyan]Imported: {member.name}[/cyan]")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Configuration import failed: {e}")
            return False
        

def check_all_requirements():
    pilot = DockerPilotEnhanced()
    return pilot.validate_system_requirements()

if __name__ == "__main__":
    # Minimal bootstrap to honor --config, --log-level and --version before launching CLI
    try:
        from . import __version__
    except ImportError:
        __version__ = "Enhanced"
    
    bootstrap_parser = argparse.ArgumentParser(add_help=False)
    bootstrap_parser.add_argument('--version', action='version', version=f'DockerPilot {__version__}')
    bootstrap_parser.add_argument('--config', '-c', type=str, default=None)
    bootstrap_parser.add_argument('--log-level', '-l', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], default='INFO')
    known_args, _ = bootstrap_parser.parse_known_args()

    try:
        log_level_enum = LogLevel[known_args.log_level]
    except Exception:
        log_level_enum = LogLevel.INFO

    pilot = DockerPilotEnhanced(config_file=known_args.config, log_level=log_level_enum)
    pilot.run_cli()

